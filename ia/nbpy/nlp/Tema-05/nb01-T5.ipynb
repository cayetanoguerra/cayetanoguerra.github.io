{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text-To-Text Transfer Transformer (T5)**\n",
    "\n",
    "El modelo T5, o **Text-To-Text Transfer Transformer**, es un modelo de lenguaje muy versátil desarrollado por Google Research. Fue introducido en un artículo titulado <a href=\"https://arxiv.org/pdf/1910.10683.pdf\">\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"</a> por Colin Raffel y otros en 2019. El modelo se basa en la arquitectura Transformer, que se ha convertido en un estándar de facto para muchas tareas de procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "### **Diseño y Filosofía**\n",
    "\n",
    "El modelo T5 adopta un enfoque unificado hacia el procesamiento del lenguaje natural: trata todas las tareas de NLP como una tarea de \"texto a texto\". Esto significa que cada tarea, ya sea traducción de idiomas, resumen de texto, clasificación de sentimientos, o cualquier otra, se formula de manera que el input y el output son siempre secuencias de texto. Por ejemplo:\n",
    "- **Traducción**: El input es texto en un idioma, y el output es texto en otro idioma.\n",
    "- **Resumen**: El input es un documento largo, y el output es su resumen.\n",
    "- **Clasificación de sentimiento**: El input es una reseña, y el output es una etiqueta de sentimiento como \"positivo\" o \"negativo\".\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/T5.png\" width=\"600px\">\n",
    "</div>\n",
    "\n",
    "### **Arquitectura**\n",
    "\n",
    "T5 es un modelo basado en la arquitectura Transformer, que utiliza bloques de encoder y decoder:\n",
    "- **Encoder**: Convierte el texto de entrada en una serie de representaciones intermedias o embeddings que capturan el contexto y el significado del texto.\n",
    "- **Decoder**: Utiliza las representaciones del encoder, junto con la salida generada previamente, para producir el texto de salida.\n",
    "\n",
    "### **Preentrenamiento**\n",
    "\n",
    "T5 fue preentrenado en un dataset diverso llamado \"Colossal Clean Crawled Corpus\" (C4), que es un subset limpio y filtrado del Common Crawl.\n",
    "\n",
    "### **Fases de entrenamiento**\n",
    "\n",
    "T5 se entrena en dos fases:\n",
    "1. **Preentrenamiento**: El modelo aprende a entender y generar texto en general a partir de grandes cantidades de texto no etiquetado.\n",
    "2. **Fine-tuning**: El modelo se ajusta a tareas específicas de NLP usando datasets etiquetados más pequeños. Aquí es donde el enfoque de \"texto a texto\" del modelo se adapta fácilmente a una variedad de tareas simplemente cambiando los formatos de los datos de entrada y salida.\n",
    "\n",
    "### **Variantes**\n",
    "\n",
    "En la biblioteca Hugging Face Transformers, el modelo T5 está disponible en varios tamaños que se adaptan a diferentes requisitos de rendimiento y capacidades de procesamiento. Cada tamaño del modelo ofrece un equilibrio entre velocidad, uso de memoria y precisión. Estas son las variantes disponibles:\n",
    "\n",
    "1. **T5 Small**\n",
    "   - **Parámetros**: Aproximadamente 60 millones.\n",
    "   - **Uso**: Ideal para aplicaciones con restricciones de recursos y para pruebas rápidas de conceptos.\n",
    "\n",
    "2. **T5 Base**\n",
    "   - **Parámetros**: Aproximadamente 220 millones.\n",
    "   - **Uso**: Un buen equilibrio entre rendimiento y tamaño, adecuado para muchas aplicaciones de producción.\n",
    "\n",
    "3. **T5 Large**\n",
    "   - **Parámetros**: Aproximadamente 770 millones.\n",
    "   - **Uso**: Para cuando se necesita una mayor precisión en las tareas y se dispone de más recursos de computación.\n",
    "\n",
    "4. **T5 3B**\n",
    "   - **Parámetros**: Aproximadamente 3 mil millones.\n",
    "   - **Uso**: Usado en escenarios donde la precisión es crítica y se dispone de infraestructura para manejar modelos grandes.\n",
    "\n",
    "5. **T5 11B**\n",
    "   - **Parámetros**: Aproximadamente 11 mil millones.\n",
    "   - **Uso**: Este tamaño es extremadamente grande, utilizado principalmente en investigación y situaciones donde se necesitan las capacidades máximas del modelo.\n",
    "\n",
    "#### **Cómo elegir el tamaño adecuado**\n",
    "\n",
    "La elección del tamaño del modelo depende de varios factores:\n",
    "- **Recursos disponibles**: Más parámetros generalmente requieren más memoria y poder de procesamiento.\n",
    "- **Requisitos de la tarea**: Tareas más complejas pueden beneficiarse de modelos más grandes.\n",
    "- **Latencia**: Modelos más pequeños ofrecen respuestas más rápidas, lo cual es crucial para aplicaciones en tiempo real.\n",
    "- **Costo**: El entrenamiento y la inferencia en modelos más grandes pueden ser más costosos en términos de computación y tiempo.\n",
    "\n",
    "Puedes acceder a estos modelos directamente a través de la interfaz de Hugging Face Transformers, lo cual facilita su uso y experimentación en una amplia gama de tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "### **Ejemplos de uso mediante Hugging Face Transformers**\n",
    "\n",
    "Aquí tienes un ejemplo de cómo cargar y usar el modelo T5 en Hugging Face Transformers para hacer resúmenes de texto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen: the Beatles were an english rock band formed in 1960. they are regarded as the most influential band of all time. they were integral to the development of 1960s counterculture.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def sumarize(text, model_name=\"t5-base\", task=\"summarize\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return summarized_text\n",
    "\n",
    "# Ejemplo de resumen\n",
    "text = \"\"\"\n",
    "\"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr. They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\n",
    "\"\"\"\n",
    "print(\"Resumen:\", sumarize(text, task=\"summarize\"))  # <-- Fíjate en el argumento task. En función de este argumento, el modelo realizará una tarea u otra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo traducir de inglés a francés utilizando el modelo T5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglés a Francés: Les Beatles sont un groupe rock anglais formé à Liverpool en 1960, composé de John Lennon, Paul McCartney, George Harrison et Ringo Starr.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglés a Francés: Ils sont considérés comme le groupe le plus influent de tous les temps et ont joué un rôle essentiel dans le développement de la contreculture des années 1960 et la reconnaissance de la musique populaire comme forme d'art.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def translate(text, model_name=\"t5-base\", task=\"translate English to French\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Ejemplo de traducción del inglés al francés\n",
    "text_en_to_es = \"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr.\"\n",
    "print(\"Inglés a Francés:\", translate(text_en_to_es, task=\"translate English to French\"))\n",
    "text_en_to_es = \"They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\"\n",
    "print(\"Inglés a Francés:\", translate(text_en_to_es, task=\"translate English to French\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejemplo: Transcripción de números a textos**\n",
    "\n",
    "T5 puede ser utilizado directamente a través de la biblioteca `transformers` de Hugging Face, que proporciona APIs de alto nivel para cargar el modelo, tokenizar textos, y generar predicciones. Esto hace que sea relativamente sencillo implementar soluciones de NLP avanzadas utilizando T5.\n",
    "\n",
    "Vamos, por tanto, a implementar un ejemplo que nos permita entender cómo funciona T5 y cómo podemos utilizarlo para tareas de procesamiento de lenguaje natural propias. En este caso, utilizaremos el modelo T5 Base para realizar la transcripción de un número representado con sus dígitos a sus palabras en inglés. Por ejemplo, si el número es \"123\", la transcripción sería \"one hundred twenty-three\". Lo haremos en inglés en lugar de español para aprovechar la capacidad de T5 de trabajar con texto en inglés y porque, en tareas de traducción, el modelo solo ha sido entrenado en alemán, francés y rumando, además del inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librería necesarias para crear el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Cargamos el dataset desde un archivo CSV\n",
    "dataset = load_dataset('csv', data_files='data/numbers.csv')\n",
    "\n",
    "# Como el dataset no está dividido en entrenamiento y prueba, lo dividimos manualmente\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.1)  # 90% entrenamiento, 10% prueba\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo cualquiera del dataset. Fíjate en que el input no está en texto, sino en formato entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': 313400614,\n",
       " 'output_text': 'three hundred thirteen million four hundred thousand six hundred fourteen'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afinaremos un modelo T5 Base preentrenado para realizar esta tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerdas que antes vimos que el input del dataset estaba en formato entero. Para poder utilizarlo con el modelo T5, necesitamos convertirlo a texto. Además, queremos añadir a cada ejemplo la tarea específica que queremos que el modelo realice. En este caso, la tarea es \"number to text\". Para todo esto vamos a crear la función `add_task`. Fíjate que numbers puede ser tanto un número entero como una lista de números enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_task(numbers):\n",
    "    if isinstance(numbers, int):\n",
    "        return \"number to text: \" + str(numbers)\n",
    "    else:\n",
    "        res = []\n",
    "        for number in numbers:\n",
    "            text = str(number)\n",
    "            text = \"number to text: \" + text\n",
    "            res.append(text)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué aspecto tiene un ejemplo después de aplicar la función `add_task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number to text: 123456789',\n",
       " 'number to text: 111111111',\n",
       " 'number to text: 987654321']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_task([123456789, 111111111, 987654321])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    number_ids = add_task(examples['input_text'])\n",
    "    text_input = tokenizer(number_ids, truncation=True, padding=\"max_length\", max_length=15)\n",
    "    labels = tokenizer(examples['output_text'], truncation=True, padding=\"max_length\", max_length=32)\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_input['input_ids'],\n",
    "        'labels': labels['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué aspecto tiene un conjunto de ejemplos antes y después de aplicar la función `preprocess_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': [104272327, 388114925, 491537915], 'output_text': ['one hundred four million two hundred seventy two thousand three hundred twenty seven', 'three hundred eighty eight million one hundred fourteen thousand nine hundred twenty five', 'four hundred ninety one million five hundred thirty seven thousand nine hundred fifteen']}\n",
      "------------------------------\n",
      "{'input_ids': [[381, 12, 1499, 10, 3, 15442, 2555, 2773, 2555, 1, 0, 0, 0, 0, 0], [381, 12, 1499, 10, 220, 4060, 18959, 28456, 1, 0, 0, 0, 0, 0, 0], [381, 12, 1499, 10, 9526, 27025, 4440, 1808, 1, 0, 0, 0, 0, 0, 0]], 'labels': [[80, 6189, 662, 770, 192, 6189, 2391, 17, 63, 192, 7863, 386, 6189, 6786, 2391, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [386, 6189, 2641, 63, 2641, 770, 80, 6189, 27137, 7863, 4169, 6189, 6786, 874, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [662, 6189, 4169, 17, 63, 80, 770, 874, 6189, 12010, 2391, 7863, 4169, 6189, 17310, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:3])\n",
    "print(\"------------------------------\")\n",
    "print(preprocess_function(dataset['train'][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight\n",
      "e\n",
      "en\n",
      "seventeen\n",
      "sixteen\n",
      "</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "a = tokenizer(\"eighteen seventeen sixteen\")\n",
    "\n",
    "\n",
    "for i in a['input_ids']:\n",
    "    print(tokenizer.decode([i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargamos el modelo T5 Base preentrenado y lo afinamos para realizar la tarea de transcripción de números a texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a859208f0f4a8589cd9840bb49d77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1453e190eea345cb89e2024c47db9bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcayetano\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cayetano/Propio/Docencia/cayetanoguerra.github.io/Adevinta-NLP/Tema-06/wandb/run-20240416_103642-cgiq4pod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cayetano/huggingface/runs/cgiq4pod' target=\"_blank\">rich-wood-28</a></strong> to <a href='https://wandb.ai/cayetano/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cayetano/huggingface' target=\"_blank\">https://wandb.ai/cayetano/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cayetano/huggingface/runs/cgiq4pod' target=\"_blank\">https://wandb.ai/cayetano/huggingface/runs/cgiq4pod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdff17441c0f4d0981c6a2c5e39c1de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.8692, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.02}\n",
      "{'loss': 8.5515, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.04}\n",
      "{'loss': 8.0295, 'learning_rate': 3e-06, 'epoch': 0.05}\n",
      "{'loss': 6.9652, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.07}\n",
      "{'loss': 5.8674, 'learning_rate': 5e-06, 'epoch': 0.09}\n",
      "{'loss': 4.7683, 'learning_rate': 6e-06, 'epoch': 0.11}\n",
      "{'loss': 3.6859, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.12}\n",
      "{'loss': 2.8453, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.14}\n",
      "{'loss': 2.3039, 'learning_rate': 9e-06, 'epoch': 0.16}\n",
      "{'loss': 2.0861, 'learning_rate': 1e-05, 'epoch': 0.18}\n",
      "{'loss': 1.691, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 1.5033, 'learning_rate': 1.2e-05, 'epoch': 0.21}\n",
      "{'loss': 1.2673, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 1.0955, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9426, 'learning_rate': 1.5e-05, 'epoch': 0.27}\n",
      "{'loss': 0.8592, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.28}\n",
      "{'loss': 0.7551, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6877, 'learning_rate': 1.8e-05, 'epoch': 0.32}\n",
      "{'loss': 0.6012, 'learning_rate': 1.9e-05, 'epoch': 0.34}\n",
      "{'loss': 0.5756, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.5403, 'learning_rate': 2.1e-05, 'epoch': 0.37}\n",
      "{'loss': 0.4963, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.39}\n",
      "{'loss': 0.4714, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.41}\n",
      "{'loss': 0.4551, 'learning_rate': 2.4e-05, 'epoch': 0.43}\n",
      "{'loss': 0.444, 'learning_rate': 2.5e-05, 'epoch': 0.44}\n",
      "{'loss': 0.4347, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3977, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3719, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3772, 'learning_rate': 2.9e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3613, 'learning_rate': 3e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3565, 'learning_rate': 3.1e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3548, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3286, 'learning_rate': 3.3e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3293, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.6}\n",
      "{'loss': 0.331, 'learning_rate': 3.5e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3074, 'learning_rate': 3.6e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3039, 'learning_rate': 3.7e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3103, 'learning_rate': 3.8e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2734, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.69}\n",
      "{'loss': 0.301, 'learning_rate': 4e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2868, 'learning_rate': 4.1e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2705, 'learning_rate': 4.2e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2622, 'learning_rate': 4.3e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2579, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2563, 'learning_rate': 4.5e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2353, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.82}\n",
      "{'loss': 0.2353, 'learning_rate': 4.7e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2281, 'learning_rate': 4.8e-05, 'epoch': 0.85}\n",
      "{'loss': 0.2327, 'learning_rate': 4.9e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2156, 'learning_rate': 5e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2044, 'learning_rate': 4.957947855340623e-05, 'epoch': 0.91}\n",
      "{'loss': 0.2235, 'learning_rate': 4.915895710681245e-05, 'epoch': 0.92}\n",
      "{'loss': 0.2048, 'learning_rate': 4.8738435660218675e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2146, 'learning_rate': 4.83179142136249e-05, 'epoch': 0.96}\n",
      "{'loss': 0.1983, 'learning_rate': 4.789739276703112e-05, 'epoch': 0.98}\n",
      "{'loss': 0.1964, 'learning_rate': 4.747687132043734e-05, 'epoch': 0.99}\n",
      "{'loss': 0.192, 'learning_rate': 4.705634987384357e-05, 'epoch': 1.01}\n",
      "{'loss': 0.1826, 'learning_rate': 4.66358284272498e-05, 'epoch': 1.03}\n",
      "{'loss': 0.1824, 'learning_rate': 4.6215306980656014e-05, 'epoch': 1.05}\n",
      "{'loss': 0.1735, 'learning_rate': 4.579478553406224e-05, 'epoch': 1.07}\n",
      "{'loss': 0.1686, 'learning_rate': 4.537426408746846e-05, 'epoch': 1.08}\n",
      "{'loss': 0.1798, 'learning_rate': 4.495374264087469e-05, 'epoch': 1.1}\n",
      "{'loss': 0.1806, 'learning_rate': 4.453322119428091e-05, 'epoch': 1.12}\n",
      "{'loss': 0.1555, 'learning_rate': 4.4112699747687136e-05, 'epoch': 1.14}\n",
      "{'loss': 0.167, 'learning_rate': 4.369217830109335e-05, 'epoch': 1.15}\n",
      "{'loss': 0.1635, 'learning_rate': 4.3271656854499584e-05, 'epoch': 1.17}\n",
      "{'loss': 0.1622, 'learning_rate': 4.285113540790581e-05, 'epoch': 1.19}\n",
      "{'loss': 0.1574, 'learning_rate': 4.2430613961312026e-05, 'epoch': 1.21}\n",
      "{'loss': 0.1565, 'learning_rate': 4.201009251471825e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1562, 'learning_rate': 4.1589571068124474e-05, 'epoch': 1.24}\n",
      "{'loss': 0.1583, 'learning_rate': 4.11690496215307e-05, 'epoch': 1.26}\n",
      "{'loss': 0.1395, 'learning_rate': 4.074852817493692e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1393, 'learning_rate': 4.032800672834315e-05, 'epoch': 1.3}\n",
      "{'loss': 0.1502, 'learning_rate': 3.990748528174937e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1342, 'learning_rate': 3.9486963835155596e-05, 'epoch': 1.33}\n",
      "{'loss': 0.1439, 'learning_rate': 3.906644238856182e-05, 'epoch': 1.35}\n",
      "{'loss': 0.1493, 'learning_rate': 3.8645920941968044e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1278, 'learning_rate': 3.822539949537426e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1351, 'learning_rate': 3.780487804878049e-05, 'epoch': 1.4}\n",
      "{'loss': 0.151, 'learning_rate': 3.738435660218672e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1255, 'learning_rate': 3.6963835155592935e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1351, 'learning_rate': 3.654331370899916e-05, 'epoch': 1.46}\n",
      "{'loss': 0.1245, 'learning_rate': 3.612279226240538e-05, 'epoch': 1.47}\n",
      "{'loss': 0.1285, 'learning_rate': 3.570227081581161e-05, 'epoch': 1.49}\n",
      "{'loss': 0.1322, 'learning_rate': 3.528174936921783e-05, 'epoch': 1.51}\n",
      "{'loss': 0.1342, 'learning_rate': 3.4861227922624056e-05, 'epoch': 1.53}\n",
      "{'loss': 0.1218, 'learning_rate': 3.4440706476030274e-05, 'epoch': 1.55}\n",
      "{'loss': 0.1224, 'learning_rate': 3.4020185029436505e-05, 'epoch': 1.56}\n",
      "{'loss': 0.1031, 'learning_rate': 3.359966358284273e-05, 'epoch': 1.58}\n",
      "{'loss': 0.1275, 'learning_rate': 3.317914213624895e-05, 'epoch': 1.6}\n",
      "{'loss': 0.1074, 'learning_rate': 3.275862068965517e-05, 'epoch': 1.62}\n",
      "{'loss': 0.1142, 'learning_rate': 3.2338099243061395e-05, 'epoch': 1.63}\n",
      "{'loss': 0.1151, 'learning_rate': 3.1917577796467626e-05, 'epoch': 1.65}\n",
      "{'loss': 0.1077, 'learning_rate': 3.1497056349873844e-05, 'epoch': 1.67}\n",
      "{'loss': 0.1038, 'learning_rate': 3.107653490328007e-05, 'epoch': 1.69}\n",
      "{'loss': 0.1194, 'learning_rate': 3.065601345668629e-05, 'epoch': 1.71}\n",
      "{'loss': 0.1063, 'learning_rate': 3.0235492010092513e-05, 'epoch': 1.72}\n",
      "{'loss': 0.1034, 'learning_rate': 2.981497056349874e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0947, 'learning_rate': 2.9394449116904965e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1091, 'learning_rate': 2.8973927670311186e-05, 'epoch': 1.78}\n",
      "{'loss': 0.1049, 'learning_rate': 2.855340622371741e-05, 'epoch': 1.79}\n",
      "{'loss': 0.1101, 'learning_rate': 2.8132884777123634e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0978, 'learning_rate': 2.7712363330529855e-05, 'epoch': 1.83}\n",
      "{'loss': 0.095, 'learning_rate': 2.729184188393608e-05, 'epoch': 1.85}\n",
      "{'loss': 0.1047, 'learning_rate': 2.6871320437342307e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0963, 'learning_rate': 2.645079899074853e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0931, 'learning_rate': 2.6030277544154752e-05, 'epoch': 1.9}\n",
      "{'loss': 0.092, 'learning_rate': 2.5609756097560977e-05, 'epoch': 1.92}\n",
      "{'loss': 0.1106, 'learning_rate': 2.5189234650967204e-05, 'epoch': 1.94}\n",
      "{'loss': 0.1019, 'learning_rate': 2.4768713204373425e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0957, 'learning_rate': 2.434819175777965e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0917, 'learning_rate': 2.392767031118587e-05, 'epoch': 1.99}\n",
      "{'loss': 0.095, 'learning_rate': 2.3507148864592095e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0848, 'learning_rate': 2.308662741799832e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0944, 'learning_rate': 2.266610597140454e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0939, 'learning_rate': 2.2245584524810768e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0847, 'learning_rate': 2.182506307821699e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0847, 'learning_rate': 2.1404541631623216e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0923, 'learning_rate': 2.0984020185029437e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0862, 'learning_rate': 2.056349873843566e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0751, 'learning_rate': 2.0142977291841886e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0806, 'learning_rate': 1.972245584524811e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0752, 'learning_rate': 1.930193439865433e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0861, 'learning_rate': 1.8881412952060555e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0727, 'learning_rate': 1.846089150546678e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0933, 'learning_rate': 1.8040370058873004e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0921, 'learning_rate': 1.7619848612279228e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0828, 'learning_rate': 1.719932716568545e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0697, 'learning_rate': 1.6778805719091676e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0822, 'learning_rate': 1.6358284272497897e-05, 'epoch': 2.31}\n",
      "{'loss': 0.082, 'learning_rate': 1.593776282590412e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0879, 'learning_rate': 1.5517241379310346e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0918, 'learning_rate': 1.5096719932716568e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0849, 'learning_rate': 1.4676198486122794e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0866, 'learning_rate': 1.4255677039529017e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0852, 'learning_rate': 1.383515559293524e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0846, 'learning_rate': 1.3414634146341466e-05, 'epoch': 2.43}\n",
      "{'loss': 0.0792, 'learning_rate': 1.2994112699747688e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0935, 'learning_rate': 1.257359125315391e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0776, 'learning_rate': 1.2153069806560135e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0824, 'learning_rate': 1.1732548359966358e-05, 'epoch': 2.5}\n",
      "{'loss': 0.0821, 'learning_rate': 1.1312026913372582e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0798, 'learning_rate': 1.0891505466778806e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0723, 'learning_rate': 1.047098402018503e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0782, 'learning_rate': 1.0050462573591253e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0785, 'learning_rate': 9.629941126997477e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0811, 'learning_rate': 9.209419680403702e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0704, 'learning_rate': 8.788898233809926e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0807, 'learning_rate': 8.368376787216148e-06, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0857, 'learning_rate': 7.947855340622373e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0727, 'learning_rate': 7.527333894028596e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0701, 'learning_rate': 7.10681244743482e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0757, 'learning_rate': 6.686291000841043e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0811, 'learning_rate': 6.265769554247266e-06, 'epoch': 2.74}\n",
      "{'loss': 0.07, 'learning_rate': 5.845248107653491e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0695, 'learning_rate': 5.424726661059714e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0769, 'learning_rate': 5.004205214465938e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0744, 'learning_rate': 4.583683767872162e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0795, 'learning_rate': 4.163162321278385e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0662, 'learning_rate': 3.7426408746846087e-06, 'epoch': 2.84}\n",
      "{'loss': 0.074, 'learning_rate': 3.322119428090833e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0854, 'learning_rate': 2.9015979814970564e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0787, 'learning_rate': 2.4810765349032802e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0707, 'learning_rate': 2.060555088309504e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0772, 'learning_rate': 1.6400336417157277e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0769, 'learning_rate': 1.2195121951219514e-06, 'epoch': 2.95}\n",
      "{'loss': 0.08, 'learning_rate': 7.98990748528175e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0755, 'learning_rate': 3.7846930193439866e-07, 'epoch': 2.98}\n",
      "{'train_runtime': 878.6912, 'train_samples_per_second': 30.728, 'train_steps_per_second': 1.922, 'train_loss': 0.5159421686663156, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1689, training_loss=0.5159421686663156, metrics={'train_runtime': 878.6912, 'train_samples_per_second': 30.728, 'train_steps_per_second': 1.922, 'train_loss': 0.5159421686663156, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # output directory\n",
    "    num_train_epochs=3,  # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir='./logs',  # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=dataset['train'].map(preprocess_function, batched=True),  # training dataset\n",
    "    eval_dataset=dataset['test'].map(preprocess_function, batched=True),  # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez completado el entrenamiento, vamos a hacer alguna prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hundred one million\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"number to text: 1000\", return_tensors=\"pt\").input_ids.to('mps')\n",
    "outputs = model.generate(input_ids, max_length=50, num_beams=1)\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tamaño del modelo y recursos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo T5 (Text-to-Text Transfer Transformer), incluido el T5-base, generalmente maneja secuencias de entrada cuya longitud máxima predeterminada es de 512 tokens. Este límite está configurado así en los modelos preentrenados disponibles en la biblioteca Hugging Face Transformers.\n",
    "\n",
    "Veamos cuántos tokens de entrada y salida tiene el modelo T5 Base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño máximo de la entrada: 512 tokens\n",
      "Memoria ocupada por el modelo: 850.31 MB\n",
      "Número de parámetros del modelo: 222.9 millones\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamaño máximo de la entrada: {tokenizer.model_max_length} tokens\")  # Generalmente será 512\n",
    "\n",
    "# Memoria ocupada por el modelo en MB\n",
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Memoria ocupada por el modelo: {round(model_size * 4 / 1024**2, 2)} MB\")\n",
    "\n",
    "# Número de parámetros del modelo en millones\n",
    "print(f\"Número de parámetros del modelo: {round(model_size/10**6,2)} millones\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text-To-Text Transfer Transformer (T5)**\n",
    "\n",
    "El modelo T5, o **Text-To-Text Transfer Transformer**, es un modelo de lenguaje muy vers√°til desarrollado por Google Research. Fue introducido en un art√≠culo titulado <a href=\"https://arxiv.org/pdf/1910.10683.pdf\">\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"</a> por Colin Raffel y otros en 2019. El modelo se basa en la arquitectura Transformer, que se ha convertido en un est√°ndar de facto para muchas tareas de procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "### **Dise√±o y Filosof√≠a**\n",
    "\n",
    "El modelo T5 adopta un enfoque unificado hacia el procesamiento del lenguaje natural: trata todas las tareas de NLP como una tarea de \"texto a texto\". Esto significa que cada tarea, ya sea traducci√≥n de idiomas, resumen de texto, clasificaci√≥n de sentimientos, o cualquier otra, se formula de manera que el input y el output son siempre secuencias de texto. Por ejemplo:\n",
    "- **Traducci√≥n**: El input es texto en un idioma, y el output es texto en otro idioma.\n",
    "- **Resumen**: El input es un documento largo, y el output es su resumen.\n",
    "- **Clasificaci√≥n de sentimiento**: El input es una rese√±a, y el output es una etiqueta de sentimiento como \"positivo\" o \"negativo\".\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/T5.png\" width=\"600px\">\n",
    "</div>\n",
    "\n",
    "### **Arquitectura**\n",
    "\n",
    "T5 es un modelo basado en la arquitectura Transformer, que utiliza bloques de encoder y decoder:\n",
    "- **Encoder**: Convierte el texto de entrada en una serie de representaciones intermedias o embeddings que capturan el contexto y el significado del texto.\n",
    "- **Decoder**: Utiliza las representaciones del encoder, junto con la salida generada previamente, para producir el texto de salida.\n",
    "\n",
    "### **Preentrenamiento**\n",
    "\n",
    "T5 fue preentrenado en un dataset diverso llamado \"Colossal Clean Crawled Corpus\" (C4), que es un subset limpio y filtrado del Common Crawl.\n",
    "\n",
    "### **Fases de entrenamiento**\n",
    "\n",
    "T5 se entrena en dos fases:\n",
    "1. **Preentrenamiento**: El modelo aprende a entender y generar texto en general a partir de grandes cantidades de texto no etiquetado.\n",
    "2. **Fine-tuning**: El modelo se ajusta a tareas espec√≠ficas de NLP usando datasets etiquetados m√°s peque√±os. Aqu√≠ es donde el enfoque de \"texto a texto\" del modelo se adapta f√°cilmente a una variedad de tareas simplemente cambiando los formatos de los datos de entrada y salida.\n",
    "\n",
    "### **Variantes**\n",
    "\n",
    "En la biblioteca Hugging Face Transformers, el modelo T5 est√° disponible en varios tama√±os que se adaptan a diferentes requisitos de rendimiento y capacidades de procesamiento. Cada tama√±o del modelo ofrece un equilibrio entre velocidad, uso de memoria y precisi√≥n. Estas son las variantes disponibles:\n",
    "\n",
    "1. **T5 Small**\n",
    "   - **Par√°metros**: Aproximadamente 60 millones.\n",
    "   - **Uso**: Ideal para aplicaciones con restricciones de recursos y para pruebas r√°pidas de conceptos.\n",
    "\n",
    "2. **T5 Base**\n",
    "   - **Par√°metros**: Aproximadamente 220 millones.\n",
    "   - **Uso**: Un buen equilibrio entre rendimiento y tama√±o, adecuado para muchas aplicaciones de producci√≥n.\n",
    "\n",
    "3. **T5 Large**\n",
    "   - **Par√°metros**: Aproximadamente 770 millones.\n",
    "   - **Uso**: Para cuando se necesita una mayor precisi√≥n en las tareas y se dispone de m√°s recursos de computaci√≥n.\n",
    "\n",
    "4. **T5 3B**\n",
    "   - **Par√°metros**: Aproximadamente 3 mil millones.\n",
    "   - **Uso**: Usado en escenarios donde la precisi√≥n es cr√≠tica y se dispone de infraestructura para manejar modelos grandes.\n",
    "\n",
    "5. **T5 11B**\n",
    "   - **Par√°metros**: Aproximadamente 11 mil millones.\n",
    "   - **Uso**: Este tama√±o es extremadamente grande, utilizado principalmente en investigaci√≥n y situaciones donde se necesitan las capacidades m√°ximas del modelo.\n",
    "\n",
    "#### **C√≥mo elegir el tama√±o adecuado**\n",
    "\n",
    "La elecci√≥n del tama√±o del modelo depende de varios factores:\n",
    "- **Recursos disponibles**: M√°s par√°metros generalmente requieren m√°s memoria y poder de procesamiento.\n",
    "- **Requisitos de la tarea**: Tareas m√°s complejas pueden beneficiarse de modelos m√°s grandes.\n",
    "- **Latencia**: Modelos m√°s peque√±os ofrecen respuestas m√°s r√°pidas, lo cual es crucial para aplicaciones en tiempo real.\n",
    "- **Costo**: El entrenamiento y la inferencia en modelos m√°s grandes pueden ser m√°s costosos en t√©rminos de computaci√≥n y tiempo.\n",
    "\n",
    "Puedes acceder a estos modelos directamente a trav√©s de la interfaz de Hugging Face Transformers, lo cual facilita su uso y experimentaci√≥n en una amplia gama de tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "### **Ejemplos de uso mediante Hugging Face Transformers**\n",
    "\n",
    "Aqu√≠ tienes un ejemplo de c√≥mo cargar y usar el modelo T5 en Hugging Face Transformers para hacer res√∫menes de texto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen: the Beatles were an english rock band formed in 1960. they are regarded as the most influential band of all time. they were integral to the development of 1960s counterculture.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def sumarize(text, model_name=\"t5-base\", task=\"summarize\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return summarized_text\n",
    "\n",
    "# Ejemplo de resumen\n",
    "text = \"\"\"\n",
    "\"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr. They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\n",
    "\"\"\"\n",
    "print(\"Resumen:\", sumarize(text, task=\"summarize\"))  # <-- F√≠jate en el argumento task. En funci√≥n de este argumento, el modelo realizar√° una tarea u otra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver c√≥mo traducir de ingl√©s a franc√©s utilizando el modelo T5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingl√©s a Franc√©s: Les Beatles sont un groupe rock anglais form√© √† Liverpool en 1960, compos√© de John Lennon, Paul McCartney, George Harrison et Ringo Starr.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingl√©s a Franc√©s: Ils sont consid√©r√©s comme le groupe le plus influent de tous les temps et ont jou√© un r√¥le essentiel dans le d√©veloppement de la contreculture des ann√©es 1960 et la reconnaissance de la musique populaire comme forme d'art.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def translate(text, model_name=\"t5-base\", task=\"translate English to French\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Ejemplo de traducci√≥n del ingl√©s al franc√©s\n",
    "text_en_to_es = \"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr.\"\n",
    "print(\"Ingl√©s a Franc√©s:\", translate(text_en_to_es, task=\"translate English to French\"))\n",
    "text_en_to_es = \"They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\"\n",
    "print(\"Ingl√©s a Franc√©s:\", translate(text_en_to_es, task=\"translate English to French\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejemplo: Transcripci√≥n de n√∫meros a textos**\n",
    "\n",
    "T5 puede ser utilizado directamente a trav√©s de la biblioteca `transformers` de Hugging Face, que proporciona APIs de alto nivel para cargar el modelo, tokenizar textos, y generar predicciones. Esto hace que sea relativamente sencillo implementar soluciones de NLP avanzadas utilizando T5.\n",
    "\n",
    "Vamos, por tanto, a implementar un ejemplo que nos permita entender c√≥mo funciona T5 y c√≥mo podemos utilizarlo para tareas de procesamiento de lenguaje natural propias. En este caso, utilizaremos el modelo T5 Base para realizar la transcripci√≥n de un n√∫mero representado con sus d√≠gitos a sus palabras en ingl√©s. Por ejemplo, si el n√∫mero es \"123\", la transcripci√≥n ser√≠a \"one hundred twenty-three\". Lo haremos en ingl√©s en lugar de espa√±ol para aprovechar la capacidad de T5 de trabajar con texto en ingl√©s y porque, en tareas de traducci√≥n, el modelo solo ha sido entrenado en alem√°n, franc√©s y rumando, adem√°s del ingl√©s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librer√≠a necesarias para crear el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Cargamos el dataset desde un archivo CSV\n",
    "dataset = load_dataset('csv', data_files='data/numbers.csv')\n",
    "\n",
    "# Como el dataset no est√° dividido en entrenamiento y prueba, lo dividimos manualmente\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.1)  # 90% entrenamiento, 10% prueba\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo cualquiera del dataset. F√≠jate en que el input no est√° en texto, sino en formato entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': 313400614,\n",
       " 'output_text': 'three hundred thirteen million four hundred thousand six hundred fourteen'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afinaremos un modelo T5 Base preentrenado para realizar esta tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerdas que antes vimos que el input del dataset estaba en formato entero. Para poder utilizarlo con el modelo T5, necesitamos convertirlo a texto. Adem√°s, queremos a√±adir a cada ejemplo la tarea espec√≠fica que queremos que el modelo realice. En este caso, la tarea es \"number to text\". Para todo esto vamos a crear la funci√≥n `add_task`. F√≠jate que numbers puede ser tanto un n√∫mero entero como una lista de n√∫meros enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_task(numbers):\n",
    "    if isinstance(numbers, int):\n",
    "        return \"number to text: \" + str(numbers)\n",
    "    else:\n",
    "        res = []\n",
    "        for number in numbers:\n",
    "            text = str(number)\n",
    "            text = \"number to text: \" + text\n",
    "            res.append(text)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qu√© aspecto tiene un ejemplo despu√©s de aplicar la funci√≥n `add_task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number to text: 123456789',\n",
       " 'number to text: 111111111',\n",
       " 'number to text: 987654321']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_task([123456789, 111111111, 987654321])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    number_ids = add_task(examples['input_text'])\n",
    "    text_input = tokenizer(number_ids, truncation=True, padding=\"max_length\", max_length=15)\n",
    "    labels = tokenizer(examples['output_text'], truncation=True, padding=\"max_length\", max_length=32)\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_input['input_ids'],\n",
    "        'labels': labels['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qu√© aspecto tiene un conjunto de ejemplos antes y despu√©s de aplicar la funci√≥n `preprocess_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': [104272327, 388114925, 491537915], 'output_text': ['one hundred four million two hundred seventy two thousand three hundred twenty seven', 'three hundred eighty eight million one hundred fourteen thousand nine hundred twenty five', 'four hundred ninety one million five hundred thirty seven thousand nine hundred fifteen']}\n",
      "------------------------------\n",
      "{'input_ids': [[381, 12, 1499, 10, 3, 15442, 2555, 2773, 2555, 1, 0, 0, 0, 0, 0], [381, 12, 1499, 10, 220, 4060, 18959, 28456, 1, 0, 0, 0, 0, 0, 0], [381, 12, 1499, 10, 9526, 27025, 4440, 1808, 1, 0, 0, 0, 0, 0, 0]], 'labels': [[80, 6189, 662, 770, 192, 6189, 2391, 17, 63, 192, 7863, 386, 6189, 6786, 2391, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [386, 6189, 2641, 63, 2641, 770, 80, 6189, 27137, 7863, 4169, 6189, 6786, 874, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [662, 6189, 4169, 17, 63, 80, 770, 874, 6189, 12010, 2391, 7863, 4169, 6189, 17310, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:3])\n",
    "print(\"------------------------------\")\n",
    "print(preprocess_function(dataset['train'][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight\n",
      "e\n",
      "en\n",
      "seventeen\n",
      "sixteen\n",
      "</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "a = tokenizer(\"eighteen seventeen sixteen\")\n",
    "\n",
    "\n",
    "for i in a['input_ids']:\n",
    "    print(tokenizer.decode([i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargamos el modelo T5 Base preentrenado y lo afinamos para realizar la tarea de transcripci√≥n de n√∫meros a texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a859208f0f4a8589cd9840bb49d77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1453e190eea345cb89e2024c47db9bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcayetano\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cayetano/Propio/Docencia/cayetanoguerra.github.io/Adevinta-NLP/Tema-06/wandb/run-20240416_103642-cgiq4pod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cayetano/huggingface/runs/cgiq4pod' target=\"_blank\">rich-wood-28</a></strong> to <a href='https://wandb.ai/cayetano/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cayetano/huggingface' target=\"_blank\">https://wandb.ai/cayetano/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cayetano/huggingface/runs/cgiq4pod' target=\"_blank\">https://wandb.ai/cayetano/huggingface/runs/cgiq4pod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdff17441c0f4d0981c6a2c5e39c1de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.8692, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.02}\n",
      "{'loss': 8.5515, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.04}\n",
      "{'loss': 8.0295, 'learning_rate': 3e-06, 'epoch': 0.05}\n",
      "{'loss': 6.9652, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.07}\n",
      "{'loss': 5.8674, 'learning_rate': 5e-06, 'epoch': 0.09}\n",
      "{'loss': 4.7683, 'learning_rate': 6e-06, 'epoch': 0.11}\n",
      "{'loss': 3.6859, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.12}\n",
      "{'loss': 2.8453, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.14}\n",
      "{'loss': 2.3039, 'learning_rate': 9e-06, 'epoch': 0.16}\n",
      "{'loss': 2.0861, 'learning_rate': 1e-05, 'epoch': 0.18}\n",
      "{'loss': 1.691, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 1.5033, 'learning_rate': 1.2e-05, 'epoch': 0.21}\n",
      "{'loss': 1.2673, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 1.0955, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9426, 'learning_rate': 1.5e-05, 'epoch': 0.27}\n",
      "{'loss': 0.8592, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.28}\n",
      "{'loss': 0.7551, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6877, 'learning_rate': 1.8e-05, 'epoch': 0.32}\n",
      "{'loss': 0.6012, 'learning_rate': 1.9e-05, 'epoch': 0.34}\n",
      "{'loss': 0.5756, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.5403, 'learning_rate': 2.1e-05, 'epoch': 0.37}\n",
      "{'loss': 0.4963, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.39}\n",
      "{'loss': 0.4714, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.41}\n",
      "{'loss': 0.4551, 'learning_rate': 2.4e-05, 'epoch': 0.43}\n",
      "{'loss': 0.444, 'learning_rate': 2.5e-05, 'epoch': 0.44}\n",
      "{'loss': 0.4347, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3977, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3719, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3772, 'learning_rate': 2.9e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3613, 'learning_rate': 3e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3565, 'learning_rate': 3.1e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3548, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3286, 'learning_rate': 3.3e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3293, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.6}\n",
      "{'loss': 0.331, 'learning_rate': 3.5e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3074, 'learning_rate': 3.6e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3039, 'learning_rate': 3.7e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3103, 'learning_rate': 3.8e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2734, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.69}\n",
      "{'loss': 0.301, 'learning_rate': 4e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2868, 'learning_rate': 4.1e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2705, 'learning_rate': 4.2e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2622, 'learning_rate': 4.3e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2579, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2563, 'learning_rate': 4.5e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2353, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.82}\n",
      "{'loss': 0.2353, 'learning_rate': 4.7e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2281, 'learning_rate': 4.8e-05, 'epoch': 0.85}\n",
      "{'loss': 0.2327, 'learning_rate': 4.9e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2156, 'learning_rate': 5e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2044, 'learning_rate': 4.957947855340623e-05, 'epoch': 0.91}\n",
      "{'loss': 0.2235, 'learning_rate': 4.915895710681245e-05, 'epoch': 0.92}\n",
      "{'loss': 0.2048, 'learning_rate': 4.8738435660218675e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2146, 'learning_rate': 4.83179142136249e-05, 'epoch': 0.96}\n",
      "{'loss': 0.1983, 'learning_rate': 4.789739276703112e-05, 'epoch': 0.98}\n",
      "{'loss': 0.1964, 'learning_rate': 4.747687132043734e-05, 'epoch': 0.99}\n",
      "{'loss': 0.192, 'learning_rate': 4.705634987384357e-05, 'epoch': 1.01}\n",
      "{'loss': 0.1826, 'learning_rate': 4.66358284272498e-05, 'epoch': 1.03}\n",
      "{'loss': 0.1824, 'learning_rate': 4.6215306980656014e-05, 'epoch': 1.05}\n",
      "{'loss': 0.1735, 'learning_rate': 4.579478553406224e-05, 'epoch': 1.07}\n",
      "{'loss': 0.1686, 'learning_rate': 4.537426408746846e-05, 'epoch': 1.08}\n",
      "{'loss': 0.1798, 'learning_rate': 4.495374264087469e-05, 'epoch': 1.1}\n",
      "{'loss': 0.1806, 'learning_rate': 4.453322119428091e-05, 'epoch': 1.12}\n",
      "{'loss': 0.1555, 'learning_rate': 4.4112699747687136e-05, 'epoch': 1.14}\n",
      "{'loss': 0.167, 'learning_rate': 4.369217830109335e-05, 'epoch': 1.15}\n",
      "{'loss': 0.1635, 'learning_rate': 4.3271656854499584e-05, 'epoch': 1.17}\n",
      "{'loss': 0.1622, 'learning_rate': 4.285113540790581e-05, 'epoch': 1.19}\n",
      "{'loss': 0.1574, 'learning_rate': 4.2430613961312026e-05, 'epoch': 1.21}\n",
      "{'loss': 0.1565, 'learning_rate': 4.201009251471825e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1562, 'learning_rate': 4.1589571068124474e-05, 'epoch': 1.24}\n",
      "{'loss': 0.1583, 'learning_rate': 4.11690496215307e-05, 'epoch': 1.26}\n",
      "{'loss': 0.1395, 'learning_rate': 4.074852817493692e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1393, 'learning_rate': 4.032800672834315e-05, 'epoch': 1.3}\n",
      "{'loss': 0.1502, 'learning_rate': 3.990748528174937e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1342, 'learning_rate': 3.9486963835155596e-05, 'epoch': 1.33}\n",
      "{'loss': 0.1439, 'learning_rate': 3.906644238856182e-05, 'epoch': 1.35}\n",
      "{'loss': 0.1493, 'learning_rate': 3.8645920941968044e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1278, 'learning_rate': 3.822539949537426e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1351, 'learning_rate': 3.780487804878049e-05, 'epoch': 1.4}\n",
      "{'loss': 0.151, 'learning_rate': 3.738435660218672e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1255, 'learning_rate': 3.6963835155592935e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1351, 'learning_rate': 3.654331370899916e-05, 'epoch': 1.46}\n",
      "{'loss': 0.1245, 'learning_rate': 3.612279226240538e-05, 'epoch': 1.47}\n",
      "{'loss': 0.1285, 'learning_rate': 3.570227081581161e-05, 'epoch': 1.49}\n",
      "{'loss': 0.1322, 'learning_rate': 3.528174936921783e-05, 'epoch': 1.51}\n",
      "{'loss': 0.1342, 'learning_rate': 3.4861227922624056e-05, 'epoch': 1.53}\n",
      "{'loss': 0.1218, 'learning_rate': 3.4440706476030274e-05, 'epoch': 1.55}\n",
      "{'loss': 0.1224, 'learning_rate': 3.4020185029436505e-05, 'epoch': 1.56}\n",
      "{'loss': 0.1031, 'learning_rate': 3.359966358284273e-05, 'epoch': 1.58}\n",
      "{'loss': 0.1275, 'learning_rate': 3.317914213624895e-05, 'epoch': 1.6}\n",
      "{'loss': 0.1074, 'learning_rate': 3.275862068965517e-05, 'epoch': 1.62}\n",
      "{'loss': 0.1142, 'learning_rate': 3.2338099243061395e-05, 'epoch': 1.63}\n",
      "{'loss': 0.1151, 'learning_rate': 3.1917577796467626e-05, 'epoch': 1.65}\n",
      "{'loss': 0.1077, 'learning_rate': 3.1497056349873844e-05, 'epoch': 1.67}\n",
      "{'loss': 0.1038, 'learning_rate': 3.107653490328007e-05, 'epoch': 1.69}\n",
      "{'loss': 0.1194, 'learning_rate': 3.065601345668629e-05, 'epoch': 1.71}\n",
      "{'loss': 0.1063, 'learning_rate': 3.0235492010092513e-05, 'epoch': 1.72}\n",
      "{'loss': 0.1034, 'learning_rate': 2.981497056349874e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0947, 'learning_rate': 2.9394449116904965e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1091, 'learning_rate': 2.8973927670311186e-05, 'epoch': 1.78}\n",
      "{'loss': 0.1049, 'learning_rate': 2.855340622371741e-05, 'epoch': 1.79}\n",
      "{'loss': 0.1101, 'learning_rate': 2.8132884777123634e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0978, 'learning_rate': 2.7712363330529855e-05, 'epoch': 1.83}\n",
      "{'loss': 0.095, 'learning_rate': 2.729184188393608e-05, 'epoch': 1.85}\n",
      "{'loss': 0.1047, 'learning_rate': 2.6871320437342307e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0963, 'learning_rate': 2.645079899074853e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0931, 'learning_rate': 2.6030277544154752e-05, 'epoch': 1.9}\n",
      "{'loss': 0.092, 'learning_rate': 2.5609756097560977e-05, 'epoch': 1.92}\n",
      "{'loss': 0.1106, 'learning_rate': 2.5189234650967204e-05, 'epoch': 1.94}\n",
      "{'loss': 0.1019, 'learning_rate': 2.4768713204373425e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0957, 'learning_rate': 2.434819175777965e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0917, 'learning_rate': 2.392767031118587e-05, 'epoch': 1.99}\n",
      "{'loss': 0.095, 'learning_rate': 2.3507148864592095e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0848, 'learning_rate': 2.308662741799832e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0944, 'learning_rate': 2.266610597140454e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0939, 'learning_rate': 2.2245584524810768e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0847, 'learning_rate': 2.182506307821699e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0847, 'learning_rate': 2.1404541631623216e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0923, 'learning_rate': 2.0984020185029437e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0862, 'learning_rate': 2.056349873843566e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0751, 'learning_rate': 2.0142977291841886e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0806, 'learning_rate': 1.972245584524811e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0752, 'learning_rate': 1.930193439865433e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0861, 'learning_rate': 1.8881412952060555e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0727, 'learning_rate': 1.846089150546678e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0933, 'learning_rate': 1.8040370058873004e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0921, 'learning_rate': 1.7619848612279228e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0828, 'learning_rate': 1.719932716568545e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0697, 'learning_rate': 1.6778805719091676e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0822, 'learning_rate': 1.6358284272497897e-05, 'epoch': 2.31}\n",
      "{'loss': 0.082, 'learning_rate': 1.593776282590412e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0879, 'learning_rate': 1.5517241379310346e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0918, 'learning_rate': 1.5096719932716568e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0849, 'learning_rate': 1.4676198486122794e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0866, 'learning_rate': 1.4255677039529017e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0852, 'learning_rate': 1.383515559293524e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0846, 'learning_rate': 1.3414634146341466e-05, 'epoch': 2.43}\n",
      "{'loss': 0.0792, 'learning_rate': 1.2994112699747688e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0935, 'learning_rate': 1.257359125315391e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0776, 'learning_rate': 1.2153069806560135e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0824, 'learning_rate': 1.1732548359966358e-05, 'epoch': 2.5}\n",
      "{'loss': 0.0821, 'learning_rate': 1.1312026913372582e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0798, 'learning_rate': 1.0891505466778806e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0723, 'learning_rate': 1.047098402018503e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0782, 'learning_rate': 1.0050462573591253e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0785, 'learning_rate': 9.629941126997477e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0811, 'learning_rate': 9.209419680403702e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0704, 'learning_rate': 8.788898233809926e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0807, 'learning_rate': 8.368376787216148e-06, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0857, 'learning_rate': 7.947855340622373e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0727, 'learning_rate': 7.527333894028596e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0701, 'learning_rate': 7.10681244743482e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0757, 'learning_rate': 6.686291000841043e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0811, 'learning_rate': 6.265769554247266e-06, 'epoch': 2.74}\n",
      "{'loss': 0.07, 'learning_rate': 5.845248107653491e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0695, 'learning_rate': 5.424726661059714e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0769, 'learning_rate': 5.004205214465938e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0744, 'learning_rate': 4.583683767872162e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0795, 'learning_rate': 4.163162321278385e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0662, 'learning_rate': 3.7426408746846087e-06, 'epoch': 2.84}\n",
      "{'loss': 0.074, 'learning_rate': 3.322119428090833e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0854, 'learning_rate': 2.9015979814970564e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0787, 'learning_rate': 2.4810765349032802e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0707, 'learning_rate': 2.060555088309504e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0772, 'learning_rate': 1.6400336417157277e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0769, 'learning_rate': 1.2195121951219514e-06, 'epoch': 2.95}\n",
      "{'loss': 0.08, 'learning_rate': 7.98990748528175e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0755, 'learning_rate': 3.7846930193439866e-07, 'epoch': 2.98}\n",
      "{'train_runtime': 878.6912, 'train_samples_per_second': 30.728, 'train_steps_per_second': 1.922, 'train_loss': 0.5159421686663156, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1689, training_loss=0.5159421686663156, metrics={'train_runtime': 878.6912, 'train_samples_per_second': 30.728, 'train_steps_per_second': 1.922, 'train_loss': 0.5159421686663156, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # output directory\n",
    "    num_train_epochs=3,  # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir='./logs',  # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=dataset['train'].map(preprocess_function, batched=True),  # training dataset\n",
    "    eval_dataset=dataset['test'].map(preprocess_function, batched=True),  # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez completado el entrenamiento, vamos a hacer alguna prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hundred one million\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"number to text: 1000\", return_tensors=\"pt\").input_ids.to('mps')\n",
    "outputs = model.generate(input_ids, max_length=50, num_beams=1)\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tama√±o del modelo y recursos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo T5 (Text-to-Text Transfer Transformer), incluido el T5-base, generalmente maneja secuencias de entrada cuya longitud m√°xima predeterminada es de 512 tokens. Este l√≠mite est√° configurado as√≠ en los modelos preentrenados disponibles en la biblioteca Hugging Face Transformers.\n",
    "\n",
    "Veamos cu√°ntos tokens de entrada y salida tiene el modelo T5 Base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o m√°ximo de la entrada: 512 tokens\n",
      "Memoria ocupada por el modelo: 850.31 MB\n",
      "N√∫mero de par√°metros del modelo: 222.9 millones\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tama√±o m√°ximo de la entrada: {tokenizer.model_max_length} tokens\")  # Generalmente ser√° 512\n",
    "\n",
    "# Memoria ocupada por el modelo en MB\n",
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Memoria ocupada por el modelo: {round(model_size * 4 / 1024**2, 2)} MB\")\n",
    "\n",
    "# N√∫mero de par√°metros del modelo en millones\n",
    "print(f\"N√∫mero de par√°metros del modelo: {round(model_size/10**6,2)} millones\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"../imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 12**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Redes neuronales recurrentes (RNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una **RNN** es un tipo de red neuronal diseñada para reconocer patrones en secuencias de datos, como series temporales o textos. Se caracterizan por tener conexiones que retroceden en la estructura de la red, lo que les permite mantener información de estados anteriores en la secuencia.\n",
    "\n",
    "#### **Definición formal:**\n",
    "\n",
    "- $ x_t $: Entrada en el paso de tiempo $ t $.\n",
    "- $ h_t $: Estado oculto en el paso de tiempo $ t $. Es la \"memoria\" de la RNN, que guarda información sobre los pasos anteriores.\n",
    "\n",
    "La relación recurrente es:\n",
    "\n",
    "$$ h_t = \\sigma(W \\cdot [h_{t-1}, x_t] + b) $$\n",
    "\n",
    "Donde:\n",
    "- $ \\sigma $ es una función de activación, comúnmente la función tanh o sigmoide.\n",
    "- $ W $ es la matriz de pesos que opera sobre la concatenación del estado oculto anterior y la entrada actual.\n",
    "- $ b $ es el vector de sesgo.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"imgs/RNN.svg\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Vamos a visualizar la misma red pero desenrollada en el tiempo, es decir, como una red neuronal profunda con una capa por paso de tiempo.\n",
    "<p align=\"center\">\n",
    "<img src=\"imgs/RNNdesenrollada.svg\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "#### **Características:**\n",
    "\n",
    "1. **Memoria a corto plazo**: La información puede ser transmitida de un paso en el tiempo a otro a través del estado oculto $ h_t $, lo que permite a las RNNs mantener una \"memoria\" de los datos anteriores en la secuencia.\n",
    "\n",
    "2. **Desenrollado en el tiempo**: Aunque las RNNs se definen de manera recursiva, a menudo se \"desenrollan\" en el tiempo para su entrenamiento y visualización, tratándolas como una red neuronal profunda con una capa por paso de tiempo.\n",
    "\n",
    "3. **Dificultades en el entrenamiento**: Las RNNs simples tienen dificultades para mantener información a largo plazo debido a los problemas como el desvanecimiento o la explosión del gradiente. Estos problemas son abordados por variantes más avanzadas, como las LSTMs y las GRUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LSTM**\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Una red LSTM, que significa \"Long Short-Term Memory\" o \"Memoria a Largo y Corto Plazo\", es un tipo de red neuronal recurrente (RNN) que se utiliza comúnmente en tareas de procesamiento de secuencias, como el procesamiento de lenguaje natural (NLP), la traducción automática, el reconocimiento de voz, la generación de texto y muchas otras aplicaciones relacionadas con datos secuenciales.\n",
    "\n",
    "La red LSTM se desarrolló para superar algunas de las limitaciones de las RNN tradicionales, que tienen dificultades para manejar dependencias a largo plazo en las secuencias debido a problemas de desvanecimiento de gradientes. La arquitectura LSTM introduce una estructura de celdas con puertas que permite a la red aprender a retener y olvidar información a lo largo del tiempo. Las partes clave de una celda LSTM son:\n",
    "\n",
    "1. **Celda de memoria (Cell State)**: Esta es la memoria a largo plazo de la red y se propaga a lo largo de toda la secuencia. Puede almacenar información relevante de la secuencia y mantenerla a lo largo del tiempo.\n",
    "\n",
    "2. **Puerta del Olvido (Forget Gate)**: Esta puerta decide qué información de la celda de memoria anterior debe olvidarse y qué información debe mantenerse. Es una capa de red neuronal sigmoide que produce valores entre 0 y 1, donde 0 significa \"olvidar completamente\" y 1 significa \"mantener completamente\".\n",
    "\n",
    "3. **Puerta de Entrada (Input Gate)**: La puerta de entrada determina qué nueva información debe ser escrita en la celda de memoria. Similar a la puerta de olvido, consta de una capa sigmoide que produce valores entre 0 y 1 y una capa de activación (típicamente tangente hiperbólica) que genera candidatos de valores para ser escritos en la celda de memoria.\n",
    "\n",
    "4. **Puerta de Salida (Output Gate)**: La puerta de salida decide qué información de la celda de memoria se utilizará en la salida de la celda LSTM. Al igual que las puertas anteriores, consta de una capa sigmoide.\n",
    "\n",
    "El proceso anterior se repite para cada elemento de la secuencia, lo que permite a la red LSTM modelar dependencias a largo plazo y retener información importante a través del tiempo. Las redes LSTM han demostrado ser efectivas para una amplia gama de tareas de aprendizaje profundo que involucran secuencias, y se utilizan ampliamente en la investigación y la industria del aprendizaje automático.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"imgs/LSTM3-chain.png\" width=\"60%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

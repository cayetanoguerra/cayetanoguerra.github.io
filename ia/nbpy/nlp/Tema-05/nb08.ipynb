{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"../imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 18**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/hf-logo-with-title.svg\" width=\"500\">\n",
    "\n",
    "https://huggingface.co/docs/hub/index\n",
    "\n",
    "Hugging Face es una empresa enfocada en inteligencia artificial y aprendizaje automático, conocida principalmente por su biblioteca \"Transformers\" que proporciona modelos preentrenados para procesamiento de lenguaje natural y otras tareas. Ofrecen una plataforma colaborativa, el Hugging Face Hub, donde los usuarios pueden compartir y trabajar en modelos de IA, conjuntos de datos y aplicaciones. La compañía promueve la democratización y accesibilidad de la IA a través de soluciones de código abierto y ciencia abierta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holaaaaa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8676064014434814}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"Yesterday my mother in law died, finally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, este **pipeline** selecciona un modelo preentrenado específico que ha sido ajustado para el análisis de sentimientos en inglés. El modelo se descarga y almacena en caché cuando creas el objeto clasificador. Si vuelves a ejecutar el comando, se utilizará el modelo almacenado en caché en lugar de descargar el modelo de nuevo. Vamos a cargar un modelo preentrenado para el análisis de sentimientos en multi-idoma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '2 stars', 'score': 0.4662598669528961}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_multi = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "classifier_multi(\"No me gusta esa playa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pasos principales cuando pasas algún texto a un pipeline son:\n",
    "\n",
    "1. El texto se preprocesa en un formato que el modelo puede entender.\n",
    "2. Las entradas preprocesadas se pasan al modelo.\n",
    "3. Las predicciones del modelo se post-procesan, para que puedas entenderlas.\n",
    "\n",
    "Algunas de los pipeline disponibles son:\n",
    "\n",
    "- Extracción de la representación vectorial de un texto\n",
    "- Rellenar-enmascaramientos\n",
    "- Reconocimiento de entidades nombradas (NER)\n",
    "- Preguntas y respuestas\n",
    "- Análisis de sentimientos\n",
    "- Resúmenes\n",
    "- Generación de texto\n",
    "- Traducción\n",
    "- Clasificación \"zero-shot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clasificación *zero-shot***\n",
    "\n",
    "Esta tarea puede ser muy desafiante ya que necesitamos clasificar textos que no han sido etiquetados. Este es un escenario común en proyectos del mundo real porque anotar texto suele ser un proceso que consume mucho tiempo y requiere experiencia en el dominio. Para este caso de uso, el *pipeline* de clasificación *zero-shot* es muy útil: te permite especificar qué etiquetas usar para la clasificación, por lo que no tienes que depender de las etiquetas del modelo preentrenado. Ya has visto cómo el modelo puede clasificar una oración como positiva o negativa, pero también puede clasificar el texto usando cualquier otro conjunto de etiquetas que consideres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library in Natural Language Processing',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.867782473564148, 0.09560419619083405, 0.03661326691508293]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library in Natural Language Processing\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reconocimiento de entidades nombradas**\n",
    "\n",
    "El reconocimiento de entidades nombradas (NER) es una tarea donde el modelo tiene que encontrar qué partes del texto de entrada corresponden a entidades como personas, ubicaciones u organizaciones. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9991522,\n",
       "  'word': 'Juan Pérez',\n",
       "  'start': 11,\n",
       "  'end': 21},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.80377835,\n",
       "  'word': 'University of Las Palmas',\n",
       "  'start': 40,\n",
       "  'end': 64},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.7861865,\n",
       "  'word': 'of Gran Canaria',\n",
       "  'start': 65,\n",
       "  'end': 80},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9997646,\n",
       "  'word': 'Spain',\n",
       "  'start': 84,\n",
       "  'end': 89}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Juan Pérez and I work at the University of Las Palmas of Gran Canaria in Spain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preguntas y respuestas**\n",
    "\n",
    "El *pipeline* de preguntas y respuestas responde preguntas utilizando información de un contexto dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344c43ecfa784673913fe7fdfe30f45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9867210984230042, 'start': 99, 'end': 109, 'answer': 'colleagues'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"What are María Pérez and Juan Pérez?\",\n",
    "    context=\"Juan Pérez and María Pérez work at the University of Las Palmas of Gran Canaria in Spain. They are colleagues. María Pérez is 32 years old and Juan Pérez is 45.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resúmenes**\n",
    "\n",
    "El resumen es la tarea de reducir un texto a un texto más corto mientras se mantienen todos (o la mayoría) de los aspectos importantes mencionados en el texto. Aquí hay un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae178c74648d45c2934b3ab97097a6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b6ef20af0c49f283cc557de744061d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1aadbf621f4a5792f67b571590ac0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c82086ab7f64e87a0264a070879bfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ad18e5ae1946dabf5aca0b014733da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Speech to text**\n",
    "\n",
    "Conversión del habla en texto escrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' Bueno, esto es solamente una prueba a ver qué tal funciona el traductor de... bueno, no es un traductor realmente, es recoger el audio del ordenador y traducirlo a texto. Vamos a ver qué tal.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "t2s = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n",
    "t2s(\"data/voz.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Todo en uno: Transformer**\n",
    "\n",
    "Un Transformer es un modelo de aprendizaje profundo que se ha vuelto extremadamente popular en el campo del procesamiento del lenguaje natural (NLP). Fue introducido en el artículo \"Attention is All You Need\" por Vaswani et al. en 2017. Los Transformers eliminan la necesidad de métodos de aprendizaje supervisado previos, como las redes neuronales recurrentes (RNN) o las redes neuronales convolutivas (CNN), al introducir una nueva arquitectura llamada codificador-decodificador. Los Transformers han demostrado ser muy eficaces en una variedad de tareas de NLP, incluido el modelado de lenguaje, la traducción automática y el resumen de texto.\n",
    "\n",
    "El modelo T5 (que veremos en el ejemplo), o Text-to-Text Transfer Transformer, fue presentado en un artículo titulado \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" publicado por investigadores de Google en octubre de 2019. Este trabajo introdujo el modelo T5 junto con su enfoque innovador para manejar todas las tareas de procesamiento de lenguaje natural (NLP) como problemas de generación de texto, utilizando un formato de texto a texto.\n",
    "\n",
    "\n",
    "`T5ForConditionalGeneration` es una clase en la biblioteca transformers de Hugging Face que proporciona una implementación del modelo T5 diseñada específicamente para tareas de generación de texto, como traducción de idiomas, resumen de texto, y generación de texto basada en prompts. La idea detrás de T5 es tratar todos los problemas de procesamiento de lenguaje natural como problemas de generación de texto. Por ejemplo, en lugar de tener una arquitectura específica para traducción y otra diferente para resumen, T5 puede ser entrenado para realizar ambas tareas (y muchas otras) simplemente cambiando el texto de entrada que se le proporciona. Esto se logra prefijando el texto de entrada con un indicador de la tarea a realizar, como \"traducir del inglés al alemán\" o \"resumir\", seguido por el texto a procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Traducción**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def translate(text, model_name=\"t5-base\", task=\"translate English to French\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Ejemplo de traducción del inglés al francés\n",
    "text_en_to_es = \"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr.\"\n",
    "print(\"Inglés a Francés:\", translate(text_en_to_es, task=\"translate English to French\"))\n",
    "text_en_to_es = \"They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\"\n",
    "print(\"Inglés a Francés:\", translate(text_en_to_es, task=\"translate English to French\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generación de resúmenes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumarize(text, model_name=\"t5-base\", task=\"summarize\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return summarized_text\n",
    "\n",
    "# Ejemplo de resumen\n",
    "text = \"\"\"\n",
    "\"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr. They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\n",
    "\"\"\"\n",
    "print(\"Resumen:\", sumarize(text, task=\"summarize\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./models/mistral-7b-instruct-v0.2.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.86 GiB (4.57 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3877.58 MiB, ( 3877.64 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3877.57 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  4096.00 MiB, ( 7975.45 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  2144.02 MiB, (10119.47 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =  2144.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    72.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path=\"./models/mistral-7b-instruct-v0.2.Q4_K_S.gguf\",  # Download the model file first\n",
    "  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6059.83 ms\n",
      "llama_print_timings:      sample time =      10.46 ms /    70 runs   (    0.15 ms per token,  6693.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6058.88 ms /    97 tokens (   62.46 ms per token,    16.01 tokens per second)\n",
      "llama_print_timings:        eval time =    2315.57 ms /    69 runs   (   33.56 ms per token,    29.80 tokens per second)\n",
      "llama_print_timings:       total time =    8564.91 ms /   166 tokens\n"
     ]
    }
   ],
   "source": [
    "# Simple inference example\n",
    "output = llm(\n",
    "  \"<s>[INST] Juan y Ana están en la misma habitación. Juan coloca una manzana dentro de un cajón y se va de la habitación. Cuando Ana sabe que Juan no puede verla, coge la manzana y la esconde bajo la cama. Ahora llega Juan a la habitación, ¿dónde creerá Ana que Juan buscará la manzana? [/INST]\", # Prompt\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-b461dba8-b8e7-4664-9c2f-4a5058d4d064',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1730723498,\n",
       " 'model': './models/mistral-7b-instruct-v0.2.Q4_K_S.gguf',\n",
       " 'choices': [{'text': '<s>[INST] Juan y Ana están en la misma habitación. Juan coloca una manzana dentro de un cajón y se va de la habitación. Cuando Ana sabe que Juan no puede verla, coge la manzana y la esconde bajo la cama. Ahora llega Juan a la habitación, ¿dónde creerá Ana que Juan buscará la manzana? [/INST] Ana creería que Juan buscará la manzana en el cajón, dado que es donde ella había visto que Juan la había colocado antes de salir de la habitación. Sin embargo, sabemos que en realidad la manzana se encuentra oculta debajo de la cama.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 97, 'completion_tokens': 69, 'total_tokens': 166}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hugging Face Cheat Sheet**\n",
    "\n",
    "Principales clases y métodos de la biblioteca Hugging Face Transformers, enfocándonos en los aspectos más comunes utilizados para el manejo de modelos, tokenización, entrenamiento, y más. Este esquema te ayudará a entender mejor cómo interactúan estas clases y métodos dentro del ecosistema de Hugging Face.\n",
    "\n",
    "### 1. **Modelos**\n",
    "\n",
    "- **`AutoModel`**\n",
    "  - Métodos:\n",
    "    - `from_pretrained('model-name')`: Carga un modelo preentrenado basado en el nombre.\n",
    "    - `save_pretrained('directory')`: Guarda el modelo en un directorio especificado.\n",
    "  - Uso: Para cargar o guardar modelos de manera genérica sin especificar la arquitectura exacta.\n",
    "\n",
    "- **`AutoModelForSequenceClassification`**, **`AutoModelForTokenClassification`**, **`AutoModelForQuestionAnswering`**\n",
    "  - Uso: Clases especializadas para tareas específicas como clasificación de secuencias, clasificación de tokens y pregunta-respuesta, respectivamente.\n",
    "  - Permiten cargar modelos con cabezales de predicción adecuados para cada tipo de tarea.\n",
    "\n",
    "### 2. **Tokenización**\n",
    "\n",
    "- **`AutoTokenizer`**\n",
    "  - Métodos:\n",
    "    - `from_pretrained('model-name')`: Carga un tokenizador que está alineado con un modelo preentrenado.\n",
    "    - `__call__(texts)`: Aplica la tokenización a uno o más textos.\n",
    "  - Uso: Para tokenizar textos de manera que sean compatibles con los modelos de Transformers.\n",
    "\n",
    "### 3. **Entrenamiento y Evaluación**\n",
    "\n",
    "- **`Trainer`**\n",
    "  - Métodos:\n",
    "    - `train()`: Comienza el entrenamiento del modelo.\n",
    "    - `evaluate()`: Evalúa el modelo en un conjunto de datos de prueba o validación.\n",
    "    - `predict(test_dataset)`: Realiza predicciones sobre un conjunto de datos no visto.\n",
    "  - Uso: Proporciona una manera simple y eficiente para entrenar y evaluar modelos de Transformers.\n",
    "\n",
    "- **`TrainingArguments`**\n",
    "  - Uso: Configura argumentos para el entrenamiento como la ubicación de los logs, tamaño de batch, número de epochs, etc.\n",
    "\n",
    "### 4. **Clases de Datasets**\n",
    "\n",
    "- **`Dataset` y `DatasetDict` de la biblioteca `datasets`**\n",
    "  - Métodos:\n",
    "    - `from_dict(data)`: Crea un dataset desde un diccionario de datos.\n",
    "    - `map(function)`: Aplica una función a todos los elementos del dataset.\n",
    "    - `train_test_split()`: Divide el dataset en subconjuntos de entrenamiento y prueba.\n",
    "  - Uso: Para manipular y preparar datos para el entrenamiento o evaluación.\n",
    "\n",
    "### 5. **Data Collators**\n",
    "\n",
    "- **`DataCollatorWithPadding`**\n",
    "  - Uso: Prepara batches de datos, asegurando que todos los inputs tengan la misma longitud mediante padding.\n",
    "  - Se pasa como argumento a `DataLoader` para manejar el batching de manera eficiente.\n",
    "\n",
    "### 6. **Utility Classes**\n",
    "\n",
    "- **`pipeline`**\n",
    "  - Uso: Facilita la ejecución de tareas comunes como clasificación, generación de texto, o análisis de sentimiento, encapsulando los procesos de tokenización, ejecución del modelo, y post-procesamiento.\n",
    "  - Ejemplo: `pipeline('sentiment-analysis', model='model-name')`\n",
    "\n",
    "Este esquema no cubre todas las funcionalidades y clases disponibles en Hugging Face Transformers, pero proporciona una visión general de las más comunes y útiles para la mayoría de las tareas de procesamiento de lenguaje natural y otras aplicaciones de inteligencia artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Clase/Método                            | Descripción                                                                                       | Métodos Principales y Uso                                                                                                  |\n",
    "|-----------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|\n",
    "| **AutoModel**                           | Clase genérica para cargar modelos preentrenados.                                                 | `from_pretrained('model-name')`: Carga un modelo. <br> `save_pretrained('directory')`: Guarda el modelo.                  |\n",
    "| **AutoModelFor...**                     | Clases especializadas para tareas específicas (clasificación, QA, etc.).                          | `AutoModelForSequenceClassification`,<br>`AutoModelForTokenClassification`,<br> `AutoModelForQuestionAnswering`,<br>`AutoModelForSeq2SeqLM`,<br>`AutoModelForTranslation`,<br>`AutoModelForMultiLabelClassification`,<br>`AutoModelForCausalLM`                 |\n",
    "| **AutoTokenizer**                       | Clase para cargar tokenizadores que corresponden a modelos preentrenados.                         | `from_pretrained('model-name')`: Carga un tokenizador. <br> `__call__(texts)`: Tokeniza texto.                            |\n",
    "| **Trainer**                             | Facilita el entrenamiento y evaluación de modelos de Transformers.                                | `train()`: Inicia el entrenamiento. <br> `evaluate()`: Evalúa el modelo. <br> `predict(test_dataset)`: Predice datos.     |\n",
    "| **TrainingArguments**                   | Configura parámetros para el entrenamiento con `Trainer`.                                         | Configura argumentos como tamaño del batch, epochs, directorio de salida, etc.                                            |\n",
    "| **Dataset & DatasetDict**               | Clases para manejar datasets, facilitando operaciones como map, split, etc.                       | `from_dict(data)`: Crea datasets. <br> `map(function)`: Aplica funciones. <br> `train_test_split()`: Divide datasets.    |\n",
    "| **DataCollatorWithPadding**             | Prepara batches asegurando que todos los inputs tengan la misma longitud mediante padding.        | Se utiliza como argumento en `DataLoader` para manejar la creación de batches.                                            |\n",
    "| **pipeline**                            | Facilita la ejecución de pipelines de procesamiento para tareas comunes como análisis de texto.   | `pipeline('task-name', model='model-name')`: Crea y ejecuta un pipeline para tareas como `sentiment-analysis`.            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la biblioteca Hugging Face Transformers, además de las clases que comienzan con \"Auto\", existen muchas otras clases diseñadas para cargar modelos específicos de manera directa. Estas clases están diseñadas para trabajar con una arquitectura de modelo particular y requieren que el usuario sepa exactamente con qué modelo está trabajando. Algunas de estas clases son:\n",
    "\n",
    "1. **Model Classes específicas**: Por ejemplo, `BertModel`, `GPT2Model`, `T5Model`, `RobertaModel`, entre otros. Cada una de estas clases está diseñada para cargar y trabajar con la arquitectura específica del modelo mencionado. Estas clases permiten un control más detallado sobre la configuración y la utilización del modelo.\n",
    "\n",
    "2. **Tokenizer Classes**: Como `BertTokenizer`, `GPT2Tokenizer`, etc., que son específicos para los modelos para los que están diseñados. Estos tokenizadores preparan los datos de entrada de manera que sean adecuados para su modelo correspondiente.\n",
    "\n",
    "3. **Pipeline Classes**: Clases como `TextClassificationPipeline`, `TranslationPipeline`, entre otros, que no solo cargan modelos y tokenizadores, sino que también encapsulan el flujo completo de procesamiento para tareas específicas.\n",
    "\n",
    "4. **Modelos para tareas específicas**: Clases como `BertForSequenceClassification`, `GPT2LMHeadModel`, etc., que son variantes de los modelos base adaptados para tareas particulares como clasificación de secuencias, generación de lenguaje, etc.\n",
    "\n",
    "La elección entre usar una clase específica del modelo o una clase \"Auto\" depende de tus necesidades:\n",
    "- **Uso de clases específicas**: Si sabes exactamente con qué modelo estás trabajando y necesitas un control fino sobre la configuración y el comportamiento del modelo, usar una clase específica puede ser beneficioso.\n",
    "- **Uso de clases \"Auto\"**: Si prefieres escribir código más genérico que pueda adaptarse fácilmente a diferentes modelos o si estás explorando diferentes modelos, usar una clase \"Auto\" puede ser más conveniente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

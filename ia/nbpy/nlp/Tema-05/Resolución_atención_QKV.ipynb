{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Atención Q, K y V**\n",
    "\n",
    "Con el objetivo de entender con detalle el funcionamiento de la atención en transformers, implementaremos una versión simplificada de la atención con los vectores Q, K y V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "\n",
      "Score:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3000, 0.6000, 0.9000, 1.2000],\n",
      "        [0.0600, 0.1200, 0.1800, 0.2400],\n",
      "        [0.0900, 0.1800, 0.2700, 0.3600]])\n",
      "\n",
      "Resultado:\n",
      " tensor([[0.2500, 0.5000, 0.5000],\n",
      "        [0.1892, 0.5432, 0.5857],\n",
      "        [0.2372, 0.5087, 0.5173],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Supongamos que tenemos los siguiente vectores Q, K y V\n",
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])\n",
    "\n",
    "print(Q.shape)\n",
    "print(K.shape)\n",
    "print(V.shape)\n",
    "\n",
    "score = Q @ K.transpose(0, 1)  # @ equivale a la multiplicación de matrices\n",
    "\n",
    "print(\"\\nScore:\\n\", score)\n",
    "\n",
    "score = score / torch.sqrt(torch.tensor(K.shape[1]).float())  # Dividimos por la raíz cuadrada de la dimensión de K\n",
    "score = torch.softmax(score, dim=1)\n",
    "\n",
    "Z = score @ V  \n",
    "\n",
    "print(\"\\nResultado:\\n\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Todo en uno: scaled_dot_product_attention**\n",
    "\n",
    "Todo el cálculo anterior lo realiza eficientemente una función de PyTorch llamada **scaled_dot_product_attention()**. Esta función calcula la atención en los tensores de consulta (query), clave (key) y valor (value), utilizando una máscara de atención opcional si se proporciona, y aplicando *dropout* si se especifica una probabilidad mayor a 0.0.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.5000, 0.5000],\n",
      "        [0.1892, 0.5432, 0.5857],\n",
      "        [0.2372, 0.5087, 0.5173],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "Z = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Atención Q, K y V con enmascaramiento**\n",
    "\n",
    "El enmascaramiento durante la etapa del decodificador en los modelos Transformer es crucial para evitar que el decodificador tenga acceso a información futura, especialmente en tareas de generación secuencial como la traducción automática o la generación de texto. Este concepto se conoce como \"enmascaramiento de atención causal\".\n",
    "\n",
    "En el contexto de los Transformers, el decodificador genera una salida secuencialmente, palabra por palabra. Durante la generación de cada palabra, es importante que el modelo solo tenga en cuenta las palabras anteriores y no las futuras, ya que estas últimas no deberían estar disponibles (en un escenario de generación de texto, por ejemplo, las palabras futuras aún no se han generado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1732, 0.3464, 0.5196, 0.6928],\n",
      "        [0.0346, 0.0693, 0.1039, 0.1386],\n",
      "        [0.0520, 0.1039, 0.1559, 0.2078]])\n",
      "tensor([[0.0000,   -inf,   -inf,   -inf],\n",
      "        [0.1732, 0.3464,   -inf,   -inf],\n",
      "        [0.0346, 0.0693, 0.1039,   -inf],\n",
      "        [0.0520, 0.1039, 0.1559, 0.2078]])\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])\n",
    "\n",
    "score = Q @ K.transpose(0, 1)\n",
    "#print(score)\n",
    "score = score / torch.sqrt(torch.tensor(K.shape[1]).float())\n",
    "print(score)\n",
    "\n",
    "tril = torch.tril(torch.ones(score.shape[0], score.shape[0]))  # Creamos la máscara\n",
    "score_masked = score.masked_fill(tril == 0, float('-inf'))  # Aplicamos la máscara al score. Todo lo que sea 0 en la máscara, lo reemplazamos por -inf para que al aplicar la softmax, se vuelva 0\n",
    "print(score_masked)\n",
    "score_masked = torch.softmax(score_masked, dim=1)\n",
    "Z = score_masked @ V  \n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos de nuevo que podemos hacer lo mismo con la función **torch.nn.functional.scaled_dot_product_attention()** estableciendo el parámetro **is_causal** a True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "Z = scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "\n",
    "print(Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

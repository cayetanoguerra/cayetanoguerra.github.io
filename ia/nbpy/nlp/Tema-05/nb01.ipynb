{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aprendizaje supervisado en clasificación de texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representación vectorial del texto**\n",
    "\n",
    "La representación vectorial del texto usando redes neuronales han revolucionado el campo del procesamiento de lenguaje natural en la última década. Estas representaciones, comúnmente conocidas como \"embeddings\", capturan relaciones semánticas y sintácticas entre palabras, frases o incluso documentos completos en un espacio de menor dimensión. Veamos los modelos más destacados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word2Vec (Mikolov et al., 2013)**\n",
    "\n",
    "Word2Vec es uno de los modelos más populares para aprender representaciones vectoriales de palabras, y su introducción marcó un antes y un después en la forma en que se abordan muchas tareas de NLP. Fue propuesto por Mikolov et al. en 2013. Se basa en la idea de que el significado o función de una palabra puede inferirse por el contexto en el que aparece frecuentemente. Por lo tanto, palabras con contextos similares tendrán representaciones vectoriales similares. Word2Vec tiene dos arquitecturas principales: CBOW (Continuous Bag of Words) y Skip-Gram.\n",
    "\n",
    "#### **CBOW**\n",
    "\n",
    "CBOW, o Continuous Bag of Words, es una técnica que busca capturar la \"esencia\" de las palabras a través de representaciones vectoriales. Supongamos que contamos con un gigantesto corpus de texto, a medida que lo recorremos, nos fijamos en cada palabra y una determinada ventana de palabras que la rodean, digamos, cinco en total. Por ejemplo, de la frase \"Me apetece una taza de chocolate y unas galletas\", si nos centramos en la palabra \"chocolate\", nuestra ventana captura \"taza\" y \"de\" a la izquierda, y \"y\" y \"unas\" a la derecha.\n",
    "\n",
    "Si nuestro corpus es lo suficientemente amplio, es probable que encontremos otras frases similares a la anterior, como \"Me tomaría una taza de té y unas pastas\".\n",
    "\n",
    "La magia del CBOW radica en su capacidad para representar vectorialmente la palabra central, \"chocolate\" en este caso, basándose exclusivamente en esas palabras circundantes. Lo mismo ocurre con \"té\" en la segunda frase. A medida que CBOW repite este cómputo a lo largo de todo el texto, ajusta sus representaciones para cada palabra. Esto significa que palabras que frecuentemente comparten un contexto similar, como \"chocolate\" y \"té\", adquieren vectores que son similares entre sí.\n",
    "\n",
    "Al concluir este proceso, lo que obtenemos son representaciones numéricas, o vectores, para cada palabra que reflejan su significado y relaciones en el lenguaje. Es una forma de traducir el vocabulario de nuestra lengua a un lenguaje matemático, que luego puede ser empleado en una variedad de aplicaciones de procesamiento del lenguaje natural, desde traducciones hasta búsquedas contextuales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 1\n",
    "\n",
    "Haz tal o cual cosa.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aprendizaje supervisado en clasificación de texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representación vectorial del texto**\n",
    "\n",
    "\n",
    ">Dime con quién andas y te diré quién eres.\n",
    "\n",
    "\n",
    "La representación vectorial del texto usando redes neuronales han revolucionado el campo del procesamiento de lenguaje natural en la última década. Estas representaciones, comúnmente conocidas como \"embeddings\", capturan relaciones semánticas y sintácticas entre palabras, frases o incluso documentos completos en un espacio de menor dimensión. Veamos los modelos más destacados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word2Vec (Mikolov et al., 2013)**\n",
    "\n",
    "Word2Vec es uno de los modelos más populares para aprender representaciones vectoriales de palabras, y su introducción marcó un antes y un después en la forma en que se abordan muchas tareas de NLP. Fue propuesto por Mikolov et al. en 2013. Se basa en la idea de que el significado o función de una palabra puede inferirse por el contexto en el que aparece frecuentemente. Por lo tanto, palabras con contextos similares tendrán representaciones vectoriales similares. Word2Vec tiene dos arquitecturas principales: Skip-Gram y CBOW (Continuous Bag of Words).\n",
    "\n",
    "\n",
    "#### **Skip-Gram**\n",
    "\n",
    "Dada una palabra objetivo, el modelo Skip-Gram trata de predecir las palabras de su contexto cercano. Por ejemplo, en la frase \"El color azul me gusta más que el rojo\", si \"azul\" es nuestra palabra objetivo y utilizamos un tamaño de ventana de contexto de 2, querríamos predecir las palabras \"El\", \"color\", \"me\" y \"gusta\" a partir de \"azul\". Si en otra frase del dataset apareciera la frase \"El color rojo me gusta más que el azul\", el modelo aprendería que \"azul\" y \"rojo\" son palabras que podrían compartir contextos similares y, por tanto, sus vectores tenderían a tener una representación similar para poder predecir correctamente las palabras de su contexto.\n",
    "\n",
    "<img src=\"imgs/Skip-Gram.svg\">\n",
    "\n",
    "\n",
    "#### **CBOW**\n",
    "\n",
    "Dado un contexto, CBOW trata de predecir la palabra objetivo o central. Por ejemplo, en la frase \"El color azul me gusta más que el rojo\", si tomamos un tamaño de ventana de contexto de 2 alrededor de la palabra \"me\", las palabras de contexto serían \"color\", \"azul\", \"más\" y \"que\", y CBOW intentaría predecir \"rojo\" a partir de estas palabras. A diferencia de Skip-Gram, en CBOW todas las palabras de contexto se suman o promedian, de ahí el nombre \"Bag of Words\" ya que no se considera el orden de las palabras.\n",
    "\n",
    "<img src=\"imgs/CBOW.svg\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### **Código**\n",
    "https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

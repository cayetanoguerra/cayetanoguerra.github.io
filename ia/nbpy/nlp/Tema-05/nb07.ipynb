{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"../imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 17**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformers**\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro\n",
    "\n",
    "\n",
    "Los modelos Transformer son una clase de arquitecturas de aprendizaje profundo que han revolucionado el campo del procesamiento del lenguaje natural (NLP) y más allá. Introducidos en el paper \"Attention Is All You Need\" por Vaswani et al. en 2017, los Transformers se destacan por su capacidad para manejar secuencias de datos, como texto, de una manera muy eficiente y efectiva. A diferencia de las arquitecturas anteriores como las redes neuronales recurrentes (RNN) y las redes neuronales convolucionales (CNN), los Transformers se basan principalmente en un mecanismo llamado \"atención\", especialmente la \"atención de múltiples cabezas\".\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"imgs/Transformer.png\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "Aquí hay algunos puntos clave sobre los modelos Transformer:\n",
    "\n",
    "1. **Mecanismo de Atención**: El componente central de un Transformer es el mecanismo de atención, que permite al modelo ponderar la importancia de diferentes partes de la entrada. En el contexto del NLP, esto significa que un Transformer puede prestar más atención a palabras relevantes y menos a las irrelevantes al procesar un texto.\n",
    "\n",
    "2. **Atención de Múltiples Cabezas**: Los Transformers utilizan lo que se llama \"atención de múltiples cabezas\". Esto les permite prestar atención a diferentes partes de la secuencia de entrada simultáneamente, lo que mejora la capacidad del modelo para aprender relaciones complejas.\n",
    "\n",
    "3. **Arquitectura de Codificador y Decodificador**: Los Transformers originales se componen de una serie de bloques de codificadores y decodificadores. El codificador procesa la entrada (por ejemplo, un texto en un idioma fuente para la traducción automática), y el decodificador genera la salida (por ejemplo, el texto traducido). Cada bloque de codificador y decodificador contiene capas de atención y redes neuronales feed-forward.\n",
    "\n",
    "4. **Paralelización y Escalabilidad**: A diferencia de las RNN, los Transformers no requieren que los datos se procesen secuencialmente. Esto significa que pueden manejar secuencias completas de datos a la vez, lo que permite una paralelización masiva y hace que los Transformers sean particularmente adecuados para el hardware moderno de GPU.\n",
    "\n",
    "5. **Aplicaciones en NLP y Más Allá**: Aunque los Transformers fueron diseñados originalmente para tareas de NLP como la traducción automática, su aplicación se ha extendido a una amplia gama de tareas, incluyendo la generación de texto (como GPT-3), el entendimiento del lenguaje (como BERT), el procesamiento de imágenes, y más.\n",
    "\n",
    "6. **Transferencia y Aprendizaje Pre-entrenado**: Modelos como BERT y GPT-3 son pre-entrenados en grandes cantidades de datos y luego afinados en tareas específicas, lo que les permite alcanzar un rendimiento destacado en una amplia gama de tareas de NLP con relativamente poco esfuerzo de ajuste específico de la tarea.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mecanismo de auto-atención del modelo Transformer**\n",
    "\n",
    "El mecanismo de autoatención con enmascaramiento en el Transformer permite que cada token en una secuencia determine cuánto debe “atender” a otros tokens en esa secuencia. La imagen siguiente ilustra este proceso paso a paso. A continuación, se describe cada etapa del mecanismo de autoatención con enmascaramiento:\n",
    "\n",
    "#### **Paso 1: Cálculo de las puntuaciones de atención**\n",
    "\n",
    "**Matrices de consultas (Q), claves (K) y valores (V):**\n",
    "\n",
    "- Cada token en la secuencia se representa mediante un vector, y se generan tres matrices para cada token: Q (consultas), K (claves) y V (valores). En la imagen, tenemos cuatro tokens: q1, q2, q3, q4 en Q y k1, k2, k3, k4 en K.\n",
    "- Estas matrices se utilizan para determinar la relevancia entre los tokens en la secuencia.\n",
    "\n",
    "**Multiplicación de matrices:**\n",
    "- La matriz de consultas (Q) se multiplica con la matriz de claves (K) para obtener una matriz de puntuaciones de atención (scores).\n",
    "- Cada elemento de la matriz de scores representa la relevancia entre un par de tokens: por ejemplo, el valor en la posición (q1, k2) representa la importancia de k2 respecto a q1.\n",
    "\n",
    "#### **Paso 2: Escalado de las puntuaciones de atención**\n",
    "\n",
    "- Para estabilizar los valores, se divide cada puntuación por la raíz cuadrada de la dimensión de los vectores K (normalmente denotada como $\\sqrt{d_k}$). Esto evita que los valores sean excesivamente grandes, lo cual podría dificultar la convergencia del modelo.\n",
    "\n",
    "#### **Paso 3: Aplicación de la matriz de enmascaramiento**\n",
    "\n",
    "**Matriz de enmascaramiento:**\n",
    "\n",
    "- En modelos como GPT, se utiliza una matriz de enmascaramiento causal para garantizar que cada token solo pueda “ver” los tokens anteriores (incluyendo el actual), evitando que el modelo acceda a tokens futuros.\n",
    "- En la imagen, vemos una matriz de enmascaramiento triangular que enmascara los valores superiores de la matriz de scores, donde solo se permiten los valores de la diagonal y los elementos inferiores. Los valores enmascarados se establecen en cero para evitar que el modelo los considere.\n",
    "\n",
    "**Aplicación de la máscara:**\n",
    "- La matriz de enmascaramiento se multiplica con la matriz de puntuaciones escaladas. Los valores enmascarados se convierten en cero, indicando que esos tokens no deben ser tenidos en cuenta en el cálculo de atención.\n",
    "\n",
    "#### **Paso 4: Aplicación de la función Softmax**\n",
    "\n",
    "- Después de enmascarar los valores, se aplica la función softmax en cada fila de la matriz resultante.\n",
    "- La función softmax convierte cada fila en una distribución de probabilidad, resaltando los tokens más relevantes en la atención y atenuando los tokens menos relevantes.\n",
    "- Esto permite que cada token asigne mayor “atención” a los tokens relevantes en función de su contexto en la secuencia.\n",
    "\n",
    "#### **Paso 5: Multiplicación con la matriz de valores (V)**\n",
    "\n",
    "**Cálculo del vector de salida:**\n",
    "- La matriz de probabilidades obtenida después de la softmax se multiplica con la matriz de valores (V) para obtener el vector de atención final para cada token.\n",
    "- En la imagen, los vectores resultantes z1, z2, z3, z4 corresponden a los valores ponderados de cada token según su relevancia.\n",
    "\n",
    "**Resultado final:**\n",
    "- Los vectores z1, z2, z3, z4 representan la salida de la capa de atención para cada token. Estos vectores contienen información agregada sobre los tokens anteriores en la secuencia, ajustada según la relevancia calculada.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./imgs/Attention_QKV_mask.svg\" width=\"70%\">\n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Práctica: implementación del mecanismo de auto-atención con enmascaramiento del modelo Transformer**\n",
    "\n",
    "Vamos a implementar el mecanismo de auto-atención con enmascaramiento del modelo Transformer en Pytorch. Para ello, vamos a seguir los pasos descritos anteriormente y suponer que ya tenemos las matrices de consultas (Q), claves (K) y valores (V) para cada token en la secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El enmascaramiento durante la etapa del decodificador en los modelos Transformer es crucial para evitar que el decodificador tenga acceso a información futura, especialmente en tareas de generación secuencial como la traducción automática o la generación de texto. Este concepto se conoce como \"enmascaramiento de atención causal\".\n",
    "\n",
    "En el contexto de los Transformers, el decodificador genera una salida secuencialmente, palabra por palabra. Durante la generación de cada palabra, es importante que el modelo solo tenga en cuenta las palabras anteriores y no las futuras, ya que estas últimas no deberían estar disponibles (en un escenario de generación de texto, por ejemplo, las palabras futuras aún no se han generado).\n",
    "\n",
    "Una vez realizado el resultado debe ser:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><b>z1</b></td><td>1.0000</td><td>0.0000</td><td>0.0000</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>z2</b></td><td>0.4568</td><td>0.5432</td><td>0.0000</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>z3</b></td><td>0.3219</td><td>0.3332</td><td>0.3449</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>z4</b></td><td>0.2309</td><td>0.5130</td><td>0.5260</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### **Objetivos de la práctica**\n",
    "\n",
    "- Entender con detalle el funcionamiento del mecanismo de auto-atención con enmascaramiento.\n",
    "- Practicar las operaciones matriciales en PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer Normalization**\n",
    "\n",
    "\n",
    "\"Layer Normalization\" es una técnica de normalización utilizada en el entrenamiento de redes neuronales profundas, introducida por Jimmy Lei Ba, Jamie Ryan Kiros y Geoffrey Hinton en 2016 (https://arxiv.org/pdf/1607.06450.pdf). Esta técnica es fundamental para mejorar la estabilidad y acelerar el entrenamiento de modelos de aprendizaje profundo, especialmente en el contexto de redes neuronales recurrentes (RNN) y modelos de atención como los Transformers.\n",
    "\n",
    "### Conceptos clave de la Layer Normalization:\n",
    "\n",
    "1. **Normalización por Ejemplo**: La Layer Normalization normaliza los datos a lo largo de las características para cada ejemplo individual en un lote. Esto significa que, para cada dato en el lote, la normalización se realiza calculando la media y la varianza de todas sus características.\n",
    "\n",
    "2. **Cálculo de la Media y la Varianza**: Para cada ejemplo, se calcula la media y la varianza de todas sus características. Estos valores se utilizan para normalizar las características de ese ejemplo específico.\n",
    "\n",
    "3. **Proceso de Normalización**: La normalización se realiza restando la media y dividiendo por la desviación estándar (raíz cuadrada de la varianza) de las características de cada ejemplo. Se añade un pequeño valor, conocido como epsilon, al denominador para evitar la división por cero.\n",
    "\n",
    "4. **Parámetros Aprendibles**: Después de normalizar las características, se aplican dos parámetros aprendibles a cada una de ellas: un factor de escala (gamma) y un término de desplazamiento (beta). Estos parámetros son específicos para cada capa de la red y se ajustan durante el proceso de entrenamiento.\n",
    "\n",
    "5. **Aplicaciones en Modelos de Secuencia**: La Layer Normalization ha demostrado ser particularmente efectiva en modelos que procesan datos secuenciales, como las redes neuronales recurrentes y los modelos basados en el mecanismo de atención, donde ayuda a mejorar la estabilidad y eficiencia del entrenamiento.\n",
    "\n",
    "### Ventajas de la Layer Normalization:\n",
    "\n",
    "- **Estabilidad en el Entrenamiento**: Contribuye a la estabilidad del entrenamiento de redes neuronales profundas, lo que es crucial para lograr un buen rendimiento en tareas complejas.\n",
    "- **Eficiencia en Modelos de Secuencia**: Facilita el entrenamiento eficiente de modelos que trabajan con datos secuenciales, mejorando tanto la velocidad de convergencia como la calidad del modelo entrenado.\n",
    "- **Flexibilidad en el Tamaño del Lote**: Al normalizar cada ejemplo de forma independiente, la Layer Normalization no depende del tamaño del lote, lo que la hace flexible para diferentes configuraciones de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación matemática:\n",
    "\n",
    "La Layer Normalization se centra en normalizar las activaciones dentro de una capa para cada ejemplo de datos. A continuación se describe el proceso matemáticamente:\n",
    "\n",
    "Dado un tensor de entrada $ X $ para una capa específica en la red, donde $ X $ tiene dimensiones $[N, F]$ (con $ N $ siendo el tamaño del lote y $ F $ el número de características o neuronas en la capa), la Layer Normalization se realiza de la siguiente manera para cada ejemplo $ n $ en el lote:\n",
    "\n",
    "1. **Calcular la Media**:\n",
    "   $$ \\mu_n = \\frac{1}{F} \\sum_{f=1}^{F} x_{nf} $$\n",
    "   Aquí, $ \\mu_n $ es la media de las activaciones para el ejemplo $ n $.\n",
    "\n",
    "2. **Calcular la Varianza**:\n",
    "   $$ \\sigma_n^2 = \\frac{1}{F} \\sum_{f=1}^{F} (x_{nf} - \\mu_n)^2 $$\n",
    "   Donde $ \\sigma_n^2 $ es la varianza de las activaciones para el ejemplo $ n $.\n",
    "\n",
    "3. **Normalizar**:\n",
    "   $$ \\hat{x}_{nf} = \\frac{x_{nf} - \\mu_n}{\\sqrt{\\sigma_n^2 + \\epsilon}} $$\n",
    "   Cada activación $ x_{nf} $ se normaliza restando la media $ \\mu_n $ y dividiendo por la raíz cuadrada de la varianza $ \\sigma_n^2 $, con un pequeño número $ \\epsilon $ añadido para la estabilidad numérica (evitar la división por cero).\n",
    "\n",
    "4. **Aplicar Parámetros Aprendibles**:\n",
    "   $$ y_{nf} = \\gamma \\hat{x}_{nf} + \\beta $$\n",
    "   Finalmente, se aplica una transformación lineal a las activaciones normalizadas, donde $ \\gamma $ y $ \\beta $ son parámetros aprendibles específicos de la capa. Estos parámetros permiten que la normalización se ajuste de manera flexible durante el entrenamiento.\n",
    "\n",
    "En esta formulación, $ n $ indexa los ejemplos en el lote y $ f $ indexa las características. Cada ejemplo se normaliza de forma independiente. Los parámetros $ \\gamma $ y $ \\beta $ son de la misma dimensión que el número de características $ F $, permitiendo una escala y un desplazamiento distintos para cada característica.\n",
    "\n",
    "La Layer Normalization es especialmente útil en modelos de secuencias como los Transformers y las RNN, donde normaliza las activaciones a lo largo de las características para cada paso de tiempo o posición en la secuencia, contribuyendo a un entrenamiento más estable y eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio: Implementación de la normalización de capa\n",
    "\n",
    "Asumiento los parámetros $\\gamma$ y $\\beta$ como 1 y 0 respectivamente, desarrolla un código que normalice los siguientes vectores de características:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([2.0, 2.0, 2.0])\n",
    "v3 = torch.tensor([23.0, 0.01, 5.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado debe ser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2247,  0.0000,  1.2247], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([0., 0., 0.], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([ 1.3838, -0.9446, -0.4392], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nl = torch.nn.LayerNorm(3, eps=1e-05, elementwise_affine=True)\n",
    "\n",
    "print(nl(v1))\n",
    "print(nl(v2))\n",
    "print(nl(v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

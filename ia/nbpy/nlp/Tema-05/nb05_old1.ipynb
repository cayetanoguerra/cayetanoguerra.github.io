{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelos seq2seq 2**\n",
    "\n",
    "Cuando la entrada de cada paso del decodificador proviene de la salida del paso anterior, estamos hablando de un modelo Seq2Seq con realimentación (feedback). Esto es especialmente común en tareas como la generación de texto.\n",
    "\n",
    "1. **Inicio de la Secuencia**: Se inicia la generación con un token especial, como `<SOS>` (Start of Sequence).\n",
    "\n",
    "2. **Generación Paso a Paso**:\n",
    "   - En el primer paso, el decodificador recibe el `<SOS>` y el estado del codificador como entrada.\n",
    "   - El decodificador procesa esta entrada y genera una salida para este paso.\n",
    "   - La salida generada se convierte en la entrada para el siguiente paso, junto con el estado actualizado del decodificador.\n",
    "   - Este proceso se repite hasta que se genera un token especial de fin de secuencia (`<EOS>`, End of Sequence) o hasta alcanzar un límite máximo de longitud.\n",
    "\n",
    "3. **Ventajas y Desventajas**:\n",
    "   - **Ventajas**: Esta forma de generar secuencias puede ayudar a mantener la coherencia en las secuencias generadas, ya que cada nueva palabra o token tiene en cuenta lo que ya se ha generado.\n",
    "   - **Desventajas**: Puede ser más lento, ya que cada paso depende del anterior, y errores en un paso pueden propagarse y afectar los pasos siguientes.\n",
    "\n",
    "\n",
    "Imagina que tienes un modelo seq2seq entrenado para traducir inglés a español. Para la frase \"How are you?\", el proceso sería algo así:\n",
    "\n",
    "1. El codificador procesa \"How are you?\" y genera un vector latente.\n",
    "2. El decodificador recibe el vector latente y el token `<SOS>`.\n",
    "3. El decodificador genera \"¿Cómo\", actualiza su estado y toma \"¿Cómo\" como entrada para el siguiente paso.\n",
    "4. El decodificador genera \"estás\", actualiza su estado y toma \"estás\" como entrada para el siguiente paso.\n",
    "5. Y así sucesivamente, hasta generar `<EOS>` para indicar el final de la secuencia.\n",
    "\n",
    "Este modelo de generación permite que el decodificador tenga en cuenta no solo el contexto proporcionado por el codificador, sino también la estructura de la secuencia que está generando, paso a paso.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"imgs/seq2seq-feedback.svg\" width=\"70%\">\n",
    "</p>\n",
    "\n",
    "\n",
    "### **Teacher Forcing**\n",
    "\n",
    "**Teacher forcing** es una técnica utilizada en el entrenamiento de modelos seq2seq en la que, en un porcentaje de las veces, se utiliza la salida real (etiqueta) de un paso de tiempo como entrada para el siguiente paso, en lugar de la salida predicha por el modelo. Esta técnica puede ayudar a acelerar la convergencia y mejorar el rendimiento del modelo, especialmente en las etapas iniciales del entrenamiento.\n",
    "\n",
    "#### **¿Cómo funciona?**\n",
    "\n",
    "1. **Durante el Entrenamiento**: En cada paso de tiempo y con una cierta probabilidad de que suceda, en lugar de pasar la predicción del modelo del paso anterior como entrada al siguiente paso, se pasa la palabra real de la secuencia objetivo. Esto proporciona al modelo información directa y clara sobre cómo debería haber respondido en el paso anterior, independientemente de si la predicción fue correcta o no.\n",
    "\n",
    "2. **Durante la Evaluación/Predicción**: El modelo debe generar secuencias por sí mismo, utilizando sus propias predicciones del paso anterior para el siguiente paso. Durante esta fase, no se utiliza \"teacher forcing\".\n",
    "\n",
    "#### **Ventajas de Teacher Forcing:**\n",
    "\n",
    "1. **Aprendizaje más Rápido**: Al proporcionar al modelo la respuesta correcta en cada paso, se reduce la propagación de errores a través de la secuencia, lo que puede llevar a un aprendizaje más rápido.\n",
    "\n",
    "2. **Mejor Rendimiento**: Puede resultar en un mejor rendimiento del modelo, especialmente en las primeras etapas del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementación traductor inglés a español**\n",
    "\n",
    "#### **Dataset Europarl**\n",
    "\n",
    "El conjunto de datos Europarl contiene las transcripciones de los procedimientos del Parlamento Europeo, proporcionando una valiosa fuente de textos paralelos en 21 idiomas europeos. Las oraciones están alineadas entre los idiomas, lo que lo hace especialmente útil para tareas de traducción automática. \n",
    "\n",
    "Descargamos el dataset Europarl para español-inglés. Una vez descargado tendremos dos ficheros de texto, uno para cada idioma con las frases alineadas. El código siguiente muestra las primeras frases de cada fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglés: Resumption of the session\n",
      "Español: Reanudación del período de sesiones\n",
      "---\n",
      "Inglés: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Español: Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\n",
      "---\n",
      "Inglés: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "Español: Como todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\n",
      "---\n",
      "Inglés: You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "Español: Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\n",
      "---\n",
      "Inglés: In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "Español: A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n",
      "---\n",
      "Inglés: Please rise, then, for this minute' s silence.\n",
      "Español: Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\n",
      "---\n",
      "Inglés: (The House rose and observed a minute' s silence)\n",
      "Español: (El Parlamento, de pie, guarda un minuto de silencio)\n",
      "---\n",
      "Inglés: Madam President, on a point of order.\n",
      "Español: Señora Presidenta, una cuestión de procedimiento.\n",
      "---\n",
      "Inglés: You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n",
      "Español: Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\n",
      "---\n",
      "Inglés: One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n",
      "Español: Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\n",
      "---\n",
      "Inglés: Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\n",
      "Español: ¿Sería apropiado que usted, Señora Presidenta, escribiese una carta al Presidente de Sri Lanka expresando las condolencias del Parlamento por esa y otras muertes violentas, pidiéndole que haga todo lo posible para encontrar una reconciliación pacífica ante la extremadamente difícil situación que está viviendo su país?\n",
      "---\n",
      "Inglés: Yes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.\n",
      "Español: Sí, señor Evans, pienso que una iniciativa como la que usted acaba de sugerir sería muy adecuada.\n",
      "---\n",
      "Inglés: If the House agrees, I shall do as Mr Evans has suggested.\n",
      "Español: Si la Asamblea está de acuerdo, haré lo que el señor Evans acaba de sugerir.\n",
      "---\n",
      "Inglés: Madam President, on a point of order.\n",
      "Español: Señora Presidenta, una cuestión de procedimiento.\n",
      "---\n",
      "Inglés: I would like your advice about Rule 143 concerning inadmissibility.\n",
      "Español: Me gustaría que me asesorara sobre el Artículo 143 concerniente a la inadmisibilidad.\n",
      "---\n",
      "Inglés: My question relates to something that will come up on Thursday and which I will then raise again.\n",
      "Español: Mi pregunta se refiere a un asunto del que se hablará el jueves, día que en volveré a plantearla.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def read_translation(archivo_ingles, archivo_espanol):\n",
    "    with open(archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
    "        for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
    "            yield oracion_ingles.strip(), oracion_espanol.strip()\n",
    "\n",
    "\n",
    "archivo_ingles = 'data/europarl/europarl-v7.es-en.en'\n",
    "archivo_espanol = 'data/europarl/europarl-v7.es-en.es'\n",
    "\n",
    "# Leer el conjunto de datos\n",
    "for i, (ingles, espanol) in enumerate(read_translation(archivo_ingles, archivo_espanol)):\n",
    "    print('Inglés:', ingles)\n",
    "    print('Español:', espanol)\n",
    "    print('---')\n",
    "    if i == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchtext\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# archivo_ingles = 'data/europarl/europarl-v7.es-en.en'\n",
    "# archivo_espanol = 'data/europarl/europarl-v7.es-en.es'\n",
    "\n",
    "\n",
    "class Translation(Dataset):\n",
    "    def __init__(self, source_file, target_file):\n",
    "        self.ingles = []\n",
    "        self.espanol = []\n",
    "        self.tokenizer_es = get_tokenizer(\"spacy\", language=\"es_core_news_md\")\n",
    "        self.tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_md\")\n",
    "        self.vocab_es = torchtext.vocab.FastText(language='es', unk_init=torch.Tensor.normal_)\n",
    "        self.vocab_en = torchtext.vocab.FastText(language='en', unk_init=torch.Tensor.normal_)\n",
    "\n",
    "        self.vocab_en = self.add_sos_eos_unk_pad(self.vocab_en)\n",
    "        self.vocab_es = self.add_sos_eos_unk_pad(self.vocab_es)\n",
    "        \n",
    "        # self.archivo_ingles = 'data/europarl/europarl-v7.es-en.en'\n",
    "        # self.archivo_espanol = 'data/europarl/europarl-v7.es-en.es'\n",
    "\n",
    "        self.archivo_ingles = source_file\n",
    "        self.archivo_espanol = target_file\n",
    "\n",
    "        # Leer el conjunto de datos\n",
    "        for ingles, espanol in self.read_translation():\n",
    "            self.ingles.append(ingles)\n",
    "            self.espanol.append(espanol)\n",
    "\n",
    "\n",
    "    def add_sos_eos_unk_pad(self, vocabulary):\n",
    "        words = vocabulary.itos\n",
    "        vocab = vocabulary.stoi\n",
    "        embedding_matrix = vocabulary.vectors\n",
    "\n",
    "        # Tokens especiales\n",
    "        sos_token = '<sos>'\n",
    "        eos_token = '<eos>'\n",
    "        pad_token = '<pad>'\n",
    "        unk_token = '<unk>'\n",
    "\n",
    "        # Inicializamos los vectores para los tokens especiales, por ejemplo, con ceros\n",
    "        sos_vector = torch.full((1, embedding_matrix.shape[1]), 1.)\n",
    "        eos_vector = torch.full((1, embedding_matrix.shape[1]), 2.)\n",
    "        pad_vector = torch.zeros((1, embedding_matrix.shape[1]))\n",
    "        unk_vector = torch.full((1, embedding_matrix.shape[1]), 3.)\n",
    "\n",
    "        # Añade los vectores al final de la matriz de embeddings\n",
    "        embedding_matrix = torch.cat((embedding_matrix, sos_vector, eos_vector, unk_vector, pad_vector), 0)\n",
    "\n",
    "        # Añade los tokens especiales al vocabulario\n",
    "        vocab[sos_token] = len(vocab)\n",
    "        vocab[eos_token] = len(vocab)\n",
    "        vocab[pad_token] = len(vocab)\n",
    "        vocab[unk_token] = len(vocab)\n",
    "\n",
    "        words.append(sos_token)\n",
    "        words.append(eos_token)\n",
    "        words.append(pad_token)\n",
    "        words.append(unk_token)\n",
    "\n",
    "        vocabulary.itos = words\n",
    "        vocabulary.stoi = vocab\n",
    "        vocabulary.vectors = embedding_matrix\n",
    "\n",
    "        default_stoi = defaultdict(lambda : len(vocabulary)-1, vocabulary.stoi)\n",
    "        vocabulary.stoi = default_stoi\n",
    "    \n",
    "        return vocabulary\n",
    "        \n",
    "\n",
    "    def read_translation(self):\n",
    "        with open(self.archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(self.archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
    "            for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
    "                yield oracion_ingles.strip().lower(), oracion_espanol.strip().lower()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ingles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            items = [(self.ingles[idx], self.espanol[idx]) for idx in range(*idx.indices(len(self.ingles)))]\n",
    "            items = [(self.vocab_en.get_vecs_by_tokens(self.tokenizer_en(item[0])), self.vocab_es.get_vecs_by_tokens(self.tokenizer_es(item[1]))) for item in items]\n",
    "\n",
    "            if not items:\n",
    "                raise RuntimeError(\"Todas las muestras en este lote están vacías.\")\n",
    "            \n",
    "            # Añade el token de fin de secuencia a la oración en inglés\n",
    "            # Añade el token de inicio y final de secuencia a la oración en español\n",
    "            for item in items:\n",
    "                if not item[0] or not item[1]:\n",
    "                    raise RuntimeError(\"Una de las muestras en este lote está vacía.\")\n",
    "                eos_token = self.vocab_en.vectors[self.vocab_en.stoi['<eos>']].unsqueeze(0)\n",
    "                sos_token = self.vocab_es.vectors[self.vocab_es.stoi['<sos>']].unsqueeze(0)\n",
    "                eos_token = self.vocab_es.vectors[self.vocab_es.stoi['<eos>']].unsqueeze(0)\n",
    "                item[0] = torch.cat((item[0], eos_token), 0)\n",
    "                item[1] = torch.cat((sos_token, item[1], eos_token), 0)                \n",
    "        \n",
    "            # ing_tensors, esp_tensors = zip(*items)\n",
    "            # ing_tensors = pad_sequence(ing_tensors, batch_first=True)\n",
    "            # esp_tensors = pad_sequence(esp_tensors, batch_first=True)\n",
    "\n",
    "            return items\n",
    "        else:\n",
    "            item = self.ingles[idx], self.espanol[idx]\n",
    "            tokens_ingles = self.tokenizer_en(item[0])\n",
    "            tokens_espanol = self.tokenizer_es(item[1])\n",
    "\n",
    "            # Add <eos> token to english sentence\n",
    "            # Add <sos> and <eos> tokens to spanish sentence\n",
    "            tokens_ingles = tokens_ingles + ['<eos>']\n",
    "            tokens_espanol = ['<sos>'] + tokens_espanol + ['<eos>']\n",
    "\n",
    "            if not tokens_ingles or not tokens_espanol:\n",
    "                return torch.zeros(1, 300), torch.zeros(1, 300)\n",
    "                # raise RuntimeError(\"Una de las muestras está vacía.\")\n",
    "        \n",
    "            tensor_ingles = self.vocab_en.get_vecs_by_tokens(tokens_ingles)\n",
    "            tensor_espanol = self.vocab_es.get_vecs_by_tokens(tokens_espanol)\n",
    "\n",
    "            indices_ingles = [self.vocab_en.stoi[token] for token in tokens_ingles]\n",
    "            indices_espanol = [self.vocab_es.stoi[token] for token in tokens_espanol]\n",
    "\n",
    "            return tensor_ingles, tensor_espanol, indices_ingles, indices_espanol\n",
    "        \n",
    "            \n",
    "        \n",
    "def collate_fn(batch):\n",
    "    pad = 0\n",
    "    ingles_batch, espanol_batch, ingles_seqs, espanol_seqs = zip(*batch)\n",
    "    ingles_batch = pad_sequence(ingles_batch, batch_first=True, padding_value=0)\n",
    "    espanol_batch = pad_sequence(espanol_batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Calcular la longitud máxima de la lista de listas de índices\n",
    "    max_len = max([len(l) for l in espanol_seqs])\n",
    "    for seq in espanol_seqs:\n",
    "        seq += [pad]*(max_len-len(seq))\n",
    "        \n",
    "    return ingles_batch, espanol_batch, ingles_seqs, espanol_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_ingles = 'data/mock/mock.en'\n",
    "archivo_espanol = 'data/mock/mock.es'\n",
    "\n",
    "translation = Translation(archivo_ingles, archivo_espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__() \n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)  # TO DO: Añadir dropout \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.rnn(x, (hidden, cell))\n",
    "        output = self.fc_out(output)\n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        target_len = target.shape[1]\n",
    "        batch_size = target.shape[0]\n",
    "        input_dim = target.shape[2]\n",
    "\n",
    "        # Tensor para almacenar las salidas del decoder\n",
    "        outputs = torch.zeros(batch_size, target_len, 985671)\n",
    "        \n",
    "        # Primero, la fuente es procesada por el encoder\n",
    "        _, (hidden, cell) = self.encoder(source)\n",
    "\n",
    "        # La primera entrada al decoder es el vector <sos>\n",
    "        x = target[:, 0, :]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, (hidden, cell) = self.decoder(x.unsqueeze(1), hidden, cell)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            # if random.random() < teacher_forcing_ratio:\n",
    "            x = target[:, t, :]\n",
    "            # else:\n",
    "            #   x = output.squeeze(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(translation, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/2], Loss: 0.9548\n",
      "Epoch [1/30], Average Loss: 0.8901\n",
      "Epoch [2/30], Step [1/2], Loss: 0.8679\n",
      "Epoch [2/30], Average Loss: 0.8344\n",
      "Epoch [3/30], Step [1/2], Loss: 0.8205\n",
      "Epoch [3/30], Average Loss: 0.7556\n",
      "Epoch [4/30], Step [1/2], Loss: 0.6580\n",
      "Epoch [4/30], Average Loss: 0.8223\n",
      "Epoch [5/30], Step [1/2], Loss: 0.7052\n",
      "Epoch [5/30], Average Loss: 0.6959\n",
      "Epoch [6/30], Step [1/2], Loss: 0.5470\n",
      "Epoch [6/30], Average Loss: 0.7197\n",
      "Epoch [7/30], Step [1/2], Loss: 0.5937\n",
      "Epoch [7/30], Average Loss: 0.6142\n",
      "Epoch [8/30], Step [1/2], Loss: 0.5450\n",
      "Epoch [8/30], Average Loss: 0.5365\n",
      "Epoch [9/30], Step [1/2], Loss: 0.4496\n",
      "Epoch [9/30], Average Loss: 0.5556\n",
      "Epoch [10/30], Step [1/2], Loss: 0.4679\n",
      "Epoch [10/30], Average Loss: 0.4812\n",
      "Epoch [11/30], Step [1/2], Loss: 0.4693\n",
      "Epoch [11/30], Average Loss: 0.4210\n",
      "Epoch [12/30], Step [1/2], Loss: 0.3411\n",
      "Epoch [12/30], Average Loss: 0.4775\n",
      "Epoch [13/30], Step [1/2], Loss: 0.4274\n",
      "Epoch [13/30], Average Loss: 0.3666\n",
      "Epoch [14/30], Step [1/2], Loss: 0.3578\n",
      "Epoch [14/30], Average Loss: 0.3665\n",
      "Epoch [15/30], Step [1/2], Loss: 0.3713\n",
      "Epoch [15/30], Average Loss: 0.3131\n",
      "Epoch [16/30], Step [1/2], Loss: 0.3058\n",
      "Epoch [16/30], Average Loss: 0.3294\n",
      "Epoch [17/30], Step [1/2], Loss: 0.2905\n",
      "Epoch [17/30], Average Loss: 0.3045\n",
      "Epoch [18/30], Step [1/2], Loss: 0.2745\n",
      "Epoch [18/30], Average Loss: 0.2866\n",
      "Epoch [19/30], Step [1/2], Loss: 0.2247\n",
      "Epoch [19/30], Average Loss: 0.2940\n",
      "Epoch [20/30], Step [1/2], Loss: 0.2448\n",
      "Epoch [20/30], Average Loss: 0.2470\n",
      "Epoch [21/30], Step [1/2], Loss: 0.2573\n",
      "Epoch [21/30], Average Loss: 0.2151\n",
      "Epoch [22/30], Step [1/2], Loss: 0.2420\n",
      "Epoch [22/30], Average Loss: 0.2014\n",
      "Epoch [23/30], Step [1/2], Loss: 0.2015\n",
      "Epoch [23/30], Average Loss: 0.2193\n",
      "Epoch [24/30], Step [1/2], Loss: 0.2061\n",
      "Epoch [24/30], Average Loss: 0.1974\n",
      "Epoch [25/30], Step [1/2], Loss: 0.1926\n",
      "Epoch [25/30], Average Loss: 0.1895\n",
      "Epoch [26/30], Step [1/2], Loss: 0.2026\n",
      "Epoch [26/30], Average Loss: 0.1536\n",
      "Epoch [27/30], Step [1/2], Loss: 0.1968\n",
      "Epoch [27/30], Average Loss: 0.1605\n",
      "Epoch [28/30], Step [1/2], Loss: 0.1893\n",
      "Epoch [28/30], Average Loss: 0.1510\n",
      "Epoch [29/30], Step [1/2], Loss: 0.1631\n",
      "Epoch [29/30], Average Loss: 0.1599\n",
      "Epoch [30/30], Step [1/2], Loss: 0.1514\n",
      "Epoch [30/30], Average Loss: 0.1589\n"
     ]
    }
   ],
   "source": [
    "# Bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        # print(tgt_indices)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "        # loss = criterion(output, torch.tensor(tgt_indices, dtype=torch.long))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(dataloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mi gato <eos>\n"
     ]
    }
   ],
   "source": [
    "# Test the model with input sentences\n",
    "model.eval()\n",
    "\n",
    "sentence = \"my cat\"\n",
    "\n",
    "# Convertir a vectores\n",
    "tokens = translation.tokenizer_en(sentence)\n",
    "tokens = tokens + ['<eos>']\n",
    "text_tensor = translation.vocab_en.get_vecs_by_tokens(tokens)\n",
    "text_tensor = text_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, (hidden, cell) = model.encoder(text_tensor)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "input_token = torch.tensor(translation.vocab_es.stoi['<sos>']).unsqueeze(0)\n",
    "input_token = translation.vocab_es.vectors[input_token].unsqueeze(0)\n",
    "    \n",
    "\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        output, (hidden, cell) = model.decoder(input_token, hidden, cell)\n",
    "        \n",
    "    # Obtener el token con la probabilidad más alta\n",
    "    best_guess = output.argmax(2).squeeze(0)\n",
    "    outputs.append(best_guess.item())\n",
    "        \n",
    "    # Si el token es <eos>, terminar la traducción\n",
    "    if best_guess == translation.vocab_es.stoi['<eos>']:\n",
    "        break\n",
    "        \n",
    "    # Utilizar la palabra predicha como la siguiente entrada al decoder\n",
    "    input_token = translation.vocab_es.vectors[best_guess].unsqueeze(0)\n",
    "        \n",
    "# Convertir los índices de salida a palabras\n",
    "translated_sentence = [translation.vocab_es.itos[idx] for idx in outputs]\n",
    "    \n",
    "result = ' '.join(translated_sentence)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "torch.save(model.state_dict(), 'seq2seq.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from file\n",
    "model.state_dict(torch.load('seq2seq.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

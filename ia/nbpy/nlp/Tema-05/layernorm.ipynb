{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer Normalization**\n",
    "\n",
    "\n",
    "\"Layer Normalization\" es una técnica de normalización utilizada en el entrenamiento de redes neuronales profundas, introducida por Jimmy Lei Ba, Jamie Ryan Kiros y Geoffrey Hinton en 2016 (https://arxiv.org/pdf/1607.06450.pdf). Esta técnica es fundamental para mejorar la estabilidad y acelerar el entrenamiento de modelos de aprendizaje profundo, especialmente en el contexto de redes neuronales recurrentes (RNN) y modelos de atención como los Transformers.\n",
    "\n",
    "### Conceptos clave de la Layer Normalization:\n",
    "\n",
    "1. **Normalización por Ejemplo**: La Layer Normalization normaliza los datos a lo largo de las características para cada ejemplo individual en un lote. Esto significa que, para cada dato en el lote, la normalización se realiza calculando la media y la varianza de todas sus características.\n",
    "\n",
    "2. **Cálculo de la Media y la Varianza**: Para cada ejemplo, se calcula la media y la varianza de todas sus características. Estos valores se utilizan para normalizar las características de ese ejemplo específico.\n",
    "\n",
    "3. **Proceso de Normalización**: La normalización se realiza restando la media y dividiendo por la desviación estándar (raíz cuadrada de la varianza) de las características de cada ejemplo. Se añade un pequeño valor, conocido como epsilon, al denominador para evitar la división por cero.\n",
    "\n",
    "4. **Parámetros Aprendibles**: Después de normalizar las características, se aplican dos parámetros aprendibles a cada una de ellas: un factor de escala (gamma) y un término de desplazamiento (beta). Estos parámetros son específicos para cada capa de la red y se ajustan durante el proceso de entrenamiento.\n",
    "\n",
    "5. **Aplicaciones en Modelos de Secuencia**: La Layer Normalization ha demostrado ser particularmente efectiva en modelos que procesan datos secuenciales, como las redes neuronales recurrentes y los modelos basados en el mecanismo de atención, donde ayuda a mejorar la estabilidad y eficiencia del entrenamiento.\n",
    "\n",
    "### Ventajas de la Layer Normalization:\n",
    "\n",
    "- **Estabilidad en el Entrenamiento**: Contribuye a la estabilidad del entrenamiento de redes neuronales profundas, lo que es crucial para lograr un buen rendimiento en tareas complejas.\n",
    "- **Eficiencia en Modelos de Secuencia**: Facilita el entrenamiento eficiente de modelos que trabajan con datos secuenciales, mejorando tanto la velocidad de convergencia como la calidad del modelo entrenado.\n",
    "- **Flexibilidad en el Tamaño del Lote**: Al normalizar cada ejemplo de forma independiente, la Layer Normalization no depende del tamaño del lote, lo que la hace flexible para diferentes configuraciones de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación matemática:\n",
    "\n",
    "La Layer Normalization se centra en normalizar las activaciones dentro de una capa para cada ejemplo de datos. A continuación se describe el proceso matemáticamente:\n",
    "\n",
    "Dado un tensor de entrada $ X $ para una capa específica en la red, donde $ X $ tiene dimensiones $[N, F]$ (con $ N $ siendo el tamaño del lote y $ F $ el número de características o neuronas en la capa), la Layer Normalization se realiza de la siguiente manera para cada ejemplo $ n $ en el lote:\n",
    "\n",
    "1. **Calcular la Media**:\n",
    "   $$ \\mu_n = \\frac{1}{F} \\sum_{f=1}^{F} x_{nf} $$\n",
    "   Aquí, $ \\mu_n $ es la media de las activaciones para el ejemplo $ n $.\n",
    "\n",
    "2. **Calcular la Varianza**:\n",
    "   $$ \\sigma_n^2 = \\frac{1}{F} \\sum_{f=1}^{F} (x_{nf} - \\mu_n)^2 $$\n",
    "   Donde $ \\sigma_n^2 $ es la varianza de las activaciones para el ejemplo $ n $.\n",
    "\n",
    "3. **Normalizar**:\n",
    "   $$ \\hat{x}_{nf} = \\frac{x_{nf} - \\mu_n}{\\sqrt{\\sigma_n^2 + \\epsilon}} $$\n",
    "   Cada activación $ x_{nf} $ se normaliza restando la media $ \\mu_n $ y dividiendo por la raíz cuadrada de la varianza $ \\sigma_n^2 $, con un pequeño número $ \\epsilon $ añadido para la estabilidad numérica (evitar la división por cero).\n",
    "\n",
    "4. **Aplicar Parámetros Aprendibles**:\n",
    "   $$ y_{nf} = \\gamma \\hat{x}_{nf} + \\beta $$\n",
    "   Finalmente, se aplica una transformación lineal a las activaciones normalizadas, donde $ \\gamma $ y $ \\beta $ son parámetros aprendibles específicos de la capa. Estos parámetros permiten que la normalización se ajuste de manera flexible durante el entrenamiento.\n",
    "\n",
    "En esta formulación, $ n $ indexa los ejemplos en el lote y $ f $ indexa las características. Cada ejemplo se normaliza de forma independiente. Los parámetros $ \\gamma $ y $ \\beta $ son de la misma dimensión que el número de características $ F $, permitiendo una escala y un desplazamiento distintos para cada característica.\n",
    "\n",
    "La Layer Normalization es especialmente útil en modelos de secuencias como los Transformers y las RNN, donde normaliza las activaciones a lo largo de las características para cada paso de tiempo o posición en la secuencia, contribuyendo a un entrenamiento más estable y eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio: Implementación de la normalización de capa\n",
    "\n",
    "Asumiento los parámetros $\\gamma$ y $\\beta$ como 1 y 0 respectivamente, desarrolla un código que normalice los siguientes vectores de características:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([2.0, 2.0, 2.0])\n",
    "v3 = torch.tensor([23.0, 0.01, 5.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado debe ser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2247,  0.0000,  1.2247], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([0., 0., 0.], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([ 1.3838, -0.9446, -0.4392], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nl = torch.nn.LayerNorm(3, eps=1e-05, elementwise_affine=True)\n",
    "\n",
    "print(nl(v1))\n",
    "print(nl(v2))\n",
    "print(nl(v3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

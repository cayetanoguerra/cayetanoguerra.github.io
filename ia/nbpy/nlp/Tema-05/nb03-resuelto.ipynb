{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Redes neuronales recurrentes (RNN)**\n",
    "\n",
    "### **LSTM**\n",
    "\n",
    "El conjunto de datos \"AG_NEWS\" es un conjunto de datos de clasificación de texto ampliamente utilizado en el campo del procesamiento de lenguaje natural (NLP). Contiene noticias de diferentes categorías y se utiliza comúnmente para tareas de clasificación de texto. El conjunto de datos AG_NEWS consta de noticias de cuatro categorías principales, que son:\n",
    "\n",
    "1. **World**: Noticias sobre eventos y acontecimientos globales, como política internacional, relaciones internacionales y noticias mundiales en general.\n",
    "\n",
    "2. **Sports**: Noticias relacionadas con eventos deportivos, resultados de partidos, eventos deportivos nacionales e internacionales, etc.\n",
    "\n",
    "3. **Business**: Noticias relacionadas con el mundo de los negocios, finanzas, economía, empresas, informes de ganancias y otros temas económicos.\n",
    "\n",
    "4. **Sci/Tech**: Noticias relacionadas con ciencia y tecnología, incluyendo avances científicos, novedades tecnológicas, gadgets, investigaciones científicas y más.\n",
    "\n",
    "Cada instancia del conjunto de datos AG_NEWS generalmente consiste en un título y un cuerpo de una noticia, junto con una etiqueta que indica la categoría a la que pertenece. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "from torchtext.data import to_map_style_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "train_iter, test_iter = datasets.AG_NEWS(split=('train', 'test'))\n",
    "\n",
    "train_ds = to_map_style_dataset(train_iter)\n",
    "test_ds = to_map_style_dataset(test_iter)\n",
    "\n",
    "train = np.array(train_ds)\n",
    "test = np.array(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary and embedding\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "glove_vectors = GloVe(name='6B', dim=300)\n",
    "\n",
    "vocab = build_vocab_from_iterator(map(lambda x: tokenizer(x[1]), train_iter), specials=['<pad>','<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 95812 tokens\n",
      "Tokenización de la frase 'Here is an example sentence': ['here', 'is', 'an', 'example', 'sentence']\n",
      "Índices de las palabras 'here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious': [476, 22, 31, 5298, 1]\n",
      "Palabras correspondientes a los índices 475, 21, 30, 5297, 0: ['version', 'at', 'from', 'establish', '<pad>']\n",
      "Las diez primeras palabras del vocabulario: ['<pad>', '<unk>', '.', 'the', ',', 'to', 'a', 'of', 'in', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño del vocabulario:\", len(vocab), \"tokens\")\n",
    "print(\"Tokenización de la frase 'Here is an example sentence':\", tokenizer(\"Here is an example sentence\"))\n",
    "print(\"Índices de las palabras 'here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious':\", vocab(['here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious']))\n",
    "print(\"Palabras correspondientes a los índices 475, 21, 30, 5297, 0:\", vocab.lookup_tokens([475, 21, 30, 5297, 0]))\n",
    "print(\"Las diez primeras palabras del vocabulario:\", vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización de la frase 'Here is an example sentence': [476, 22, 31, 5298, 2994]\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "# text_pipeline = lambda x: tokenizer(x)\n",
    "# embed_pipeline = lambda x: [glove_vectors.get_vecs_by_tokens(token) for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "print(\"Tokenización de la frase 'Here is an example sentence':\", text_pipeline(\"Here is an example sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list = []\n",
    "    text_list = []\n",
    "    # L = [len(tokenizer(sample[1])) for sample in batch]\n",
    "    # print(L)\n",
    "    max_len = max([len(tokenizer(sample[1])) for sample in batch])\n",
    "    # print(max_len)\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        label, text = sample\n",
    "        # print(i)\n",
    "        # print(text[:20])\n",
    "\n",
    "        embed_list = glove_vectors.get_vecs_by_tokens(tokenizer(text))\n",
    "        padding_list = glove_vectors[0].unsqueeze(0).repeat(max_len - len(embed_list), 1)\n",
    "        \n",
    "        #print(\"Padding list: \", padding_list.shape) \n",
    "        #print(\"Embed list: \", embed_list.shape)\n",
    "\n",
    "        embed_list = torch.cat((embed_list, padding_list), 0) \n",
    "        \n",
    "        #print(\"Final embed:\", embed_list.shape)\n",
    "        # print(label)\n",
    "\n",
    "        # text_list.append(torch.tensor(text_pipeline(text), dtype=torch.long))\n",
    "        text_list.append(embed_list)\n",
    "        label_list.append(label_pipeline(label))\n",
    "        \n",
    "    # return torch.tensor(label_list, dtype=torch.long), torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    #print(text_list)\n",
    "    # Convert to tensor\n",
    "    #tensor_text_list = torch.stack(text_list)\n",
    "\n",
    "    # print(torch.tensor(text_list).shape)\n",
    "    return torch.tensor(label_list, dtype=torch.long), torch.stack(text_list)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que estamos creando correctamente los lotes, vamos a imprimir las primeras cuatro instancias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0148,  0.2784, -0.5527,  ..., -0.0333, -0.0765,  0.0316],\n",
      "         [-0.2576, -0.0571, -0.6719,  ..., -0.1604,  0.0467, -0.0706],\n",
      "         [-0.1674, -0.0937, -0.4510,  ..., -0.1884,  0.0358, -0.2040],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2702,  0.2535,  0.4751,  ...,  0.3080,  0.0611, -0.3626],\n",
      "         [-0.4440,  0.1282, -0.2525,  ..., -0.2004, -0.0822, -0.0626],\n",
      "         [-0.3142, -0.3453,  0.0977,  ..., -0.3042, -0.1494, -0.3194],\n",
      "         ...,\n",
      "         [-0.6020,  0.0929, -0.8632,  ...,  0.0666,  0.1424,  0.1066],\n",
      "         [-0.1741, -0.1192,  0.0176,  ...,  0.0382,  0.0282,  0.1074],\n",
      "         [-0.1256,  0.0136,  0.1031,  ..., -0.3422, -0.0224,  0.1368]],\n",
      "\n",
      "        [[ 0.0292,  0.1023,  0.1499,  ...,  0.2165, -0.2081,  0.2571],\n",
      "         [ 0.4185,  0.3042,  0.0979,  ..., -0.3217, -0.2157,  0.1466],\n",
      "         [ 0.6175,  0.2808,  0.0373,  ...,  0.1261,  0.2034, -0.2965],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.2870,  0.3352,  0.1356,  ..., -0.5342, -0.1488,  0.2146],\n",
      "         [-0.1256,  0.0136,  0.1031,  ..., -0.3422, -0.0224,  0.1368],\n",
      "         [-0.3613,  0.1158,  0.0210,  ..., -0.0375,  0.4505, -0.5536],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "\n",
      "\n",
      "tensor([2, 3, 0, 0])\n",
      "\n",
      "\n",
      "torch.Size([64, 150, 300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[1][:4])\n",
    "    print(\"\\n\")\n",
    "    print(batch[0][:4])\n",
    "    print(\"\\n\")\n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super(LSTMTextClassificationModel, self).__init__()\n",
    "\n",
    "        # self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # self.glove_embedding = nn.Embedding.from_pretrained(glove_vectors.vectors)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, embedded):\n",
    "        # embedded = self.glove_embedding(text)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Tomar la última salida de la secuencia LSTM\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        # last_output = lstm_out[:, -1]\n",
    "        output = self.fc(last_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 108, 300])\n",
      "tensor([[[ 0.1121,  0.3463, -0.1169,  ..., -0.1499,  0.2519, -0.2226],\n",
      "         [-0.3620,  0.2663, -0.4980,  ...,  0.0977,  0.0702,  0.1810],\n",
      "         [ 0.5321,  0.1240,  0.0354,  ..., -0.7439,  0.0667,  0.1422],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0470, -0.3525,  0.2789,  ...,  0.0649,  0.2298, -0.1721],\n",
      "         [ 0.0561,  0.2004, -0.1366,  ..., -0.5013, -0.0210,  0.4466],\n",
      "         [-0.0374, -0.3325, -0.5968,  ...,  0.8214,  0.6356,  0.3285],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.2450, -0.1249, -0.2536,  ..., -0.4107, -0.6193,  0.0039],\n",
      "         [-0.2853,  0.1124, -0.1908,  ..., -0.7520,  0.5028,  0.1516],\n",
      "         [-0.1091,  0.0645, -0.1898,  ..., -0.4909,  0.2627, -0.1146],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3856, -0.5780, -0.1654,  ...,  0.3295,  0.0615, -0.5307],\n",
      "         [-0.2024, -0.0610,  0.3906,  ...,  0.0721, -0.4548,  0.0948],\n",
      "         [-0.2614,  0.3398, -0.0958,  ..., -0.2839, -0.1703,  0.1158],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[ 0.1100,  0.0302, -0.1341,  0.0441],\n",
      "        [ 0.1100,  0.0302, -0.1341,  0.0441],\n",
      "        [ 0.1100,  0.0302, -0.1341,  0.0441],\n",
      "        [ 0.1100,  0.0302, -0.1341,  0.0441]], grad_fn=<SliceBackward0>)\n",
      "tensor([3, 0, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTMTextClassificationModel(len(vocab), 300, 64, 4).to(device)\n",
    "model.train()\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch[1].shape)\n",
    "    predicted_label = model(batch[1])\n",
    "    label = batch[0]\n",
    "    break\n",
    "\n",
    "print(batch[1][:4])\n",
    "print(predicted_label[:4])\n",
    "print(label[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count, max_acc = 0, 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            #print(total_acc / total_count)\n",
    "\n",
    "            print('| {:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(idx, \n",
    "                                              total_acc / total_count))\n",
    "\n",
    "            if max_acc < total_acc / total_count:\n",
    "                max_acc = total_acc / total_count\n",
    "                \n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    return max_acc\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   500 batches | accuracy    0.248\n",
      "|  1000 batches | accuracy    0.255\n",
      "|  1500 batches | accuracy    0.354\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 71.98s | valid accuracy    0.686 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.702\n",
      "|  1000 batches | accuracy    0.806\n",
      "|  1500 batches | accuracy    0.845\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 73.40s | valid accuracy    0.886 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.886\n",
      "|  1000 batches | accuracy    0.894\n",
      "|  1500 batches | accuracy    0.912\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 71.98s | valid accuracy    0.901 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.902\n",
      "|  1000 batches | accuracy    0.907\n",
      "|  1500 batches | accuracy    0.928\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 72.23s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.904\n",
      "|  1000 batches | accuracy    0.912\n",
      "|  1500 batches | accuracy    0.931\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 72.14s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.903\n",
      "|  1000 batches | accuracy    0.912\n",
      "|  1500 batches | accuracy    0.932\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 72.14s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.906\n",
      "|  1000 batches | accuracy    0.910\n",
      "|  1500 batches | accuracy    0.931\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 72.68s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.904\n",
      "|  1000 batches | accuracy    0.911\n",
      "|  1500 batches | accuracy    0.932\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 72.27s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.905\n",
      "|  1000 batches | accuracy    0.911\n",
      "|  1500 batches | accuracy    0.932\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 72.58s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.905\n",
      "|  1000 batches | accuracy    0.909\n",
      "|  1500 batches | accuracy    0.932\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 73.32s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    accu_train = train(train_dataloader)\n",
    "    accu_val = evaluate(test_dataloader)\n",
    "\n",
    "    if accu_train > accu_val:\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# **Estrategias de clasificación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aprendizaje supervisado en clasificación de texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clasificación mediante regresión logística**\n",
    "\n",
    "La regresión logística es un método estadístico utilizado para modelar la relación entre una variable dependiente binaria y una o más variables independientes. \n",
    "\n",
    "1. **Modelo**:\n",
    "   - La regresión logística modela la probabilidad $P(Y=1)$ de que la variable dependiente $Y$ sea 1 (generalmente representando la \"clase positiva\" en clasificación).\n",
    "   - Se utiliza la función logística (o función sigmoide) $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ para transformar cualquier valor en el rango (0, 1), siendo $z$ la combinación lineal de variables independientes, es decir, $ z = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots $.\n",
    "\n",
    "2. **Estimación de parámetros**:\n",
    "   - Los coeficientes $\\beta$ se estiman utilizando el método de máxima verosimilitud. La idea es encontrar los valores de $\\beta$ que maximizan la probabilidad (verosimilitud) de observar la muestra dada. Esto se puede hacer usando algoritmos de optimización como el método del gradiente, el método de Newton-Raphson, entre otros.\n",
    "\n",
    "3. **Predicción**:\n",
    "   - Una vez entrenado el modelo y estimados los coeficientes, se puede predecir la probabilidad $P(Y=1)$ para nuevos datos.\n",
    "   - Para tomar una decisión de clasificación, se establece un umbral (comúnmente 0.5). Si $P(Y=1)$ es mayor que el umbral, se clasifica como 1, de lo contrario, se clasifica como 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de ejemplo\n",
    "documents = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\",\n",
    "    \"El libro estaba basado en emocionantes paisajes.\",\n",
    "    \"El guion fue aclamado por su trama intrigante.\",\n",
    "    \"El cine muestra películas emocionantes de acción.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0]  # 1: Positivo, 0: Negativo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar la tokenización y conteo de palabras para construir la representación vectorial de los documentos. Para ello, vamos a utilizar la clase `CountVectorizer` de `scikit-learn`. Además, nos permite realizar el filtrado de palabras vacías (stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('spanish'))\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz que almacena las frecuencias de aparición de las palabras contiene muchos ceros, debido a que la mayoría de las palabras no aparecen en un documento dado. Por lo tanto, la matriz es muy dispersa. Para reducir la cantidad de ceros, scikit-learn utiliza una matriz dispersa para almacenar la matriz de frecuencia de términos. Una matriz dispersa es una matriz que tiene muy pocos valores distintos de cero. Básicamente, lo que hace es almacenar solo los valores distintos de cero. Esto reduce el uso de memoria y acelera los cálculos.\n",
    "\n",
    "Si quisiéramos ver la matriz podríamos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisiéramos ver el vocabulario que se ha generado, podemos hacerlo así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acción' 'aclamado' 'actores' 'autor' 'basada' 'basado' 'cine' 'describe'\n",
      " 'detalle' 'emocionante' 'emocionantes' 'escrito' 'excelente' 'famoso'\n",
      " 'fascinado' 'giros' 'gran' 'guion' 'hicieron' 'inesperados' 'intrigante'\n",
      " 'libro' 'llena' 'muestra' 'novela' 'novelista' 'paisajes' 'película'\n",
      " 'películas' 'personajes' 'realistas' 'siempre' 'trabajo' 'trama']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y si quisiéramos verlo todo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Documento  Palabra         Frecuencia\n",
      "-----------  ------------  ------------\n",
      "          0  película                 1\n",
      "          0  emocionante              1\n",
      "          0  llena                    1\n",
      "          0  acción                   1\n",
      "          1  libro                    1\n",
      "          1  trama                    1\n",
      "          1  intrigante               1\n",
      "          2  actores                  1\n",
      "          2  hicieron                 1\n",
      "          2  trabajo                  1\n",
      "          2  excelente                1\n",
      "          3  autor                    1\n",
      "          3  describe                 1\n",
      "          3  paisajes                 1\n",
      "          3  gran                     1\n",
      "          3  detalle                  1\n",
      "          4  autor                    1\n",
      "          4  cine                     1\n",
      "          4  siempre                  1\n",
      "          4  fascinado                1\n",
      "          5  llena                    1\n",
      "          5  novela                   1\n",
      "          5  giros                    1\n",
      "          5  inesperados              1\n",
      "          6  película                 1\n",
      "          6  guion                    1\n",
      "          6  escrito                  1\n",
      "          6  famoso                   1\n",
      "          6  novelista                1\n",
      "          7  libro                    1\n",
      "          7  personajes               1\n",
      "          7  realistas                1\n",
      "          8  película                 1\n",
      "          8  libro                    1\n",
      "          8  basada                   1\n",
      "          8  aclamado                 1\n",
      "          9  libro                    1\n",
      "          9  paisajes                 1\n",
      "          9  basado                   1\n",
      "          9  emocionantes             1\n",
      "         10  trama                    1\n",
      "         10  intrigante               1\n",
      "         10  guion                    1\n",
      "         10  aclamado                 1\n",
      "         11  acción                   1\n",
      "         11  cine                     1\n",
      "         11  emocionantes             1\n",
      "         11  muestra                  1\n",
      "         11  películas                1\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "vocabulario = vectorizer.get_feature_names_out()\n",
    "\n",
    "table = []\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    fila = X.getrow(i)\n",
    "    palabras_indices = fila.indices\n",
    "    frecuencias = fila.data\n",
    "\n",
    "    for idx, freq in zip(palabras_indices, frecuencias):\n",
    "        table.append([i, vocabulario[idx], freq])\n",
    "\n",
    "print(tabulate(table, headers=['Documento', 'Palabra', 'Frecuencia']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos del modelo de regresión logística con los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos la exactitud del modelo con el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones: [0 1 0]\n",
      "Etiquetas reales: [0, 1, 0]\n",
      "Exactitud del modelo: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predicciones = clf.predict(X_test)\n",
    "\n",
    "print(f\"Predicciones: {predicciones}\")\n",
    "print(f\"Etiquetas reales: {y_test}\")\n",
    "\n",
    "exactitud = accuracy_score(y_test, predicciones)\n",
    "print(f\"Exactitud del modelo: {exactitud * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regresión lineal en Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definición del modelo\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/10000], Loss: 0.0007\n",
      "Epoch [2000/10000], Loss: 0.0003\n",
      "Epoch [3000/10000], Loss: 0.0002\n",
      "Epoch [4000/10000], Loss: 0.0002\n",
      "Epoch [5000/10000], Loss: 0.0001\n",
      "Epoch [6000/10000], Loss: 0.0001\n",
      "Epoch [7000/10000], Loss: 0.0001\n",
      "Epoch [8000/10000], Loss: 0.0001\n",
      "Epoch [9000/10000], Loss: 0.0001\n",
      "Epoch [10000/10000], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Suponiendo X_train y y_train son tus datos de entrenamiento\n",
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Parámetros\n",
    "input_dim = X_train.toarray().shape[1]\n",
    "learning_rate = 1\n",
    "epochs = 10000\n",
    "\n",
    "# Modelo, función de pérdida y optimizador\n",
    "model = LogisticRegression(input_dim)\n",
    "# criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor.unsqueeze(1))\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Imprimir la pérdida cada 10 épocas\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0.])\n",
      "tensor([[0.7085],\n",
      "        [0.8811],\n",
      "        [0.1093]])\n",
      "Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "# Suponiendo X_test y y_test son tus datos de prueba\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "print(y_test_tensor)\n",
    "# Evaluar el modelo\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    print(test_outputs)\n",
    "    predicted = test_outputs.round()  # Clasifica como 1 si la salida > 0.5, de lo contrario 0\n",
    "    accuracy = (predicted.squeeze() == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# **Estrategias de clasificación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aprendizaje supervisado en clasificación de texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clasificador Naive Bayes**\n",
    "\n",
    "El algoritmo **Naive Bayes** es un clasificador probabilístico basado en el **teorema de Bayes** con supuestos de independencia entre las características. Es especialmente adecuado para la clasificación de texto. Antes, recordemos algunos conceptos básicos:\n",
    "\n",
    "**Teorema de Bayes:**\n",
    "El fundamento detrás del algoritmo Naive Bayes es el teorema de Bayes, que se formula como:\n",
    "\n",
    "$$ P(C|D) = \\frac{P(D|C) \\cdot P(C)}{P(D)} $$\n",
    "\n",
    "Donde:\n",
    "- $P(C|D)$ es la probabilidad posterior de la clase C dado un dato D.\n",
    "- $P(D|C)$ es la probabilidad de observar el dato D dado que pertenece a la clase C.\n",
    "- $P(C)$ es la probabilidad a priori de la clase C.\n",
    "- $P(D)$ es la probabilidad total de observar el dato D.\n",
    "\n",
    "\n",
    "**Independencia:**\n",
    "Dentro del Teorema de Bayes, el concepto de **independencia** se refiere a la suposición de que ciertas variables o eventos no afectan la probabilidad de otros eventos o variables.\n",
    "\n",
    "Cuando hablamos del clasificador Naive Bayes, nos referimos específicamente a la **independencia condicional** de las características dado un resultado o clase particular. Esta es la suposición \"ingenua\" (naive) que le da nombre al método.\n",
    "\n",
    "En términos matemáticos, la independencia condicional en Naive Bayes se expresa así:\n",
    "\n",
    "$ P(X_1, X_2, \\ldots , X_n | Y) = P(X_1 | Y) \\cdot P(X_2 | Y) \\ldots P(X_n | Y) $\n",
    "\n",
    "Donde:\n",
    "- $ X_1, X_2, ..., X_n $ son las características (por ejemplo, en clasificación de texto, estas podrían ser palabras o frases).\n",
    "- $ Y $ es una clase particular (por ejemplo, una etiqueta como \"spam\" o \"no spam\").\n",
    "\n",
    "Lo que esto significa es que, dado un valor particular de $ Y $, la probabilidad conjunta de todas las características es simplemente el producto de sus probabilidades individuales. En otras palabras, estamos asumiendo que la presencia (o ausencia) de una característica no afecta la presencia (o ausencia) de cualquier otra característica, siempre que conozcamos la clase $ Y $.\n",
    "\n",
    "Este supuesto simplifica enormemente los cálculos y, aunque rara vez es cierto en la práctica (especialmente en el procesamiento del lenguaje natural donde las palabras están frecuentemente relacionadas entre sí), el clasificador Naive Bayes puede ser sorprendentemente eficaz en muchas situaciones a pesar de su suposición de independencia.\n",
    "\n",
    "\n",
    "\n",
    "#### **Ejemplo**\n",
    "\n",
    "Veamos un ejemplo. Supongamos que queremos clasificar frases entre las categorías \"Cine\" y \"Literatura\". Las frases de entrenamiento son:\n",
    "\n",
    "**Frases de entrenamiento:**\n",
    "1. \"La película fue emocionante y llena de acción.\" - Cine\n",
    "2. \"Ese libro tiene una trama intrigante.\" - Literatura\n",
    "3. \"Los actores hicieron un trabajo excelente.\" - Cine\n",
    "4. \"El autor describe paisajes con gran detalle.\" - Literatura\n",
    "5. \"El cine de autor siempre me ha fascinado.\" - Cine\n",
    "6. \"La novela estaba llena de giros inesperados.\" - Literatura\n",
    "7. \"El guion de esa película fue escrito por un famoso novelista.\" - Cine\n",
    "8. \"Los personajes del libro eran muy realistas.\" - Literatura\n",
    "9. \"Esa película está basada en un libro aclamado.\" - Cine\n",
    "\n",
    "Algunas palabras, como \"libro\", \"película\", y \"autor\", aparecen en ambas categorías.\n",
    "\n",
    "La probabilidad a priori de cada categoría es:\n",
    "\n",
    "$$ P(Cine) = \\frac{5}{9} $$\n",
    "$$ P(Literatura) = \\frac{4}{9} $$\n",
    "\n",
    "Esto viene a significar que una frase a clasificar tiene, a priori, una probabilidad de $ \\frac{5}{9} $ de ser de la categoría \"Cine\" y una probabilidad de $ \\frac{4}{9} $ de ser de la categoría \"Literatura\".\n",
    "\n",
    "Ahora, calculamos las probabilidades condicionales para cada palabra en cada categoría (eliminamos previamente las stop-words). Fíjate que hay palabras que aparecen en ambas categorías. Por ejemplo, la palabra \"libro\" aparece en dos frases de \"Literatura\" y en una frase de \"Cine\". La palabra \"autor\" aparece en una frase de \"Literatura\" y en otra frase de \"Cine\". Y así sucesivamente. Por tanto, tenemos que:\n",
    "\n",
    "$$ P(libro|Literatura) = \\frac{2}{10} $$\n",
    "$$ P(libro|Cine) = \\frac{1}{10} $$\n",
    "$$ P(autor|Cine) = \\frac{1}{10} $$\n",
    "$$ P(autor|Literatura) = \\frac{1}{10} $$\n",
    "\n",
    "...y así sucesivamente para las demás palabras.\n",
    "\n",
    "Nos podemos dar cuenta de que puede haber palabras que no aparezcan en una categoría. Por ejemplo, la palabra \"paisajes\" no aparece en ninguna frase de \"Cine\". En este caso, la probabilidad condicional es cero:\n",
    "\n",
    "$$ P(paisajes|Cine) = 0 $$\n",
    "\n",
    "Esto es un problema, porque si multiplicamos muchas probabilidades condicionales, el resultado será cero. Para evitar esto, podemos usar un **suavizado** (smoothing) para evitar que las probabilidades condicionales sean cero. Por ejemplo, podemos usar el suavizado de Laplace, que consiste en sumar un valor $\\alpha$ (normalmente, 1) al numerador y el número de palabras únicas (vocabulario) en el denominador.\n",
    "\n",
    "$$ P(w_i|C_k) = \\frac{\\text{Número de veces que } w_i \\text{ aparece en } C_k + \\alpha}{\\text{Total de palabras en } C_k + \\alpha \\times \\text{Tamaño del vocabulario}} $$\n",
    "\n",
    "...y así sucesivamente para las demás palabras.\n",
    "\n",
    "El motivo de sumar el tamaño del vocabulario en el denominador es para que la suma de todas las probabilidades condicionales sea 1.\n",
    "\n",
    "Vayamos haciendo un script en Python para calcular las probabilidades condicionales de cada palabra en cada categoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades a priori:\n",
      "Cine 0.556\n",
      "Literatura 0.444\n",
      "\n",
      "Palabra        Pr. Cine    Pr. Literatura\n",
      "-----------  ----------  ----------------\n",
      "intrigante       0.0037            0.0524\n",
      "actores          0.0407            0.0048\n",
      "trabajo          0.0407            0.0048\n",
      "giros            0.0037            0.0524\n",
      "autor            0.0407            0.0524\n",
      "novela           0.0037            0.0524\n",
      "basada           0.0407            0.0048\n",
      "trama            0.0037            0.0524\n",
      "escrito          0.0407            0.0048\n",
      "gran             0.0037            0.0524\n",
      "novelista        0.0407            0.0048\n",
      "guion            0.0407            0.0048\n",
      "personajes       0.0037            0.0524\n",
      "siempre          0.0407            0.0048\n",
      "detalle          0.0037            0.0524\n",
      "llena            0.0407            0.0524\n",
      "emocionante      0.0407            0.0048\n",
      "película         0.1148            0.0048\n",
      "describe         0.0037            0.0524\n",
      "inesperados      0.0037            0.0524\n",
      "acción           0.0407            0.0048\n",
      "realistas        0.0037            0.0524\n",
      "libro            0.0407            0.1\n",
      "paisajes         0.0037            0.0524\n",
      "aclamado         0.0407            0.0048\n",
      "fascinado        0.0407            0.0048\n",
      "excelente        0.0407            0.0048\n",
      "famoso           0.0407            0.0048\n",
      "cine             0.0407            0.0048\n",
      "hicieron         0.0407            0.0048\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "documents = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "label_names = {0: \"Cine\", 1: \"Literatura\"}\n",
    "\n",
    "# Preprocesamiento\n",
    "def preprocess(docs):\n",
    "    txts = [doc.lower().replace(\".\", \"\") for doc in docs]\n",
    "    txts = [doc.split() for doc in txts]\n",
    "    # Eliminación de stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"spanish\")\n",
    "    txts = [[word for word in doc if word not in stopwords] for doc in txts]\n",
    "    return txts\n",
    "\n",
    "documents = preprocess(documents)\n",
    "\n",
    "# Clasificador Naive Bayes\n",
    "\n",
    "# Creación del vocabulario\n",
    "def get_vocab(docs):\n",
    "    vocab = set()\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            vocab.add(word)\n",
    "    vocab = list(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(documents)\n",
    "\n",
    "# Creación de la matriz de características\n",
    "import numpy as np\n",
    "X = np.zeros((len(documents), len(vocab)))\n",
    "for i, doc in enumerate(documents):\n",
    "    for word in doc:\n",
    "        j = vocab.index(word)\n",
    "        X[i][j] += 1\n",
    "\n",
    "# Probabilidades a priori\n",
    "def get_prior(labels):\n",
    "    prior = np.zeros(2)\n",
    "    for label in labels:\n",
    "        prior[label] += 1\n",
    "    prior = prior / len(labels)\n",
    "    return prior\n",
    "\n",
    "prior = get_prior(labels)\n",
    "\n",
    "# Probabilidades condicionales\n",
    "def get_conditional(X, labels, alpha=1):\n",
    "    conditional = np.ones((2, X.shape[1])) * alpha\n",
    "    for i, label in enumerate(labels):\n",
    "        conditional[label] += X[i]\n",
    "    conditional = conditional / (conditional.sum(axis=1).reshape(-1, 1) + alpha * len(vocab))\n",
    "    return conditional\n",
    "\n",
    "conditional = get_conditional(X, labels, 0.1)\n",
    "\n",
    "print(\"Probabilidades a priori:\")\n",
    "print(label_names[0], round(prior[0], 3))\n",
    "print(label_names[1], round(prior[1], 3))\n",
    "print()\n",
    "\n",
    "from tabulate import tabulate\n",
    "table = []\n",
    "for i, w in enumerate(vocab):\n",
    "    table.append([w, round(conditional[0][i],4), round(conditional[1][i],4)])\n",
    "print(tabulate(table, headers=[\"Palabra\", \"Pr. Cine\", \"Pr. Literatura\"]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autor 0.041\n",
      "libro 0.041\n",
      "------ Probabilidad de ser de la clase Cine: 0.0009221 \n",
      "\n",
      "autor 0.052\n",
      "libro 0.1\n",
      "------ Probabilidad de ser de la clase Literatura: 0.002328 \n",
      "\n",
      "\n",
      "Predicción: Literatura\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clasificación\n",
    "def predict(doc, vocab, prior, conditional):\n",
    "    x = np.zeros(len(vocab))\n",
    "    for word in doc:\n",
    "        j = vocab.index(word)\n",
    "        x[j] += 1\n",
    "    p = prior.copy()\n",
    "    for i in range(len(p)):\n",
    "        for j in range(len(x)):\n",
    "            if vocab[j] in doc:\n",
    "                print(vocab[j],  round(conditional[i][j],3))\n",
    "            p[i] *= conditional[i][j] ** x[j]\n",
    "        print(\"------ Probabilidad de ser de la clase \" + label_names[i] + \":\", round(p[i],7), \"\\n\")\n",
    "    return np.argmax(p)\n",
    "\n",
    "docs = [\"el libro es de un autor.\"]\n",
    "# docs = [\"el libro es de un autor de trama.\"]\n",
    "# docs = [\"el libro es de un autor aclamado.\"]\n",
    "\n",
    "docs = preprocess(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"\\nPredicción: \" + label_names[predict(doc, vocab, prior, conditional)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate en las probabilidades que nos resultan para cada clase, son muy pequeñas. Si el tamaño de nuestro vocabulario fuera mucho mayor (lo que sería muy normal) las probabilidades serían aún mucho más pequeñas y podríamos tener problemas de precisión numérica para calcularlas. Es una práctica común y recomendada transformar las probabilidades con logaritmos cuando se trabaja con Naive Bayes, precisamente para evitar problemas de precisión numérica. Los productos de probabilidades pequeñas pueden acercarse a cero en la aritmética de punto flotante, lo que puede dar lugar a errores o inestabilidades.\n",
    "\n",
    "Dada la propiedad del logaritmo:\n",
    "$$ \\log(a \\times b) = \\log(a) + \\log(b) $$\n",
    "\n",
    "Puedes transformar las multiplicaciones de probabilidades en sumas de logaritmos. \n",
    "\n",
    "Si estás calculando:\n",
    "$$ P(C_k|\\text{documento}) \\propto P(C_k) \\times \\prod_{i} P(w_i|C_k) $$\n",
    "\n",
    "Puedes tomar el logaritmo en ambos lados:\n",
    "$$ \\log(P(C_k|\\text{documento})) \\propto \\log(P(C_k)) + \\sum_{i} \\log(P(w_i|C_k)) $$\n",
    "\n",
    "Al clasificar un documento, calculas el valor anterior para cada clase y eliges la clase con el valor más alto. No es necesario convertir estos valores de nuevo usando la función exponencial porque el logaritmo es una función monótona creciente. Por lo tanto, si $ \\log(a) > \\log(b) $, entonces $ a > b $.\n",
    "\n",
    "Al trabajar con sumas en lugar de productos, evitas los problemas de precisión numérica y, además, el cálculo se vuelve computacionalmente más eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "autor -3.201\n",
      "libro -3.201\n",
      "------ Probabilidad de ser de la clase Cine: -6.988840037302129 \n",
      "\n",
      "autor -2.949\n",
      "libro -2.303\n",
      "------ Probabilidad de ser de la clase Literatura: -6.062727567129474 \n",
      "\n",
      "\n",
      "Predicción: Literatura\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def predict(doc, vocab, prior, conditional):\n",
    "    x = np.zeros(len(vocab))\n",
    "    for word in doc:\n",
    "        j = vocab.index(word)\n",
    "        x[j] += 1\n",
    "    # Tomar el logaritmo de los priors\n",
    "    p = [math.log(prior_val) for prior_val in prior]\n",
    "    for i in range(len(p)):\n",
    "        for j in range(len(x)):\n",
    "            if vocab[j] in doc:\n",
    "                print(vocab[j], round(math.log(conditional[i][j]), 3))\n",
    "            # Sumar el logaritmo de las probabilidades\n",
    "            p[i] += x[j] * math.log(conditional[i][j])\n",
    "        print(\"------ Probabilidad de ser de la clase \" + label_names[i] + \":\", p[i], \"\\n\")\n",
    "    return np.argmax(p)\n",
    "\n",
    "docs = [\"el libro es de un autor.\"]\n",
    "\n",
    "docs = preprocess(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nPredicción: \" + label_names[predict(doc, vocab, prior, conditional)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 1\n",
    "\n",
    "Modifica el código anterior para poder hacer la clasificación entre tres categorías: \"Cine\", \"Literatura\" y \"Música\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\",\n",
    "    \"El nuevo álbum de la banda es increíble.\",\n",
    "    \"El concierto en el estadio estuvo lleno.\",\n",
    "    \"La guitarra eléctrica tiene un sonido potente y claro.\",\n",
    "    \"Los festivales de música al aire libre son mis favoritos.\",\n",
    "    \"El pianista interpretó una pieza clásica maravillosamente.\",\n",
    "    \"La lista de reproducción incluye varios géneros, desde jazz hasta rock.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2]\n",
    "\n",
    "label_names = {0: \"Cine\", 1: \"Literatura\", 2: \"Música\"}\n",
    "\n",
    "documents_test = [\n",
    "    \"El libro estaba basado en emocionantes paisajes.\",\n",
    "    \"El guion fue aclamado por su trama intrigante.\",\n",
    "    \"La banda tocó jazz y rock en el concierto.\",\n",
    "    \"El cine muestra películas emocionantes de acción.\",\n",
    "    \"El pianista tocó una pieza de música clásica.\"\n",
    "]\n",
    "\n",
    "labels_test = [1, 0, 2, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 2\n",
    "\n",
    "¿Qué ocurre cuando tenemos palabras en un conjunto de test que no están en el vocabulario del conjunto de entrenamiento? ¿Cómo podríamos solucionarlo?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NLTK**\n",
    "https://www.nltk.org/index.html\n",
    "\n",
    "NLTK, que significa \"Natural Language Toolkit\", es una biblioteca líder en Python para trabajar con datos de lenguaje humano. Fue creado con el objetivo de facilitar la investigación y el desarrollo en el procesamiento del lenguaje natural (NLP) y la lingüística computacional.\n",
    "\n",
    "Algunas características y capacidades clave de NLTK incluyen:\n",
    "\n",
    "1. **Tokenización**: Divide el texto en palabras o frases.\n",
    "2. **Stemming y Lemmatization**: Reduce las palabras a su raíz o forma base.\n",
    "3. **Clasificación**: Herramientas y algoritmos para etiquetar o categorizar texto.\n",
    "4. **Análisis de Sentimiento**: Determinar la actitud o emoción expresada en un fragmento de texto.\n",
    "5. **Parsing**: Analizar la estructura gramatical de las frases.\n",
    "6. **Entidades con nombre**: Identificación de nombres de personas, lugares, organizaciones, etc.\n",
    "7. **Conjuntos de datos y corpus**: NLTK incluye una variedad de corpus y conjuntos de datos para entrenamiento y experimentación en NLP.\n",
    "8. **WordNet Integration**: Un diccionario léxico semántico para el inglés, que puede ser usado para encontrar sinónimos, antónimos, hiperónimos, hipónimos, etc.\n",
    "9. **Algoritmos de Machine Learning**: Incluye implementaciones básicas de varios algoritmos de aprendizaje automático, especialmente útiles para clasificación de texto.\n",
    "\n",
    "NLTK es ampliamente utilizado en la academia y la industria debido a su versatilidad, amplia gama de funcionalidades y comunidad activa. Es especialmente útil para prototipos y enseñanza debido a su diseño claro y documentación extensa. Sin embargo, para ciertas aplicaciones de producción o tareas que requieren alta eficiencia, otros frameworks o bibliotecas, como spaCy o scikit-learn, podrían ser más adecuados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Datos de entrenamiento\n",
    "texts = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\",\n",
    "    \"El nuevo álbum de la banda es increíble.\",\n",
    "    \"El concierto en el estadio estuvo lleno.\",\n",
    "    \"La guitarra eléctrica tiene un sonido potente y claro.\",\n",
    "    \"Los festivales de música al aire libre son mis favoritos.\",\n",
    "    \"El pianista interpretó una pieza clásica maravillosamente.\",\n",
    "    \"La lista de reproducción incluye varios géneros, desde jazz hasta rock.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2]\n",
    "\n",
    "label_names = {0: \"Cine\", 1: \"Literatura\", 2: \"Música\"}\n",
    "\n",
    "# Convertir texto a formato NLTK\n",
    "documents = [(word_tokenize(text), label) for text, label in zip(texts, labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in word_tokenize(' '.join(texts)))\n",
    "word_features = list(all_words)[:2000]  # Usamos las 2000 palabras más comunes como características, aunque en este ejemplo no es necesario ya que son pocas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set = featuresets\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"El libro estaba basado en emocionantes paisajes.\",\n",
    "    \"El guion fue aclamado por su trama intrigante.\",\n",
    "    \"La banda tocó jazz y rock en el concierto.\",\n",
    "    \"El cine muestra películas emocionantes de acción.\",\n",
    "    \"El pianista tocó una pieza de música clásica.\"\n",
    "]\n",
    "\n",
    "labels_test = [1, 0, 2, 0, 2]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(classifier.classify(document_features(word_tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/sklearn-logo.png\" width=\"250\"/>\n",
    "\n",
    "## **scikit-learn**\n",
    "https://scikit-learn.org/stable/index.html\n",
    "\n",
    "`scikit-learn` (comúnmente conocido como `sklearn`) es una biblioteca de código abierto para Python que proporciona herramientas simples y eficientes para el análisis y modelado de datos. Está construida sobre `NumPy`, `SciPy` y `matplotlib`. Aunque no está diseñada específicamente para el procesamiento del lenguaje natural (NLP), ofrece herramientas y utilidades que son ampliamente utilizadas en tareas de NLP.\n",
    "\n",
    "Aquí hay un resumen de sus características principales:\n",
    "\n",
    "1. **Modelos de Aprendizaje Supervisado**: Incluye algoritmos como máquinas de soporte vectorial, regresión logística, árboles de decisión, random forests, gradient boosting, k-vecinos más cercanos y muchos otros.\n",
    "\n",
    "2. **Modelos de Aprendizaje No Supervisado**: Incluye algoritmos como clustering (k-means, clustering jerárquico, DBSCAN), reducción de dimensionalidad (PCA, t-SNE, NMF), y detección de outliers.\n",
    "\n",
    "3. **Herramientas de Selección y Evaluación de Modelos**: Proporciona métodos para la validación cruzada, ajuste de hiperparámetros (como búsqueda en grilla y búsqueda aleatoria), métricas de rendimiento, y más.\n",
    "\n",
    "4. **Transformadores y Pipelines**: `sklearn` incluye herramientas para preprocesar datos, como normalización, estandarización, codificación one-hot, y otras transformaciones. La funcionalidad de \"pipelines\" permite combinar transformadores y estimadores en una secuencia coherente de pasos de procesamiento.\n",
    "\n",
    "5. **Interoperabilidad**: `scikit-learn` es compatible con estructuras de datos de Python estándar como listas y arrays de `NumPy`, y también con estructuras de datos de `pandas`.\n",
    "\n",
    "6. **Documentación y Comunidad**: Una de las grandes ventajas de `scikit-learn` es su extensa documentación que incluye muchos ejemplos prácticos. Además, tiene una comunidad activa que contribuye regularmente a su desarrollo y mejora.\n",
    "\n",
    "7. **Diseño Consistente**: Una de las características distintivas de `sklearn` es su API coherente. Una vez que te familiarizas con los fundamentos (como `fit()`, `transform()`, `predict()`), es fácil de usar con muchos algoritmos diferentes.\n",
    "\n",
    "Dado que es una herramienta versátil y ampliamente adoptada, `scikit-learn` es una elección popular para muchos profesionales del campo de la ciencia de datos y el aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Datos de entrenamiento\n",
    "texts = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\",\n",
    "    \"El nuevo álbum de la banda es increíble.\",\n",
    "    \"El concierto en el estadio estuvo lleno.\",\n",
    "    \"La guitarra eléctrica tiene un sonido potente y claro.\",\n",
    "    \"Los festivales de música al aire libre son mis favoritos.\",\n",
    "    \"El pianista interpretó una pieza clásica maravillosamente.\",\n",
    "    \"La lista de reproducción incluye varios géneros, desde jazz hasta rock.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `scikit-learn`, un **Pipeline** es una forma de automatizar y simplificar un flujo de trabajo que involucra múltiples pasos de procesamiento y modelado. A menudo, en el aprendizaje automático, los datos pasan por una serie de etapas de preprocesamiento antes de ser utilizados por un algoritmo de aprendizaje. Un Pipeline ayuda a definir y automatizar estos pasos secuenciales.\n",
    "\n",
    "`CountVectorizer` implementa el modelo \"Bag-of-Words\" (BoW). El enfoque BoW se refiere al proceso de convertir texto en una representación numérica basada en la frecuencia de las palabras en el texto, sin tener en cuenta el orden o la estructura de las frases.\n",
    "\n",
    "Cuando utilizas `CountVectorizer`, básicamente estás aplicando el proceso de BoW:\n",
    "\n",
    "1. **Tokenización**: Divide el texto en palabras individuales (o tokens).\n",
    "  \n",
    "2. **Construcción del vocabulario**: Se crea un vocabulario con todas las palabras únicas que aparecen en el conjunto de datos.\n",
    "\n",
    "3. **Vectorización**: Se codifica cada documento en función de la frecuencia de las palabras del vocabulario en ese documento.\n",
    "\n",
    "El resultado es una matriz en la que cada fila representa un documento y cada columna representa una palabra del vocabulario. El valor en una posición específica de la matriz indica la frecuencia con la que la palabra (columna) aparece en el documento (fila).\n",
    "\n",
    "`MultinomialNB` implementa el algoritmo de Naive Bayes multinomial diseñado especialmente para características discretas (como cuentas de palabras en textos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "model.fit(texts, labels)  # Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Frases de test\n",
    "test_texts = [\n",
    "    \"El libro estaba basado en emocionantes paisajes.\",\n",
    "    \"El guion fue aclamado por su trama intrigante.\",\n",
    "    \"La banda tocó jazz y rock en el concierto.\",\n",
    "    \"El cine muestra películas emocionantes de acción.\",\n",
    "    \"El pianista tocó una pieza de música clásica.\"\n",
    "]\n",
    "\n",
    "labels_test = [1, 0, 2, 0, 2]\n",
    "\n",
    "# Predicciones\n",
    "predicted_labels = model.predict(test_texts)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

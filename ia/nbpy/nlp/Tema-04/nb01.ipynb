{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# **Estrategias de clasificación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aprendizaje supervisado en clasificación de texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clasificador Naive Bayes**\n",
    "\n",
    "El algoritmo **Naive Bayes** es un clasificador probabilístico basado en el **teorema de Bayes** con supuestos de independencia entre las características. Es especialmente adecuado para la clasificación de texto. Antes, recordemos algunos conceptos básicos:\n",
    "\n",
    "**Teorema de Bayes:**\n",
    "El fundamento detrás del algoritmo Naive Bayes es el teorema de Bayes, que se formula como:\n",
    "\n",
    "$$ P(C|D) = \\frac{P(D|C) * P(C)}{P(D)} $$\n",
    "\n",
    "Donde:\n",
    "- $P(C|D)$ es la probabilidad posterior de la clase C dado un dato D.\n",
    "- $P(D|C)$ es la probabilidad de observar el dato D dado que pertenece a la clase C.\n",
    "- $P(C)$ es la probabilidad a priori de la clase C.\n",
    "- $P(D)$ es la probabilidad total de observar el dato D.\n",
    "\n",
    "\n",
    "**Independencia:**\n",
    "Dentro del Teorema de Bayes, el concepto de **independencia** se refiere a la suposición de que ciertas variables o eventos no afectan la probabilidad de otros eventos o variables.\n",
    "\n",
    "Cuando hablamos del clasificador Naive Bayes, nos referimos específicamente a la **independencia condicional** de las características dado un resultado o clase particular. Esta es la suposición \"ingenua\" (naive) que le da nombre al método.\n",
    "\n",
    "En términos matemáticos, la independencia condicional en Naive Bayes se expresa así:\n",
    "\n",
    "$ P(X_1, X_2, ..., X_n | Y) = P(X_1 | Y) \\times P(X_2 | Y) \\times ... \\times P(X_n | Y) $\n",
    "\n",
    "Donde:\n",
    "- $ X_1, X_2, ..., X_n $ son las características (por ejemplo, en clasificación de texto, estas podrían ser palabras o frases).\n",
    "- $ Y $ es una clase particular (por ejemplo, una etiqueta como \"spam\" o \"no spam\").\n",
    "\n",
    "Lo que esto significa es que, dado un valor particular de $ Y $, la probabilidad conjunta de todas las características es simplemente el producto de sus probabilidades individuales. En otras palabras, estamos asumiendo que la presencia (o ausencia) de una característica no afecta la presencia (o ausencia) de cualquier otra característica, siempre que conozcamos la clase $ Y $.\n",
    "\n",
    "Este supuesto simplifica enormemente los cálculos y, aunque rara vez es cierto en la práctica (especialmente en el procesamiento del lenguaje natural donde las palabras están frecuentemente relacionadas entre sí), el clasificador Naive Bayes puede ser sorprendentemente eficaz en muchas situaciones a pesar de su suposición de independencia.\n",
    "\n",
    "\n",
    "\n",
    "#### **Ejemplo**\n",
    "\n",
    "Veamos un ejemplo. Supongamos que queremos clasificar frases entre las categorías \"Cine\" y \"Literatura\". Las frases de entrenamiento son:\n",
    "\n",
    "**Frases de entrenamiento:**\n",
    "1. \"La película fue emocionante y llena de acción.\" - Cine\n",
    "2. \"Ese libro tiene una trama intrigante.\" - Literatura\n",
    "3. \"Los actores hicieron un trabajo excelente.\" - Cine\n",
    "4. \"El autor describe paisajes con gran detalle.\" - Literatura\n",
    "5. \"El cine de autor siempre me ha fascinado.\" - Cine\n",
    "6. \"La novela estaba llena de giros inesperados.\" - Literatura\n",
    "7. \"El guion de esa película fue escrito por un famoso novelista.\" - Cine\n",
    "8. \"Los personajes del libro eran muy realistas.\" - Literatura\n",
    "9. \"Esa película está basada en un libro aclamado.\" - Cine\n",
    "\n",
    "Algunas palabras, como \"libro\", \"película\", y \"autor\", aparecen en ambas categorías.\n",
    "\n",
    "La probabilidad a priori de cada categoría es:\n",
    "\n",
    "$$ P(Cine) = \\frac{5}{9} $$\n",
    "$$ P(Literatura) = \\frac{4}{9} $$\n",
    "\n",
    "Esto viene a significar que una frase a clasificar tiene, a priori, una probabilidad de $ \\frac{5}{9} $ de ser de la categoría \"Cine\" y una probabilidad de $ \\frac{4}{9} $ de ser de la categoría \"Literatura\".\n",
    "\n",
    "Ahora, calculamos las probabilidades condicionales para cada palabra en cada categoría (eliminamos previamente las stop-words). Fíjate que hay palabras que aparecen en ambas categorías. Por ejemplo, la palabra \"libro\" aparece en dos frases de \"Literatura\" y en una frase de \"Cine\". La palabra \"autor\" aparece en una frase de \"Literatura\" y en otra frase de \"Cine\". Y así sucesivamente. Por tanto, tenemos que:\n",
    "\n",
    "$$ P(libro|Literatura) = \\frac{2}{10} $$\n",
    "$$ P(libro|Cine) = \\frac{1}{10} $$\n",
    "$$ P(autor|Cine) = \\frac{1}{10} $$\n",
    "$$ P(autor|Literatura) = \\frac{1}{10} $$\n",
    "\n",
    "...y así sucesivamente para las demás palabras.\n",
    "\n",
    "Nos podemos dar cuenta de que puede haber palabras que no aparezcan en una categoría. Por ejemplo, la palabra \"paisajes\" no aparece en ninguna frase de \"Cine\". En este caso, la probabilidad condicional es cero:\n",
    "\n",
    "$$ P(paisajes|Cine) = 0 $$\n",
    "\n",
    "Esto es un problema, porque si multiplicamos muchas probabilidades condicionales, el resultado será cero. Para evitar esto, podemos usar un **suavizado** (smoothing) para evitar que las probabilidades condicionales sean cero. Por ejemplo, podemos usar el suavizado de Laplace, que consiste en sumar un valor $\\alpha$ (normalmente, 1) al numerador y el número de palabras únicas (vocabulario) en el denominador.\n",
    "\n",
    "$$ P(w_i|C_k) = \\frac{\\text{Número de veces que } w_i \\text{ aparece en } C_k + \\alpha}{\\text{Total de palabras en } C_k + \\alpha \\times \\text{Tamaño del vocabulario}} $$\n",
    "\n",
    "...y así sucesivamente para las demás palabras.\n",
    "\n",
    "El motivo de sumar el tamaño del vocabulario en el denominador es para que la suma de todas las probabilidades condicionales sea 1.\n",
    "\n",
    "Vayamos haciendo un script en Python para calcular las probabilidades condicionales de cada palabra en cada categoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades a priori:\n",
      "Cine 0.556\n",
      "Literatura 0.444\n",
      "\n",
      "Palabra        Pr. Cine    Pr. Literatura\n",
      "-----------  ----------  ----------------\n",
      "intrigante       0.0037            0.0524\n",
      "actores          0.0407            0.0048\n",
      "trabajo          0.0407            0.0048\n",
      "giros            0.0037            0.0524\n",
      "autor            0.0407            0.0524\n",
      "novela           0.0037            0.0524\n",
      "basada           0.0407            0.0048\n",
      "trama            0.0037            0.0524\n",
      "escrito          0.0407            0.0048\n",
      "gran             0.0037            0.0524\n",
      "novelista        0.0407            0.0048\n",
      "guion            0.0407            0.0048\n",
      "personajes       0.0037            0.0524\n",
      "siempre          0.0407            0.0048\n",
      "detalle          0.0037            0.0524\n",
      "llena            0.0407            0.0524\n",
      "emocionante      0.0407            0.0048\n",
      "película         0.1148            0.0048\n",
      "describe         0.0037            0.0524\n",
      "inesperados      0.0037            0.0524\n",
      "acción           0.0407            0.0048\n",
      "realistas        0.0037            0.0524\n",
      "libro            0.0407            0.1\n",
      "paisajes         0.0037            0.0524\n",
      "aclamado         0.0407            0.0048\n",
      "fascinado        0.0407            0.0048\n",
      "excelente        0.0407            0.0048\n",
      "famoso           0.0407            0.0048\n",
      "cine             0.0407            0.0048\n",
      "hicieron         0.0407            0.0048\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "documents = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "label_names = {0: \"Cine\", 1: \"Literatura\"}\n",
    "\n",
    "# Preprocesamiento\n",
    "def preprocess(docs):\n",
    "    txts = [doc.lower().replace(\".\", \"\") for doc in docs]\n",
    "    txts = [doc.split() for doc in txts]\n",
    "    # Eliminación de stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"spanish\")\n",
    "    txts = [[word for word in doc if word not in stopwords] for doc in txts]\n",
    "    return txts\n",
    "\n",
    "documents = preprocess(documents)\n",
    "\n",
    "# Clasificador Naive Bayes\n",
    "\n",
    "# Creación del vocabulario\n",
    "def get_vocab(docs):\n",
    "    vocab = set()\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            vocab.add(word)\n",
    "    vocab = list(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(documents)\n",
    "\n",
    "# Creación de la matriz de características\n",
    "import numpy as np\n",
    "X = np.zeros((len(documents), len(vocab)))\n",
    "for i, doc in enumerate(documents):\n",
    "    for word in doc:\n",
    "        j = vocab.index(word)\n",
    "        X[i][j] += 1\n",
    "\n",
    "# Probabilidades a priori\n",
    "def get_prior(labels):\n",
    "    prior = np.zeros(2)\n",
    "    for label in labels:\n",
    "        prior[label] += 1\n",
    "    prior = prior / len(labels)\n",
    "    return prior\n",
    "\n",
    "prior = get_prior(labels)\n",
    "\n",
    "# Probabilidades condicionales\n",
    "def get_conditional(X, labels, alpha=1):\n",
    "    conditional = np.ones((2, X.shape[1])) * alpha\n",
    "    for i, label in enumerate(labels):\n",
    "        conditional[label] += X[i]\n",
    "    conditional = conditional / (conditional.sum(axis=1).reshape(-1, 1) + alpha * len(vocab))\n",
    "    return conditional\n",
    "\n",
    "conditional = get_conditional(X, labels, 0.1)\n",
    "\n",
    "print(\"Probabilidades a priori:\")\n",
    "print(label_names[0], round(prior[0], 3))\n",
    "print(label_names[1], round(prior[1], 3))\n",
    "print()\n",
    "\n",
    "from tabulate import tabulate\n",
    "table = []\n",
    "for i, w in enumerate(vocab):\n",
    "    table.append([w, round(conditional[0][i],4), round(conditional[1][i],4)])\n",
    "print(tabulate(table, headers=[\"Palabra\", \"Pr. Cine\", \"Pr. Literatura\"]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autor 0.041\n",
      "libro 0.041\n",
      "------ Probabilidad de ser de la clase Cine: 0.0009221 \n",
      "\n",
      "autor 0.052\n",
      "libro 0.1\n",
      "------ Probabilidad de ser de la clase Literatura: 0.002328 \n",
      "\n",
      "\n",
      "Predicción: Literatura\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clasificación\n",
    "def predict(doc, vocab, prior, conditional):\n",
    "    x = np.zeros(len(vocab))\n",
    "    for word in doc:\n",
    "        j = vocab.index(word)\n",
    "        x[j] += 1\n",
    "    p = prior.copy()\n",
    "    for i in range(len(p)):\n",
    "        for j in range(len(x)):\n",
    "            if vocab[j] in doc:\n",
    "                print(vocab[j],  round(conditional[i][j],3))\n",
    "            p[i] *= conditional[i][j] ** x[j]\n",
    "        print(\"------ Probabilidad de ser de la clase \" + label_names[i] + \":\", round(p[i],7), \"\\n\")\n",
    "    return np.argmax(p)\n",
    "\n",
    "docs = [\"el libro es de un autor.\"]\n",
    "# docs = [\"el libro es de un autor de trama.\"]\n",
    "# docs = [\"el libro es de un autor aclamado.\"]\n",
    "\n",
    "docs = preprocess(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"\\nPredicción: \" + label_names[predict(doc, vocab, prior, conditional)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate en las probabilidades que nos resultan para cada clase, son muy pequeñas. Si el tamaño de nuestro vocabulario fuera mucho mayor (lo que sería muy normal) las probabilidades serían aún mucho más pequeñas y podríamos tener problemas de precisión numérica para calcularlas. Es una práctica común y recomendada transformar las probabilidades con logaritmos cuando se trabaja con Naive Bayes, precisamente para evitar problemas de precisión numérica. Los productos de probabilidades pequeñas pueden acercarse a cero en la aritmética de punto flotante, lo que puede dar lugar a errores o inestabilidades.\n",
    "\n",
    "Dada la propiedad del logaritmo:\n",
    "$$ \\log(a \\times b) = \\log(a) + \\log(b) $$\n",
    "\n",
    "Puedes transformar las multiplicaciones de probabilidades en sumas de logaritmos. \n",
    "\n",
    "Si estás calculando:\n",
    "$$ P(C_k|\\text{documento}) \\propto P(C_k) \\times \\prod_{i} P(w_i|C_k) $$\n",
    "\n",
    "Puedes tomar el logaritmo en ambos lados:\n",
    "$$ \\log(P(C_k|\\text{documento})) \\propto \\log(P(C_k)) + \\sum_{i} \\log(P(w_i|C_k)) $$\n",
    "\n",
    "Al clasificar un documento, calculas el valor anterior para cada clase y eliges la clase con el valor más alto. No es necesario convertir estos valores de nuevo usando la función exponencial porque el logaritmo es una función monótona creciente. Por lo tanto, si $ \\log(a) > \\log(b) $, entonces $ a > b $.\n",
    "\n",
    "Al trabajar con sumas en lugar de productos, evitas los problemas de precisión numérica y, además, el cálculo se vuelve computacionalmente más eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "autor -3.201\n",
      "libro -3.201\n",
      "------ Probabilidad de ser de la clase Cine: -6.988840037302129 \n",
      "\n",
      "autor -2.949\n",
      "libro -2.303\n",
      "------ Probabilidad de ser de la clase Literatura: -6.062727567129474 \n",
      "\n",
      "\n",
      "Predicción: Literatura\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def predict(doc, vocab, prior, conditional):\n",
    "    x = np.zeros(len(vocab))\n",
    "    for word in doc:\n",
    "        j = vocab.index(word)\n",
    "        x[j] += 1\n",
    "    # Tomar el logaritmo de los priors\n",
    "    p = [math.log(prior_val) for prior_val in prior]\n",
    "    for i in range(len(p)):\n",
    "        for j in range(len(x)):\n",
    "            if vocab[j] in doc:\n",
    "                print(vocab[j], round(math.log(conditional[i][j]), 3))\n",
    "            # Sumar el logaritmo de las probabilidades\n",
    "            p[i] += x[j] * math.log(conditional[i][j])\n",
    "        print(\"------ Probabilidad de ser de la clase \" + label_names[i] + \":\", p[i], \"\\n\")\n",
    "    return np.argmax(p)\n",
    "\n",
    "docs = [\"el libro es de un autor.\"]\n",
    "\n",
    "docs = preprocess(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nPredicción: \" + label_names[predict(doc, vocab, prior, conditional)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 1\n",
    "\n",
    "Modifica el código anterior para poder hacer la clasificación entre tres categorías: \"Cine\", \"Literatura\" y \"Música\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\",\n",
    "    \"El nuevo álbum de la banda es increíble.\",\n",
    "    \"El concierto en el estadio estuvo lleno.\",\n",
    "    \"La guitarra eléctrica tiene un sonido potente y claro.\",\n",
    "    \"Los festivales de música al aire libre son mis favoritos.\",\n",
    "    \"El pianista interpretó una pieza clásica maravillosamente.\",\n",
    "    \"La lista de reproducción incluye varios géneros, desde jazz hasta rock.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2]\n",
    "\n",
    "label_names = {0: \"Cine\", 1: \"Literatura\", 2: \"Música\"}\n",
    "\n",
    "documents_test = [\n",
    "    \"El libro estaba basado en emocionantes paisajes.\",\n",
    "    \"El guion fue aclamado por su trama intrigante.\",\n",
    "    \"La banda tocó jazz y rock en el concierto.\",\n",
    "    \"El cine muestra películas emocionantes de acción.\",\n",
    "    \"El pianista tocó una pieza de música clásica.\"\n",
    "]\n",
    "\n",
    "labels_test = [1, 0, 2, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 2\n",
    "\n",
    "¿Qué ocurre cuando tenemos palabras en un conjunto de test que no están en el vocabulario del conjunto de entrenamiento? ¿Cómo podríamos solucionarlo?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

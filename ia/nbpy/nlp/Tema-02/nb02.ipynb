{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"../imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 2**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelos estadísticos del lenguaje**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelos markovianos**\n",
    "\n",
    "Los modelos markovianos del lenguaje, también conocidos como modelos de Markov de lenguaje, son una clase de modelos estadísticos que se utilizan para predecir la probabilidad de una secuencia de palabras en un texto. Estos modelos se basan en la teoría de procesos estocásticos de Markov.\n",
    "\n",
    "#### **Proceso de Markov**\n",
    "Un proceso de Markov es un tipo de modelo estadístico que predice el futuro de un proceso estocástico basándose únicamente en su estado actual, y no en cómo llegó a ese estado (esto es, no tiene memoria). Por ejemplo, el tiempo atmosférico de día a día podría ser simulado de esta forma:\n",
    "\n",
    "<DIV ALIGN=\"center\">\n",
    "<img src=\"imgs/markov.svg\" width=\"50%\">\n",
    "</DIV>\n",
    "\n",
    "Esto quiere decir, por ejemplo, que si el martes está nublado hay un 10% de probabilidades de que el miércoles esté soleado. Las probabilidades las podemos representar también mediante una tabla de probabilidades de transición, donde la suma de cada fila debe ser 1.\n",
    "\n",
    "|          | soleado | nublado | lluvioso |\n",
    "|----------|---------|---------|----------|\n",
    "| **soleado**  | 0\\.6    | 0\\.3    | 0\\.1     |\n",
    "| **nublado**  | 0\\.1    | 0\\.5    | 0\\.4     |\n",
    "| **lluvioso** | 0\\.6    | 0\\.2    | 0\\.2     |\n",
    "\n",
    "Matemáticamente se expresaría así:\n",
    "\n",
    "$$ P[s_{t+1} | s_t] = P[s_{t+1} | s_t, s_{t-1}, \\cdots ,s_1 ] $$\n",
    "\n",
    "#### **Cadena de Markov**\n",
    "Una cadena de Markov es una secuencia de eventos, cada uno de los cuales depende solo del evento que lo precedió. En el ejemplo anterior, la cadena de Markov sería:\n",
    "\n",
    "\"soleado→nublado→nublado→lluvioso\"\n",
    "\n",
    "Esta cadena en particular tendría la siguiente probabilidad de aparición:\n",
    "\n",
    "$$ P[soleado→nublado→nublado→lluvioso] = P[soleado] \\cdot P[nublado|soleado] \\cdot P[nublado|nublado] \\cdot P[lluvioso|nublado] $$\n",
    "\n",
    "Lo cual es:\n",
    "\n",
    "$$ P[soleado→nublado→nublado→lluvioso] = 0.43 \\cdot 0.6 \\cdot 0.5 \\cdot 0.4 = 0.0516 $$\n",
    "\n",
    "En el contexto de los modelos de lenguaje, los eventos son palabras o símbolos.\n",
    "\n",
    "\n",
    "## **Modelos de Lenguaje**\n",
    "\n",
    "Un **modelo de lenguaje** es un modelo matemático y computacional que está diseñado para predecir la probabilidad de una secuencia de palabras en un lenguaje dado. Los modelos de lenguaje pueden generar texto de una manera que sigue las normas gramaticales y estilísticas del lenguaje que están modelando. Los modelos más sencillos que podemos construir son los **modelos markovianos**. Es decir, modelos que asumen que la probabilidad de una palabra depende solo de la palabra que la precede. Es posible hacerlos aún más sencillos haciendo que cada palabra sea independiente de las demás. Estos modelos se conocen como **modelos de unigramas**. Cuando depende de la anterior palabra, se conocen como **modelos de bigramas**. Cuando depende de las dos palabras anteriores, se conocen como **modelos de trigramas**. Y así sucesivamente. En general, se conocen como **modelos de n-gramas**. Matemáticamente, un modelo de lenguaje se puede expresar como:\n",
    "\n",
    "$$ P[w_1, w_2, \\cdots ,w_n] = \\prod_{i=1}^{n} P[w_i | w_1, \\cdots ,w_{i-1}] $$\n",
    "\n",
    "Donde $w_i$ es la i-ésima palabra de la secuencia.\n",
    "\n",
    "\n",
    "### **Entrenamiento de los Modelos**\n",
    "Para entrenar estos modelos, se utilizan corpus de texto para calcular las probabilidades condicionales de palabras dado su contexto histórico. Por ejemplo, para un modelo de bigramas, se calcula la probabilidad de que una palabra $w_i$ aparezca dado que la palabra anterior es $w_{i-1}$:\n",
    "\n",
    "#### Generación de Texto\n",
    "Una vez entrenado, un modelo de Markov puede usarse para generar texto nuevo, seleccionando palabras una a una de acuerdo a las probabilidades calculadas a partir del texto de entrenamiento.\n",
    "\n",
    "#### Ventajas\n",
    "1. **Simplicidad**: Los modelos de Markov son relativamente simples de entender y de implementar.\n",
    "2. **Eficiencia**: Pueden ser computacionalmente menos intensivos que otros modelos más complejos.\n",
    "\n",
    "#### Desventajas\n",
    "1. **Memoria Limitada**: Los modelos de Markov no capturan dependencias de largo alcance en el texto debido a su naturaleza de \"memoria corta\".\n",
    "2. **Espacio de Estado Grande**: Para modelos de n-gramas con \\(n\\) grande, el espacio de estados (y, por lo tanto, los requerimientos de memoria) pueden crecer exponencialmente.\n",
    "\n",
    "#### Aplicaciones\n",
    "Los modelos markovianos del lenguaje se utilizan en una variedad de aplicaciones, como reconocimiento de voz, procesamiento de lenguaje natural, generación de texto, entre otros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 1\n",
    "\n",
    "Crea un modelo del lenguaje basado en unigramas, bigramas y en trigramas para la novela \"Cien años de soledad\" de Gabriel García Márquez. Tokeniza en función de los caracteres, no de las palabras.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolución del ejercio anterior para el modelo de bigramas.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Carga el fichero de texto\n",
    "with open('novela.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Aquí están todos los caracteres únicos que aparecen en este texto\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# crea un mapeo de caracteres a enteros\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: toma una cadena, devuelve una lista de enteros\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: toma una lista de enteros, devuelve una cadena\n",
    "\n",
    "# Preparación de la tabla que contendrá los bigramas\n",
    "bigram_table = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "# Recorre el texto y cuenta los bigramas\n",
    "for i in range(len(text)-1):\n",
    "    bigram_table[stoi[text[i]], stoi[text[i+1]]] += 1\n",
    "\n",
    "# Normaliza las filas de la tabla\n",
    "bigram_table = bigram_table / bigram_table.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cana e sistricun pibonord pa l les ste as fasena abites, uatadesé Aule carase pra, esamo tatavioro y '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Genera un texto a partir de la tabla de bigramas\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_text(bigram_table, n_words, word=\"H\"):\n",
    "    text = [word]\n",
    "    key_vocab = list(range(vocab_size))\n",
    "    for i in range(n_words):\n",
    "        key = random.choices(key_vocab, bigram_table[stoi[text[-1]]])[0]\n",
    "        word = itos[key]\n",
    "        text.append(word)\n",
    "    return ''.join(text)\n",
    "\n",
    "generate_text(bigram_table, 100, \"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perplejidad (perplexity)**\n",
    "\n",
    "Para entender la **perplejidad**, primero necesitamos tener una noción básica sobre los modelos de lenguaje. Como decíamos antes, los modelos del lenguaje se entrenan para predecir la probabilidad de una palabra dado su contexto en una secuencia de palabras. Por tanto, la **perplejidad es una métrica** que nos ayuda a evaluar qué tan bien un modelo de lenguaje puede predecir una muestra de texto. Cuanto menor sea la perplejidad, mejor será el modelo en predecir palabras en un texto. \n",
    "\n",
    "Matemáticamente, la perplejidad se define como $2^{H(p)}$, donde $H(p)$ es la <a href=\"https://nbviewer.org/url/cayetanoguerra.github.io/ia/nbpy/misc/Entropy.ipynb\">entropía cruzada</a> de la distribución de probabilidad $p$, que representa las predicciones de nuestro modelo. La fórmula para calcular la perplejidad es:\n",
    "\n",
    "$$\n",
    "\\text{Perplejidad} = 2^{-\\frac{1}{N}\\sum_{i=1}^{N} \\log_2(p(x_i))}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $N$ es el número de palabras en la muestra.\n",
    "- $p(x_i)$ es la probabilidad asignada por el modelo a la palabra $x_i$.\n",
    "\n",
    "Podrías pensar que la fórmula anterior no es realmente la entropía cruzada ya que no aparece la otra distribución de probabilidad ($Q(x)$). Sin embargo, la distribución de probabilidad $Q(x)$ es la distribución real de las palabras en el texto en cada momento y asumimos que la palabra que aparece tiene probabilidad 1.\n",
    "\n",
    "La interpretación de la perplejidad es la siguiente:\n",
    "\n",
    "- **Baja Perplejidad**: El modelo asigna alta probabilidad a las palabras observadas, lo que indica una buena predicción.\n",
    "- **Alta Perplejidad**: El modelo asigna baja probabilidad a las palabras observadas, lo que indica una mala predicción.\n",
    "\n",
    "La perplejidad se utiliza principalmente en la fase de evaluación y ajuste de modelos de lenguaje, ayudando a seleccionar los mejores parámetros para el modelo y a comparar diferentes modelos entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.958406504127758\n",
      "861.7711344375253\n"
     ]
    }
   ],
   "source": [
    "# Calcula la perplejidad del modelo de lenguaje basado en bigram_table.\n",
    "\n",
    "def perplexity(bigram_table, text):\n",
    "    perplexity = 0\n",
    "    for i in range(len(text)-1):\n",
    "        c1 = stoi[text[i]]\n",
    "        c2 = stoi[text[i+1]]\n",
    "        probability = bigram_table[c1][c2]\n",
    "        if probability == 0:\n",
    "            probability = 1e-10\n",
    "        perplexity += np.log(probability)\n",
    "    perplexity = np.exp(-perplexity/len(text))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "txt1= \"Muchos años después, frente al pelotón de fusilamiento, el coronel Aureliano Buendía\"\n",
    "print(perplexity(bigram_table, txt1))\n",
    "\n",
    "# ¿Qué perplejidad obtienes para el texto invertido?\n",
    "print(perplexity(bigram_table, txt1[::-1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 2\n",
    "\n",
    "Calcula y compara la perplejidad para un mismo texto con los modelos de lenguaje basados en unigramas, bigramas y trigramas.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelos estadísticos del lenguaje**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelos markovianos**\n",
    "\n",
    "Los modelos markovianos del lenguaje, también conocidos como modelos de Markov de lenguaje, son una clase de modelos estadísticos que se utilizan para predecir la probabilidad de una secuencia de palabras en un texto. Estos modelos se basan en la teoría de procesos estocásticos de Markov.\n",
    "\n",
    "#### **1. Proceso de Markov**\n",
    "Un proceso de Markov es un tipo de modelo estadístico que predice el futuro de un proceso estocástico basándose únicamente en su estado actual, y no en cómo llegó a ese estado (esto es, no tiene memoria). Por ejemplo, el tiempo atmosférico de día a día podría ser simulado de esta forma:\n",
    "\n",
    "<img src=\"imgs/markov.svg\" width=\"30%\">\n",
    "\n",
    "Esto quiere decir, por ejemplo, que si el martes está nublado hay un 10% de probabilidades de que el miércoles esté soleado. Las probabilidades las podemos representar también mediante una tabla de probabilidades de transición, donde la suma de cada fila debe ser 1.\n",
    "\n",
    "|          | soleado | nublado | lluvioso |\n",
    "|----------|---------|---------|----------|\n",
    "| **soleado**  | 0\\.6    | 0\\.3    | 0\\.1     |\n",
    "| **nublado**  | 0\\.1    | 0\\.5    | 0\\.4     |\n",
    "| **lluvioso** | 0\\.6    | 0\\.2    | 0\\.2     |\n",
    "\n",
    "Matemáticamente se expresaría así:\n",
    "\n",
    "$$ P[s_{t+1} | s_t] = P[s_{t+1} | s_t, s_{t-1}, \\cdots ,s_1 ] $$\n",
    "\n",
    "#### **2. Cadena de Markov**\n",
    "Una cadena de Markov es una secuencia de eventos, cada uno de los cuales depende solo del evento que lo precedió. En el ejemplo anterior, la cadena de Markov sería:\n",
    "\n",
    "\"soleado→nublado→nublado→lluvioso\"\n",
    "\n",
    "Esta cadena en particular tendría la siguiente probabilidad de aparición:\n",
    "\n",
    "$$ P[soleado→nublado→nublado→lluvioso] = P[soleado] \\cdot P[nublado|soleado] \\cdot P[nublado|nublado] \\cdot P[lluvioso|nublado] $$\n",
    "\n",
    "Lo cual es:\n",
    "\n",
    "$$ P[soleado→nublado→nublado→lluvioso] = 0.43 \\cdot 0.6 \\cdot 0.5 \\cdot 0.4 = 0.0516 $$\n",
    "\n",
    "En el contexto de los modelos de lenguaje, los eventos son palabras o símbolos.\n",
    "\n",
    "\n",
    "### **Modelos de Lenguaje:**\n",
    "\n",
    "Un **modelo de lenguaje** es un modelo matemático y computacional que está diseñado para predecir la probabilidad de una secuencia de palabras en un lenguaje dado. Los modelos de lenguaje pueden generar texto de una manera que sigue las normas gramaticales y estilísticas del lenguaje que están modelando. \n",
    "\n",
    "#### **1. Modelos de Unigramas**\n",
    "Son los modelos markovianos más simples donde cada palabra se modela como un evento independiente, sin tener en cuenta las palabras anteriores.\n",
    "\n",
    "#### **2. Modelos de Bigramas**\n",
    "En estos modelos, la probabilidad de cada palabra solo depende de la palabra que la precede inmediatamente. La probabilidad de una secuencia de palabras se calcula como el producto de las probabilidades de cada bigrama (pares de palabras consecutivas) en la secuencia.\n",
    "\n",
    "#### **3. Modelos de Trigramas**\n",
    "Similar a los modelos de bigramas, pero aquí la probabilidad de cada palabra depende de las dos palabras que la preceden. \n",
    "\n",
    "#### **4. Modelos de n-gramas**\n",
    "Por extensión, se pueden definir modelos de n-gramas como aquellos donde la probabilidad de cada palabra depende de las n palabras que la preceden.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Entrenamiento de los Modelos**:\n",
    "Para entrenar estos modelos, se utilizan corpora de texto para calcular las probabilidades condicionales de palabras dado su contexto histórico.\n",
    "\n",
    "#### Generación de Texto:\n",
    "Una vez entrenado, un modelo de Markov puede usarse para generar texto nuevo, seleccionando palabras una a una de acuerdo a las probabilidades calculadas a partir del texto de entrenamiento.\n",
    "\n",
    "#### Ventajas:\n",
    "1. **Simplicidad**: Los modelos de Markov son relativamente simples de entender y de implementar.\n",
    "2. **Eficiencia**: Pueden ser computacionalmente menos intensivos que otros modelos más complejos.\n",
    "\n",
    "#### Desventajas:\n",
    "1. **Memoria Limitada**: Los modelos de Markov no capturan dependencias de largo alcance en el texto debido a su naturaleza de \"memoria corta\".\n",
    "2. **Espacio de Estado Grande**: Para modelos de n-gramas con \\(n\\) grande, el espacio de estados (y, por lo tanto, los requerimientos de memoria) pueden crecer exponencialmente.\n",
    "\n",
    "#### Aplicaciones:\n",
    "Los modelos markovianos del lenguaje se utilizan en una variedad de aplicaciones, como reconocimiento de voz, procesamiento de lenguaje natural, generación de texto, entre otros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicios:\n",
    "\n",
    "Crea un modelo del lenguaje basado en unigramas, bigramas y en trigramas para la novela \"Cien años de soledad\" de Gabriel García Márquez. Tokeniza en función de los caracteres, no de las palabras.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolución del ejercio anterior para el modelo de bigramas.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Carga el fichero de texto\n",
    "with open('novela.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Aquí están todos los caracteres únicos que aparecen en este texto\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# crea un mapeo de caracteres a enteros\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: toma una cadena, devuelve una lista de enteros\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: toma una lista de enteros, devuelve una cadena\n",
    "\n",
    "# Preparación de la tabla que contendrá los bigramas\n",
    "bigram_table = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "# Recorre el texto y cuenta los bigramas\n",
    "for i in range(len(text)-1):\n",
    "    bigram_table[stoi[text[i]], stoi[text[i+1]]] += 1\n",
    "\n",
    "# Normaliza las filas de la tabla\n",
    "bigram_table = bigram_table / bigram_table.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hel luvídra co, sanaquergagro-Cu da a dertauidasuió \\ns, ca n é calo \\nEl sporonaba Recostes e vos idin'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Genera un texto a partir de la tabla de bigramas\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_text(bigram_table, n_words, word=\"H\"):\n",
    "    text = [word]\n",
    "    key_vocab = list(range(vocab_size))\n",
    "    for i in range(n_words):\n",
    "        key = random.choices(list(key_vocab), bigram_table[stoi[ text[-1]  ]])[0]\n",
    "        word = itos[key]\n",
    "        text.append(word)\n",
    "    return ''.join(text)\n",
    "\n",
    "generate_text(bigram_table, 100, \"H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perplejidad (perplexity)**\n",
    "\n",
    "Para entender la **perplejidad**, primero necesitamos tener una noción básica sobre los modelos de lenguaje. Estos modelos se entrenan para predecir la probabilidad de una palabra dado su contexto en una secuencia de palabras. Por tanto, la **perplejidad es una métrica** que nos ayuda a evaluar qué tan bien un modelo de lenguaje puede predecir una muestra de texto. Cuanto menor sea la perplejidad, mejor será el modelo en predecir palabras en un texto. \n",
    "\n",
    "Matemáticamente, la perplejidad se define como $2^{H(p)}$, donde $H(p)$ es la <a href=\"https://nbviewer.org/url/cayetanoguerra.github.io/ia/nbpy/misc/Entropy.ipynb\">entropía cruzada</a> de la distribución de probabilidad $p$, que representa las predicciones de nuestro modelo. La fórmula para calcular la perplejidad es:\n",
    "\n",
    "$$\n",
    "\\text{Perplejidad} = 2^{-\\frac{1}{N}\\sum_{i=1}^{N} \\log_2(p(x_i))}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $N$ es el número de palabras en la muestra.\n",
    "- $p(x_i)$ es la probabilidad asignada por el modelo a la palabra $x_i$.\n",
    "\n",
    "La interpretación de la perplejidad es la siguiente:\n",
    "\n",
    "- **Baja Perplejidad**: El modelo asigna alta probabilidad a las palabras observadas, lo que indica una buena predicción.\n",
    "- **Alta Perplejidad**: El modelo asigna baja probabilidad a las palabras observadas, lo que indica una mala predicción.\n",
    "\n",
    "La perplejidad se utiliza principalmente en la fase de evaluación y ajuste de modelos de lenguaje, ayudando a seleccionar los mejores parámetros para el modelo y a comparar diferentes modelos entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.958406504127758\n",
      "861.7711344375253\n"
     ]
    }
   ],
   "source": [
    "# Calcula la perplejidad del modelo de lenguaje basado en bigram_table.\n",
    "\n",
    "def perplexity(bigram_table, text):\n",
    "    perplexity = 0\n",
    "    for i in range(len(text)-1):\n",
    "        c1 = stoi[text[i]]\n",
    "        c2 = stoi[text[i+1]]\n",
    "        probability = bigram_table[c1][c2]\n",
    "        if probability == 0:\n",
    "            probability = 1e-10\n",
    "        perplexity += np.log(probability)\n",
    "    perplexity = np.exp(-perplexity/len(text))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "txt1= \"Muchos años después, frente al pelotón de fusilamiento, el coronel Aureliano Buendía\"\n",
    "print(perplexity(bigram_table, txt1))\n",
    "print(perplexity(bigram_table, txt1[::-1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"../imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 21**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Afinamiento de modelos de lenguaje para dominios específicos**\n",
    "\n",
    "Se han alcanzado avances notables en la creación de modelos de lenguaje que sirven como base para una amplia variedad de aplicaciones. Sin embargo, para maximizar su efectividad en dominios específicos, es necesario un proceso de adaptación que asegure que el modelo entienda y responda correctamente dentro del contexto particular en el que se aplicará. Aquí es donde intervienen técnicas como el **entrenamiento autorregresivo** y el **ajuste fino supervisado** (SFT).\n",
    "\n",
    "#### **Entrenamiento Autorregresivo**\n",
    "\n",
    "El entrenamiento autorregresivo es un proceso mediante el cual los modelos de lenguaje aprenden a predecir la siguiente palabra en una secuencia, dados los tokens previos. Este método implica entrenar el modelo en una enorme cantidad de datos de texto en los cuales el modelo \"observa\" cada palabra o token y genera probabilísticamente la siguiente palabra, continuando hasta completar la secuencia. Esta técnica permite que el modelo \"aprenda\" las relaciones sintácticas y semánticas en el lenguaje, así como patrones comunes en la generación de texto.\n",
    "\n",
    "Los modelos autorregresivos, como GPT (Generative Pre-trained Transformer), son ideales para la generación de lenguaje natural, ya que pueden completar o generar texto que tenga coherencia y siga una estructura lógica. En contextos de dominio específico, esta capacidad es valiosa para personalizar respuestas o generar texto en campos especializados, como el jurídico, médico o técnico, donde el vocabulario y las convenciones lingüísticas difieren significativamente del lenguaje común.\n",
    "\n",
    "#### **Ajuste Fino Supervisado (Supervised Fine-Tuning o SFT)**\n",
    "\n",
    "Una vez que el modelo ha sido preentrenado de manera autorregresiva, el ajuste fino supervisado (SFT) se utiliza para adaptar el modelo a tareas o dominios específicos. En este proceso, el modelo recibe ejemplos etiquetados y supervisados de la tarea objetivo, que puede incluir respuestas a preguntas en un dominio particular, clasificaciones específicas o instrucciones de generación de texto que siguen convenciones particulares.\n",
    "\n",
    "El SFT permite que el modelo mejore su rendimiento en tareas específicas al refinar sus parámetros en función de ejemplos reales. A través de este ajuste, el modelo puede aprender matices y detalles que no se encuentran en el lenguaje común, lo que resulta en respuestas o generaciones de texto más precisas y relevantes para el dominio en cuestión.\n",
    "\n",
    "#### **Aplicación en Dominios Específicos**\n",
    "\n",
    "La combinación del entrenamiento autorregresivo y el SFT permite a los modelos no solo generar texto de manera fluida, sino también adaptarse al lenguaje y a las normas de un contexto especializado. En la educación, por ejemplo, un modelo afinado mediante SFT puede adaptarse para responder preguntas complejas de matemáticas o ciencias con mayor precisión, mientras que en el sector de la salud, el modelo puede proporcionar recomendaciones de tratamiento o explicaciones basadas en terminología médica precisa.\n",
    "\n",
    "Esta capacidad de personalización abre nuevas posibilidades para el uso de modelos de lenguaje en áreas específicas, optimizando su rendimiento y asegurando una mayor relevancia y exactitud en las respuestas.\n",
    "\n",
    "### **Materiales**\n",
    "\n",
    "En este enlace [notebooks y datasets](https://drive.google.com/file/d/18yruSmwBgM6oJ61sKjwaHJhQIklrdUfG/view?usp=sharing) vas a encontrar notebooks y datasets para el entrenamiento de modelos de lenguaje en dominios específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

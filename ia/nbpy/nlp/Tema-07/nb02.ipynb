{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"../imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 7.2**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BLIP**\n",
    "\n",
    "BLIP (Bootstrapping Language-Image Pre-training) es un modelo **multimodal** que combina Visión por Computador (CV) y PLN ([Junnan et al., 2022](https://arxiv.org/abs/2201.12086)). Está pensado para trabajar con pares imagen-texto y se usa para tareas como:\n",
    "\n",
    "* **Image captioning** (describir imágenes con texto).\n",
    "* **Visual Question Answering (VQA)** (responder preguntas sobre una imagen).\n",
    "* **Cross-modal retrieval** (buscar imágenes a partir de texto y viceversa).\n",
    "* En general, cualquier tarea donde haya que **entender conjuntamente imagen y lenguaje**.\n",
    "\n",
    "\n",
    "## **Arquitectura general de BLIP**\n",
    "\n",
    "BLIP tiene una arquitectura **encoder–decoder multimodal**, pensada para tanto **comprender** como **generar** texto condicionado por imágenes. \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./imgs/blip.png\" alt=\"BLIP Architecture\" width=\"900\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "La estructura básica tiene tres componentes principales:\n",
    "\n",
    "### **Unimodal Encoder (Codificador unimodal)**\n",
    "\n",
    "Se encarga de codificar **por separado** imagen y texto:\n",
    "\n",
    "* La imagen se codifica típicamente con un backbone tipo ViT / CNN que ofrece como resultados *embeddings* visuales.\n",
    "* El texto se procesa con un transformer de lenguaje que produce *embeddings* de tokens.\n",
    "\n",
    "En esta parte aún no hay interacción fuerte entre modalidades; se generan representaciones “puras” de cada una.\n",
    "\n",
    "### **Image-grounded Text Encoder (Codificador de texto condicionado por imagen)**\n",
    "\n",
    "Aquí ya hablamos de un transformer de texto que **incorpora información visual** mediante **cross-attention**. El flujo de procesamiento es:\n",
    "\n",
    "* Recibe como entrada los tokens de texto (p.ej. una frase candidata o un prompt).\n",
    "* Tiene acceso a los embeddings de la imagen como “memoria” en las capas de atención cruzada.\n",
    "* El objetivo es producir una representación de texto que **esté alineada con la imagen**; es decir, el texto se codifica sabiendo qué se ve en la imagen.\n",
    "\n",
    "### **Image-grounded Text Decoder (Decodificador de texto condicionado por imagen)**\n",
    "Es la parte **generativa** del modelo:\n",
    "\n",
    "* Usa **self-attention causal** (como un GPT) en los tokens de salida.\n",
    "* Además, usa **cross-attention** para consultar la representación visual.\n",
    "* A partir de la imagen (y opcionalmente un prompt textual), el decodificador va generando la secuencia palabra a palabra.\n",
    "\n",
    "En conjunto:\n",
    "\n",
    "* El encoder unimodal y el encoder “image-grounded” sirven para **entender** (clasificación, matching, VQA…).\n",
    "* El decoder “image-grounded” sirve para **generar texto** (captioning, respuestas completas, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objetivos de preentrenamiento de BLIP**\n",
    "\n",
    "El entrenamiento de BLIP se basa en **tres objetivos principales**, combinando contrastive learning, matching y language modeling.\n",
    "\n",
    "### **Image-Text Contrastive Loss (ITC)**\n",
    "\n",
    "El **ITC** es un objetivo de entrenamiento inspirado en CLIP que obliga al modelo a aprender un **espacio latente común** donde:\n",
    "\n",
    "* la **imagen** y su **texto correcto** estén **muy cerca**,\n",
    "* la **imagen** y textos **incorrectos** estén **muy lejos**,\n",
    "\n",
    "Su función principal es **alinear el encoder de imagen y el encoder de texto** antes de combinar modalidades más complejas.En BLIP, este loss se aplica sobre el **vision encoder** (ViT normalmente), y el **unimodal text encoder** (transformer textual sin intervención visual). Esto es importante porque si no existiera este alineamiento previo, no existiría una relación numérica clara entre \"lo que ve\" y \"lo que lee\". Una vez alineados ambos espacios latentes, el resto de componentes del modelo pueden saber que \"dog\", \"black dog\", \"a dog running\" y \"animal\" están cerca de la imagen de un perro, mientras que \"car\" o \"computer\" deben estar lejos.\n",
    "\n",
    "Para el entrenamiento con ITC debemos contar con un conjunto de datos de pares (imagen, texto). El loss se calcula así:\n",
    "\n",
    "$$\n",
    "L_{ITC} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\log \\frac{\\exp(\\text{sim}(v_i, t_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(v_i, t_j) / \\tau)} + \\log \\frac{\\exp(\\text{sim}(t_i, v_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(t_i, v_j) / \\tau)} \\right)\n",
    "$$\n",
    "\n",
    "* $N$: tamaño del batch (= número de pares imagen–texto).\n",
    "* $v_i$: embedding **visual** de la imagen $i$.\n",
    "* $t_i$: embedding **textual** del texto $i$.\n",
    "* $\\text{sim}(v_i, t_j)$: **similitud coseno** entre la imagen $i$ y el texto $j$.\n",
    "* $\\tau$: *temperatura*, un hiperparámetro que controla la \"dureza\" de la distribución softmax.\n",
    "\n",
    "\n",
    "Esta fórmula puede parecer compleja, pero vamos a desglosarla paso a paso. El primer término dentro del sumatorio es:\n",
    "\n",
    "$$\n",
    "\\log \\frac{\\exp(\\text{sim}(v_i, t_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(v_i, t_j) / \\tau)}\n",
    "$$  \n",
    "\n",
    "Si te fijas, es una **probabilidad softmax** que mide qué tan probable es que el texto correcto $t_i$ sea el más similar a la imagen $v_i$ entre todos los textos del batch. Por tanto, este término fuerza al modelo a que la similitud entre **imagen $i$** y su **texto correcto $i$** sea **alta**, y que la similitud entre **imagen $i$** y textos **incorrectos $j\\neq i$** sea **baja**. Y esto lo hace tanto en la dirección **imagen → texto** como en la dirección **texto → imagen** (segundo sumando de la expresión de la pérdida). Es decir, busca alineamiento **bidireccional**.\n",
    "\n",
    "La división por $N$ promedia la pérdida sobre todos los pares del batch, y el signo negativo convierte la maximización de la probabilidad en minimización de la pérdida. En cuanto al hiperparámetro $\\tau$, controla la \"dureza\" de la distribución softmax: valores bajos hacen que el modelo se enfoque más en los ejemplos difíciles.\n",
    "\n",
    "\n",
    "### **Image-Text Matching Loss (ITM)**\n",
    "\n",
    "Aquí se plantea un problema de **clasificación binaria**. Dado un par (imagen, texto), el modelo debe decidir si el texto **realmente describe** la imagen o no. Para ello se usa la representación multimodal (texto condicionado por imagen). El modelo **predice “match / no match”** forzando al modelo a aprender **representaciones más finas** que las del puro contraste global: no solo \"parecidos\", sino \"¿esta frase encaja exactamente con esta imagen?\".\n",
    "\n",
    "Su expresión matemática es la de una pérdida de entropía cruzada binaria estándar:\n",
    "\n",
    "$$\n",
    "L_{ITM} = - \\frac{1}{M} \\sum_{i=1}^{M} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "$$\n",
    "\n",
    "* $M$: número total de pares (imagen, texto) usados en este loss (incluye positivos y negativos).\n",
    "* $y_i \\in \\{0,1\\}$: etiqueta binaria que indica si el par $i$ es positivo (1) o negativo (0).\n",
    "* $p_i$: probabilidad predicha por el modelo de que el par $i$ sea positivo (match).\n",
    "\n",
    "\n",
    "La pérdida ITM ocupa un papel fundamental porque permite que el modelo aprenda a distinguir cuándo una imagen y un texto realmente están relacionados y cuándo no. A diferencia de la pérdida contrastiva ITC (que trabaja de forma global con similitudes en un espacio vectorial común) la ITM opera a un nivel más fino, evaluando la correspondencia semántica entre una imagen concreta y una frase específica. Podríamos decir que la ITM enseña al modelo a “leer” la relación imagen–texto con mayor detalle y no solo a nivel de orientación global en el embedding space.\n",
    "\n",
    "El mecanismo funciona del siguiente modo. BLIP toma una imagen y su descripción correcta (el par positivo) y posteriormente genera versiones negativas, normalmente seleccionando textos que no corresponden a esa imagen o imágenes que no corresponden a ese texto. Con estos pares mezclados, el modelo debe predecir si la combinación es verdadera o falsa. Para ello utiliza un clasificador binario montado sobre el encoder multimodal, el cual recibe tanto los embeddings visuales como los tokens textuales y produce una probabilidad de que ese par encaje semánticamente.\n",
    "\n",
    "Este clasificador se entrena con una pérdida de tipo binary cross-entropy. Las parejas auténticas deben obtener una probabilidad alta, mientras que las parejas erróneas deben quedar relegadas a valores bajos. Con este proceso, BLIP no solo aprende qué imágenes y textos son globalmente similares —como hace ITC— sino que además desarrolla sensibilidad hacia relaciones más sutiles: detalles objetuales, interacciones entre elementos, atributos concretos o descripciones incorrectas pero plausibles. En otras palabras, la ITM obliga al modelo a comprender contenido, no solo alineación vectorial.\n",
    "\n",
    "\n",
    "### **Language Modeling Loss (LM)**\n",
    "\n",
    "El tercer componente funcional de BLIP es su **módulo de modelado del lenguaje**, responsable de que el modelo no solo pueda alinear imagen y texto (ITC) o verificar si coinciden (ITM), sino también **generar descripciones completas, naturales y precisas** a partir de una imagen. Esta capacidad generativa es esencial para tareas como *image captioning* o *visual question answering*, en las que el modelo debe producir una secuencia lingüística condicionada por la información visual.\n",
    "\n",
    "La arquitectura que BLIP utiliza para esta parte sigue el patrón clásico de los modelos **encoder–decoder**:\n",
    "\n",
    "* el *encoder multimodal* —que combina los tokens visuales procedentes del ViT y los tokens textuales de entrada— sirve como **contexto**,\n",
    "* y un **decodificador lingüístico autoregresivo**, basado en un Transformer unidireccional, genera palabra por palabra la descripción final.\n",
    "\n",
    "Lo importante es que el decodificador no genera texto de manera aislada, sino atendiendo explícitamente a los **embeddings visuales** mediante mecanismos de *cross-attention*. Esto permite que cada palabra generada esté informada por la imagen. Por ejemplo, para generar la palabra \"gato\", el decodificador no solo se guía por la estructura sintáctica de la frase, sino también por los patrones visuales en los que se detecta un gato.\n",
    "\n",
    "La pérdida utilizada para entrenar este módulo es la **pérdida de modelado del lenguaje**, también conocida como **pérdida de entropía cruzada autoregresiva**. Formalmente, el decodificador intenta maximizar la probabilidad de la secuencia textual correcta $T = (t_1, t_2, \\dots, t_n)$ dadas la imagen $I$ y el contexto anterior:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{LM} = - \\sum_{k=1}^{n} \\log P(t_k \\mid t_{<k}, I).\n",
    "$$\n",
    "\n",
    "Este objetivo fuerza al modelo a predecir correctamente cada token de la descripción, condicionándose tanto en los tokens previos como en los elementos visuales. En otras palabras, la pérdida LM enseña al modelo **gramática, semántica y correspondencia visual–lingüística en un espacio generativo**.\n",
    "\n",
    "BLIP incluye además un mecanismo interesante llamado *captioning bootstrapping*: durante el entrenamiento temprano, el texto asociado a muchas imágenes web es ruidoso o irrelevante. Gracias a sus módulos ITC e ITM, el modelo es capaz de identificar pares de baja calidad y generar sus **propias descripciones limpias** utilizando el decodificador. Estas descripciones generadas, conocidas como *pseudo-captions*, se utilizan después como datos adicionales para mejorar el entrenamiento. Es un proceso de auto-mejora que incrementa notablemente la calidad final del modelo.\n",
    "\n",
    "En conjunto, la pérdida LM convierte a BLIP en un sistema plenamente multimodal, capaz de **entender una imagen hasta el punto de describirla con lenguaje natural**. Mientras ITC entrena la alineación global imagen–texto, e ITM enseña la verificación fina de correspondencias, la pérdida LM confiere al modelo la capacidad de producir lenguaje de forma fluida, detallada y contextualmente informada. Esta integración de comprensión, alineación y generación es lo que hizo que BLIP representara un avance significativo en los modelos multimodales previos a la llegada de BLIP-2 y los LLM modernos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejemplo práctico con Hugging Face Transformers**\n",
    "\n",
    "El modelo BLIP que vamos a usar tiene dos modos de funcionamiento principales:\n",
    "* **Captioning**: genera una descripción textual completa a partir de una imagen.\n",
    "* **Visual Question Answering (VQA)**: responde a preguntas específicas sobre el contenido de una imagen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <3EF8D74C-9F11-3C9E-85DB-9E76BCCBE7A0> /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <B6BD92AE-4D03-3F92-9E03-2E2594A12866> /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a man in a parking holding a white ball\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n",
    "model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n",
    "\n",
    "image = Image.open(\"imgs/rugby.png\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(**inputs)\n",
    "\n",
    "caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"imgs/perro.png\" width=\"230px\" align=\"left\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: 5\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "# Carga el procesador y el modelo de VQA\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "image = Image.open(\"imgs/rugby.png\")\n",
    "\n",
    "# Tu pregunta\n",
    "question = \"How many cars are there?\"\n",
    "\n",
    "inputs = processor(image, question, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs)\n",
    "\n",
    "answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Respuesta:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: in parking lot\n"
     ]
    }
   ],
   "source": [
    "# Tu pregunta\n",
    "question = \"Where is the man?\"\n",
    "\n",
    "inputs = processor(image, question, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs)\n",
    "\n",
    "answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Respuesta:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"../imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 7.1**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visual Transformers**\n",
    "\n",
    "\n",
    "## **¿Qué es un Visual Transformer?**\n",
    "\n",
    "Un Visual Transformer (a veces llamado Vision Transformer o ViT) es un modelo de deep learning que aplica la arquitectura Transformer, que originalmente revolucionó el procesamiento de lenguaje natural, a visión por computador.\n",
    "\n",
    "En vez de usar convoluciones (como en las CNN clásicas tipo ResNet), un Visual Transformer trata una imagen como si fuera una “secuencia de tokens”, igual que una frase. Cada token no es una palabra, sino un trocito de la imagen.\n",
    "\n",
    "Objetivo típico: clasificación de imágenes, detección de objetos, segmentación, etc.\n",
    "\n",
    "\n",
    "## **¿Por qué esto fue un cambio importante?**\n",
    "\n",
    "Antes de los Transformers visuales, prácticamente todo en visión profunda eran CNNs. Las CNNs tienen *inductive biases* fuertes:\n",
    "\n",
    "- Localidad: miran primero ventanas pequeñas.\n",
    "- Equivarianza traslacional: detectar un borde arriba o abajo es parecido.\n",
    "\n",
    "Eso es bueno… pero también limita:\n",
    "\n",
    "- Cuesta modelar dependencias globales (relaciones entre partes lejanas de la imagen).\n",
    "- Escalar a imágenes enormes no siempre es eficiente.\n",
    "\n",
    "Los **Visual Transformers** hacen algo diferente:\n",
    "\n",
    "- Usan atención global. Cualquier parte de la imagen puede “mirar” cualquier otra parte desde el primer bloque.\n",
    "- Escalan muy bien con más datos y más tamaño de red.\n",
    "\n",
    "Resultado: cuando entrenas con datasets gigantes (no sólo ImageNet, sino cientos de millones de imágenes), los Vision Transformers igualan o superan a las CNNs en muchas tareas.\n",
    "\n",
    "\n",
    "## **¿Cómo funciona un Vision Transformer paso a paso?**\n",
    "\n",
    "Vamos con el ViT básico [Dosovitskiy et al., 2020](https://arxiv.org/abs/2010.11929), que es el diseño canónico:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./imgs/Vit.png\" alt=\"ViT Architecture\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "### **Patch embedding**\n",
    "\n",
    "1. Tomas una imagen, por ejemplo 224×224×3.\n",
    "2. La divides en parches no solapados, por ejemplo 16×16.\n",
    "\n",
    "   ¿Cuántos parches salen?\n",
    "\n",
    "     - En horizontal: 224 / 16 = 14\n",
    "     - En vertical: 224 / 16 = 14\n",
    "     - Total: 14 × 14 = 196 parches.\n",
    "3. Cada parche de 16×16×3 píxeles se aplana (es un vector) y pasa por una proyección lineal → queda como un embedding de dimensión fija (por ejemplo 768).\n",
    "\n",
    "Traducción: conviertes la imagen en una secuencia de 196 “tokens visuales”.\n",
    "\n",
    "Esto es análogo a: “cada palabra → embedding”, pero aquí “cada parche → embedding”.\n",
    "\n",
    "### **Token [CLS]**\n",
    "\n",
    "Se añade un token especial de clase (`[CLS]`) al principio de la secuencia. Este token actuará como resumen global de toda la imagen. Al final lo pasas a una softmax para predecir la clase.\n",
    "\n",
    "Igual que en BERT para texto.\n",
    "\n",
    "### **Positional embeddings**\n",
    "\n",
    "Los Transformers puros no saben de posiciones espaciales. Para que el modelo sepa dónde está cada parche dentro de la imagen, se suma a cada token un embedding posicional (aprendido o sinusoidal).\n",
    "\n",
    "En visión esto es crucial, porque “ojo arriba + pico amarillo abajo” no es lo mismo que al revés (pájaro vs. sol en la playa).\n",
    "\n",
    "### **Encoder Transformer**\n",
    "\n",
    "Después, la secuencia pasa por varios bloques Transformer idénticos. Cada bloque tiene:\n",
    "\n",
    "- Multi-Head Self-Attention (MHSA)\n",
    "- MLP feed-forward grande\n",
    "- Normalizaciones + conexiones residuales\n",
    "\n",
    "#### **¿Qué hace la self-attention aquí?**\n",
    "\n",
    "Calcula, para cada parche, con qué otros parches debe interactuar y cuánto. Matemáticamente, cada token genera Query, Key, Value. Con eso calcula pesos de atención y mezcla información.\n",
    "\n",
    "Intuición: El parche que contiene, por ejemplo, “rueda de coche” puede prestar atención a “parche con faro delantero”, aunque estén lejos en la imagen. Eso le da contexto global muy pronto.\n",
    "\n",
    "### **Clasificación**\n",
    "\n",
    "Al final, tomamos el embedding final del token `[CLS]` y lo pasamos por una capa lineal → logits de clases.\n",
    "\n",
    "\n",
    "## **Ventajas frente a CNNs**\n",
    "\n",
    "1. **Contexto global desde el inicio**\n",
    "   Las CNNs necesitan muchas capas y pooling para juntar información distante. En ViT, atención global lo hace en el primer bloque.\n",
    "\n",
    "2. **Escalabilidad limpia**\n",
    "   Aumentar el tamaño de un Transformer (más capas, más anchura, más cabezas) es bastante sistemático y ya está súper estudiado en NLP. Básicamente puedes “fabricar” modelos enormes y entrenarlos con datasets masivos.\n",
    "\n",
    "3. **Unifica visión y lenguaje**\n",
    "   Como el backbone es Transformer, enganchar texto y visión en el mismo espacio (por ejemplo CLIP: imagen ↔ texto) es natural. Esto disparó los modelos multimodales.\n",
    "\n",
    "\n",
    "## **Inconvenientes / retos**\n",
    "\n",
    "1. **Datos, datos, datos**\n",
    "   Un ViT puro entrenado desde cero en un dataset relativamente pequeño (por ejemplo sólo ImageNet-1k con 1.2M imágenes) tiende a generalizar peor que una buena CNN inicial. Necesitan o:\n",
    "\n",
    "   * preentrenamiento masivo en datasets enormes,\n",
    "   * o técnicas de regularización/agumentación fuertes (como DeiT).\n",
    "\n",
    "2. **Costo de atención cuadrático**\n",
    "   Atención clásica tiene coste O(N²) en número de tokens.\n",
    "   Si partes una imagen en muchos parches pequeños (por ejemplo, 14×14 = 196 tokens está bien... pero 32×32 = 1024 tokens ya duele), la memoria y el cómputo suben rápido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\n",
    "    task=\"image-classification\",\n",
    "    model=\"google/vit-base-patch16-224\",\n",
    "    dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

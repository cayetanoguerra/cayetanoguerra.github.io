{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo es un método de aprendizaje automático en el que un agente aprende a realizar ciertas acciones en un entorno que lo llevan a maximizar una recompensa. Lo hace mediante la **exploración** de nuevas vías y soluciones y la **explotación** del conocimiento que adquiere mediante repetidas pruebas.\n",
    "\n",
    "## Características del aprendizaje por refuerzo\n",
    "\n",
    "- Está dirigido por objetivos. Este objetivo se expresa por una recompensa que devuelve el entorno al realizar una acción sobre él. No se conoce cuál es la salida adecuada para el sistema. Tan solo que el efecto que debe producir esta salida sobre el entorno sea tal que se maximice la recompensa recibida a largo plazo.\n",
    "\n",
    "\n",
    "\n",
    "- El comportamiento del entorno es, en general, desconocido y puede ser estocástico, es decir, que la evolución del entorno y la recompensa generada pueden obedecer a una cierta función de probabilidad.\n",
    "\n",
    "\n",
    "\n",
    "- La recompensa puede tener un cierto retardo. Es decir, la bondad de una acción tomada por el sistema puede que no se refleje hasta un cierto número de evaluaciones posteriores.\n",
    "\n",
    "\n",
    "- Dado que el comportamiento del entorno es desconocido, el aprendizaje por refuerzo conlleva una fuerte carga de ensayo y error.\n",
    "\n",
    "\n",
    "- Uno de los problemas asociados es el balance exploración-explotación. Se trata de evaluar si es mejor explorar el entorno para mejorar el conocimiento del problema (a costa de empeorar a corto plazo la recompensa obtenida) o explotar el conocimiento acumulado (intentando maximizar la recompensa).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Agente\n",
    "\n",
    "Es el sujeto del aprendizaje por refuerzo. Su funcionamiento consiste en leer el estado del entorno, realizar acciones sobre el entorno y leer las recompensas que producen estas acciones.\n",
    "\n",
    "\n",
    "### Entorno\n",
    "\n",
    "Es el objeto sobre el que opera el agente. El entorno recibe las acciones del agente y evoluciona. Su comportamiento suele ser desconocido y estocástico. Es el responsable de generar las recompensas asociadas a las acciones y cambios de estado.\n",
    "\n",
    "\n",
    "### Política\n",
    "\n",
    "Define el comportamiento del agente. Puede verse como un mapeo de estado a acción, es decir, establece las reglas de asociación entre el estado del entorno y la acción a tomar. Puede ser estocástica.\n",
    "\n",
    "\t\n",
    "### Función de refuerzo\n",
    "\n",
    "Establece la recompensa a generar en función del estado del entorno y la acción realizada sobre él. Puede ser\n",
    "estocástica. El objetivo del aprendizaje por refuerzo es maximizar la recompensa total obtenida a largo plazo.\n",
    "\n",
    "### Función de evaluación (función de valor)\n",
    "\n",
    "Refleja una estimación de la recompensa que se va a recibir partiendo de un cierto estado y siguiendo una cierta política. Esta función sirve de base para escoger la acción a realizar (aquella que conduzca al estado con mayor\n",
    "valor). El objetivo de los algoritmos de aprendizaje por refuerzo es construir esta función.\n",
    "\n",
    "### Modelo del entorno\n",
    "\n",
    "Permite predecir el comportamiento del entorno y aprovechar esta información para resolver el problema.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema de los n bandidos con arma\n",
    "\n",
    "En Las Vegas, un bandido con un arma (*bandit with an arm*) se refiere a una máquina tragaperras. Se trata de un juego de palabras (*arm* significa arma y brazo y hace referencia a la palanca de estas máquinas).\n",
    "\n",
    "<img src=\"./imgs/tragaperras.jpg\" width=30%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales artificiales (RNA) constituyen un paradigma de computación inspirado en las <a href=\"https://es.wikipedia.org/wiki/Neurona\">neuronas</a> biológicas y su interconexión. Las neuronas biológicas son células compuestas principalmente de tres partes: soma (cuerpo celular), dendritas (canales de entrada) y axón (canal de salida). Descrito de una forma muy simplificada, las neuronas transmiten información a través de procesos electroquímicos. Cuando una neurona recibe, a través de las denritas, una cantidad de estímulos mayor a un cierto umbral, ésta se despolariza excitando, a través del axón, a otras neuronas próximas conectadas a través de las sinapsis.\n",
    "\n",
    "<img src=\"imgs/neurona.jpg\" width=\"70%\">\n",
    "\n",
    "## La neurona artificial\n",
    "\n",
    "Inspirados por esta idea se concibió el modelo de <a href=\"https://es.wikipedia.org/wiki/Neurona_de_McCulloch-Pitts\">neurona artificial</a>. Fundamentalmente, consiste en una unidad de cálculo que admite como entrada un vector de características $\\vec{e}$ cuyos valores se suman de forma ponderada mediante unos pesos $\\vec{w}$ y, si esta suma supera un cierto umbral $\\theta$, genera un cierto valor de salida, por ejemplo $1$ y, si no lo supera, genera otro valor, por ejemplo, un $0$. Cuando la neurona está sola, es decir, no conectada a otras conformando una red, actúa como un clasificador lineal. \n",
    "\n",
    "La expresión más básica de la neurona artificial es la siguiente:\n",
    "\n",
    "$$\n",
    "f(\\textbf{e}) = \\begin{cases} \\textrm{1, si} \\sum_{i=1}^{n} {w_i  e_i} \\geq \\theta \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "Para entender esto, veamos antes unas cuantas cosas.\n",
    "\n",
    "## Problemas de clasificación. ¿Qué es eso?\n",
    "\n",
    "Una de las principales tareas que resuelven las redes neuronales son las tareas de clasificación. Pero, ¿en qué consisten esas tareas? Todos sabemos lo que significa clasificar. Consiste en agrupar objetos de categorías similares. Por ejemplo, si nos dan un conjunto de monedas y nos piden que las clasifiquemos, podemos hacerlo por el valor de la moneda. Las de 1€ con las de 1€, las de 50 céntimos con las de 50 céntimos, etc. La propiedad que observamos para agrupar es su valor. Otro ejemplo, podría ser la clasificación de las manzanas atendiendo a su color como \"rojas\" y \"verdes\". Es posible también tener en cuenta más de una propiedad del objeto para su clasificación. Por ejemplo, supongamos que clasifico teléfonos móviles como \"gama alta\" si su cámara supera los 15 megapixeles y además tiene más de 128GB de memoria. Podríamos seguir así y utilizar tantas propiedades de los objetos como queramos para su clasificación. Por tanto, vamos a concretar esto y definiremos como **vector de características** al vector ordenado que agrupa cada uno de los valores de las propiedades que vamos a tener en cuenta para clasificar un objeto. \n",
    "\n",
    "\n",
    "$$ \\vec{e} = (e_1, e_2, \\dots, e_n) $$\n",
    "\n",
    "Por tanto, un vector de características \"caracteriza\" un objeto. En el caso de los móviles, el móvil *A* podría tener como vector de características $\\vec{e_A} = (10, 64)$, siendo 10 el número de megapixeles y 64 el de megabytes. El móvil *B*: $\\vec{e_B} = (12, 256)$, el móvil *C*: $\\vec{e_C} = (8, 32)$, etc.\n",
    "\n",
    "Si representamos estos vectores de características como **puntos** en unos ejes de coordenadas cartesianas tendríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "features = [[10,16,8], # megapixeles de la cámara\n",
    "            [64,256,32] # gigabytes de memoria\n",
    "           ]\n",
    "\n",
    "plt.scatter(features[0], features[1])\n",
    "\n",
    "plt.xlabel(\"Eje X: Megapixeles de la cámara\")\n",
    "plt.ylabel(\"Eje Y: Gigabytes de memoria\");\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tuviéramos un vector de características con tres propiedades representaríamos el punto en un espacio tridimensional, y así sucesivamente. Bien, pues ahora los móviles que cumplan con la condición anterior de gama alta serán los que aparecen marcados en azul y el resto, en rojo, serán los de gama baja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "features = [[10,16,8], # megapixeles de la cámara\n",
    "            [64,256,32]] # gigabytes de memoria\n",
    "            \n",
    "classes = []\n",
    "for e1, e2 in zip(features[0], features[1]):\n",
    "    if e1>15 and e2>128:\n",
    "        classes.append('b')\n",
    "    else:\n",
    "        classes.append('r')\n",
    "\n",
    "plt.scatter(features[0], features[1], c = classes)\n",
    "\n",
    "plt.xlabel(\"Eje X: Megapixeles de la cámara\")\n",
    "plt.ylabel(\"Eje Y: Gigabytes de memoria\");\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación lineal\n",
    "\n",
    "Supongamos que tenemos ahora un ejemplo más complejo donde los objetos o **muestras** (así es como se suelen llamar estos puntos) tengan la siguiente disposición:\n",
    "\n",
    "<img src=\"imgs/set1.png\" width=\"50%\">\n",
    "\n",
    "> Decimos que un conjunto de muestras es **separable linealmente** si podemos trazar una recta (en un espacio tridimensional sería un plano y en un espacio multidimensional sería un hiperplano) que separe a ambas clases o categorías.\n",
    "\n",
    "<img src=\"imgs/set2.png\" width=\"50%\">\n",
    "\n",
    "Veamos cuándo dos conjuntos (clases o categorías) no son separables linealmente. En este caso, no podemos trazar una recta que separe perfectamente ambos comjuntos.\n",
    "\n",
    "<img src=\"imgs/set3.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminología\n",
    "\n",
    "Ahora que ya tenemos claro lo que significa \"clasificar\", definamos algo de terminología. Cada uno de los puntos u objetos a clasificar se denomina **muestra**. El conjunto de todas las muestras se denomina **conjunto de datos** (aunque te lo vas encontrar en muchos textos en español con el término anglosajón **dataset**). Todas estas muestras pertenecerán a un grupo u otro. A cada uno de estos dos grupos lo denominamos **clase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Perceptrón como clasificador lineal\n",
    "\n",
    "Volvamos de nuevo a la definición de neurona articial y veamos qué relación tiene con los problemas de clasificación lineal. Recordemos su expresión como la vimos arriba, pero vamos a modificarla ligeramente moviendo $\\theta$ a la izquierda del símbolo \"mayor o igual\", de esta manera:\n",
    "\n",
    "$$\n",
    "f(\\textbf{e}) = \\begin{cases} \\textrm{1, si} \\sum_{i=1}^{n} {w_i  e_i} - \\theta\\geq 0 \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "Si queremos, podemos visualizar gráficamente la neurona de esta manera:\n",
    "\n",
    "<img src=\"imgs/model.svg\" width=\"70%\">\n",
    "\n",
    "Donde la función $g(x)$ tiene la forma \"1 si $x\\geq 0$ y $0$ si $x<0$\". En este caso $x=\\sum_{i=1}^{n} {w_i  e_i} - \\theta$. Más adelante veremos que $g(x)$ tendrá otras formas. Si estudiamos bien esta fórmula nos daremos cuenta de que se trata de un discriminador lineal. \n",
    "\n",
    "Supongamos que tenemos un conjunto de puntos ${a,b,c,d,e}$ en un espacio $R^2$ tal como muestra la figura.\n",
    "\n",
    "<img src=\"imgs/ejemplo1.png\" width=\"40%\">\n",
    "\n",
    "Algunos de ellos ($a,b,c$) pertencen a una clase (clase 1) y los otros a otra (clase 2). Estas dos regiones están delimitadas por una recta. Nótese que la recta que separa ambas clases no es única, puede ser cualquiera que satisfaga la condición de separación de las clases. Por tanto, tenemos la función de una recta con la ecuación genérica:\n",
    "\n",
    "$$\n",
    "y = mx+b \n",
    "$$\n",
    "\n",
    "Haciendo unos cálculos básicos, podemos concretar esta recta como la recta de la figura de ejemplo anterior:\n",
    "\n",
    "$$\n",
    " y = \\frac{1}{2} x +1 \n",
    "$$\n",
    "\n",
    "Esta recta corresponde al conjunto de todos los puntos $(x,y)$ que satisfacen la **ecuación**. Por ejemplo, el punto $a(2,2)$. Pero vemos que los puntos $b$,$c$,$d$ y $e$ no satisfacen la ecuación. Sin embargo, algunos de ellos, concretamente los puntos $a$,$b$ y $c$ no satisfacen la **ecuación** pero sí satisfarían la **inecuación**:\n",
    "\n",
    "$$\n",
    "\ty \\geq \\frac{1}{2} x +1 \n",
    "$$\n",
    "\n",
    "Observa entonces que la inecuación separa el espacio en dos subesapcios. Uno de estos subespacios, el sombreado de color celeste, satisface la inecuación, pero el otro subespacio, no.\n",
    "\n",
    "\n",
    "Operando un poco sobre esta inecuación tendríamos:\n",
    "\n",
    "$$\n",
    "\t-\\frac{1}{2} x + y \\geq 1 \n",
    "$$\n",
    "\n",
    "Y cambiando la nomenclatura. Es decir, cambiando $x$ por $e_{1}$ e $y$ por $e_{2}$ tenemos:\n",
    "\n",
    "$$\n",
    "\t-\\frac{1}{2} e_{1} + e_{2} \\geq 1 \n",
    "$$\n",
    "\n",
    "Con lo cual podemos hacer que $w_1 = -\\frac{1}{2}$, $w_2 = 1$ y $\\theta=1$, que es, justamente, la neurona que actuaría de discriminador lineal de nuestro ejemplo.\n",
    "\n",
    "El verdadero potencial de la neuronal artificial no está en que calculemos a mano sus pesos y umbral sino en dejar que ella misma \"aprenda\" esos valores.\n",
    "\n",
    "\n",
    "## Aprendizaje\n",
    "\n",
    "Antes de meternos de lleno con el aprendizaje vamos a ver antes un par de cosas: la **función sigmoide** y la técnica de **descenso por el gradiente**.\n",
    "\n",
    "\n",
    "### Función sigmoide\n",
    "\n",
    "Utilizaremos la función **sigmoide** como **función de activación** en lugar de la función \"mayor o igual\" ya que ofrece una venjata importante: es derivable. Sí, ya sé lo que puedes estar pensando, *¿Y qué pasa con que sea derivable?*. Nos daremos cuenta de eso más adelante.\n",
    "\n",
    "La función sigmoide tiene la siguiente expresión: \n",
    "\n",
    "$$Sig(x)=\\frac{ 1 }{1+{ e }^{ -x }}$$ \n",
    "\n",
    "\n",
    "Y si la representamos gráficamente tiene este aspecto:\n",
    "\n",
    "<img src=\"imgs/sigmoide.png\" width=\"60%\">\n",
    "\n",
    "Vemos que tiene un rango que va desde $-\\infty$ a $\\infty$. Si nos fijamos bien, a partir de $-4$ hacia atrás su valor es prácticamente $0$ y a partir del $4$ hacia adelante su valor es prácticamente $1$. Es parecida a la función \"mayor o igual\" que definimos más arriba. Pero, a diferencia de la función sigmoide, esta tiene una discontinuidad en $0$, como observamos en la expresión y figura siguientes.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases} \\textrm{1, si } x \\geq 0 \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"imgs/mayorigual.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "La derivada de la función sigmoide es:\n",
    "\n",
    "$$Sig'(x)=\\frac { 1 }{ (1+e^{ x })} -\\frac { 1 }{ (1+e^{ x })^{ 2 }  } $$\n",
    "\n",
    "Puedes hacer los cálculos tú mismo para verificarlo. Además, si reordenamos un poco los términos, surge una propiedad curiosa, y es que podemos expresar la derivada de la sigmoide utilizando la propia sigmoide:\n",
    "\n",
    "$$Sig'(x)=\\frac { 1 }{ (1+e^{ x }) } \\left[ 1-\\frac { 1 }{ (1+e^{ x }) }  \\right] =\\frac { 1 }{ (1+e^{ -x }) } \\left[ 1-\\frac { 1 }{ (1+e^{ -x }) }  \\right] =Sig(x)\\cdot \\left[ 1-Sig(x) \\right] $$\n",
    "\n",
    "\n",
    "\n",
    "### Descenso por el gradiente\n",
    "\n",
    "Supongamos que quiero encontrar el mínimo de una función, por ejemplo: $y=x^2-2x+2$. \n",
    "\n",
    "<img src=\"imgs/descenso.png\" width=\"30%\">\n",
    "\n",
    "Lo primero que se nos ocurre es hallar su derivada: $y'=2x-2$, igualar a $0$ y despejar $x$. Lo que nos daría: $x=1$. Supongamos ahora que, por algún motivo, no podemos resolverlo de forma algebraica y lo tenemos que hacer de forma numérica. Es decir, partimos desde algún punto y nos vamos moviendo poco a poco en la dirección de bajada hasta que empecemos a remontar, lo cual quiere decir que hemos alcanzado el mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aproximación al mínimo: 1.0010000000000097\n",
      "Pasos: 150\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2 -2*x + 2\n",
    "\n",
    "x = 2.501 # algún punto inicial \n",
    "delta = 0.01 # algún valor pequeño\n",
    "\n",
    "counter = 0\n",
    "while (f(x) - f(x - delta)) > 0:\n",
    "    x -= delta # nuevo x\n",
    "    counter += 1\n",
    "    \n",
    "print(\"Aproximación al mínimo:\", x)\n",
    "print(\"Pasos:\", counter)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos han hecho falta 150 pasos para llegar a una aproximación del mínimo con un error menor de $1\\%$. Hay otro método mucho más eficiente para llegar a esa aproximación, se llama: **descenso por el gradiente**.\n",
    "\n",
    "Si nos fijamos en la pendiente de la función, vemos que, a medida que nos alejamos del mínimo, la pendiente (o derivada) es cada vez más pronunciada. Cuando estamos muy cerca del mínimo, la pendiente es casi $0$. El truco del descenso por el gradiente es aprovechar este hecho y utilizar la pendiente como paso (delta) para hacer avanzar la $x$ rápidamente cuando estamos lejos del mínimo y despacio cuando estamos cerca. Veámoslo en el siguiente código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 1.6004 - tamaño del paso: 3.002\n",
      "x: 1.2402 - tamaño del paso: 1.2008\n",
      "x: 1.0961 - tamaño del paso: 0.4803\n",
      "x: 1.0384 - tamaño del paso: 0.1921\n",
      "x: 1.0154 - tamaño del paso: 0.0769\n",
      "x: 1.0061 - tamaño del paso: 0.0307\n",
      "x: 1.0025 - tamaño del paso: 0.0123\n",
      "Pasos: 7\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2 -2*x + 2\n",
    "\n",
    "x = 2.501 # algún punto inicial \n",
    "delta = 0.01\n",
    "rho = 0.3\n",
    "\n",
    "counter = 0\n",
    "while (f(x) - f(x - delta)) > 0:\n",
    "    h = (f(x + delta) - f(x - delta)) / (2*delta)  # Cálculo numérico de la derivada en el punto x\n",
    "    x -= h * rho # nuevo x\n",
    "    counter += 1\n",
    "    print(\"x:\", round(x,4) ,\"- tamaño del paso:\", round(h,4))\n",
    "    \n",
    "print(\"Pasos:\", counter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con esta técnica logramos una aproximación similar... ¡¡en sólo 7 pasos!! \n",
    "\n",
    "Hay un parámetro nuevo que ha aparecido, rho ($\\rho$). Este parámetro lo llamaremos más adelante **tasa de aprendizaje**. ¿Qué función tiene? Ahora simplemente sirve como un parámetro de escala para el descenso. Observemos la figura siguiente, hay dos funciones que parecen la misma pero que, si nos fijamos bien, están a escalas diferentes. La de la izquierda tiene el mínimo en $x=1$ y la de la derecha en $x=0.1$. Sin embargo, el valor de la derivada en $x=2$ y en $x=0.2$ es el mismo, $2$. Prestemos atención primero a la función de la izquerda. Cuando hagamos el descenso por el gradiente, la nueva $x$ será: $x \\leftarrow x - m$, y esto nos llevará a $x=0$. Ahí la pendiente será $m=-2$, lo cual nos llevará de nuevo a $x=2$. Por tanto, necesitamos rebajar la amplitud del paso de alguna forma, y es ahí donde entra en juego el parámetro rho. Si damos a rho, por ejemplo, el valor $0.3$ conseguiremos reducir el paso y aproximarnos correctamente al mínimo. En la función de la derecha ocurre un efecto aún peor. Cuando actualicemos, la nueva $x$ será $x \\leftarrow 0.2 - 2$ lo que nos lleva a $x=-1.8$. Es decir, nos estaremos alejando progresivamente del mínimo. De nuevo, rho viene al rescate y si le damos un valor de, por ejemplo, $0.03$ nos estaremos aproximando adecuadamente al mínimo. \n",
    "\n",
    "La pregunta que surge es: *¿y cómo sé qué valor debe tener rho?*. La respuesta es que no lo podemos saber *a priori*. Habrá que probar hasta ver que el algoritmo converge.\n",
    "\n",
    "<img src=\"imgs/rho.jpg\" width=\"80%\">\n",
    "\n",
    "De la misma forma que podemos hacer descenso por el gradiente en una función de una variable $f(x)$, lo podemos hacer en una función con dos variables $f(x,y)$, y con tres, con cuatro, etc. La diferencia está en que ahora usamos **derivadas parciales** en lugar de derivadas. Por ejemplo, supongamos que tenemos la función $f(x,y,z)$, si quieremos hacer descenso por el gradiente tendríamos:\n",
    "\n",
    "$$\n",
    "x \\leftarrow x - \\rho \\frac{\\partial f(x,y,z)}{\\partial x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y \\leftarrow y - \\rho \\frac{\\partial f(x,y,z)}{\\partial y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z \\leftarrow z - \\rho \\frac{\\partial f(x,y,z)}{\\partial z}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de la neurona con función de activación sigmoide\n",
    "\n",
    "Antes vimos el modelo de la neurona articicial de la siguiente forma:\n",
    "\n",
    "$$\n",
    "f(\\textbf{e}) = \\begin{cases} \\textrm{1, si} \\sum_{i=1}^{n} {w_i  e_i} - \\theta\\geq 0 \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "Ahora, vamos a hacer algunos cambios \"estéticos\" a la neurona. Primero, le cambiaremos el nombre a $-\\theta$ y la llamaremos $w_0$.\n",
    "\n",
    "$$\n",
    "f(\\textbf{e}) = \\begin{cases} \\textrm{1, si} \\sum_{i=1}^{n} {w_i  e_i} + w_0 \\geq 0 \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "Si $w_0$ tuviera un valor $e_0$ para poder integrarlo dentro del sumatorio se nos quedaría una representación más compacta. Por tanto, vamos a insertar un $e_0$ que siempre tenga el valor $1$. Así:\n",
    "\n",
    "$$\n",
    "f(\\textbf{e}) = \\begin{cases} \\textrm{1, si} \\sum_{i=1}^{n} {w_i  e_i} + w_0 e_0 \\geq 0 \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "Y ahora sí que podemos dejarlo de una forma más compacta:\n",
    "\n",
    "$$\n",
    "f(\\textbf{e}) = \\begin{cases} \\textrm{1, si} \\sum_{i=0}^{n} {w_i  e_i} \\geq 0 \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "Esta función $f(\\textbf{e})$ devuelve un $1$ si $\\sum_{i=0}^{n} {w_i  e_i} \\geq 0$, y un $0$ cuando $\\sum_{i=0}^{n} {w_i  e_i} < 0$. Vemos que no es una función derivable en $x=0$, ya nos daremos cuenta de lo que implica esto. Así que vamos a cambiar esos menores, mayores e iguales por nuestra función sigmoide.\n",
    "\n",
    "$$\n",
    "f(\\textbf{e}) = Sigmoide(\\sum_{i=0}^{n} {w_i  e_i})\n",
    "$$\n",
    "\n",
    "De nuevo, esta función es prácticamente $0$ cuando $\\sum_{i=0}^{n} {w_i  e_i}$ es menor que $0$ y $1$ en caso contrario. Y, además, es derivables en $x=0$.\n",
    "\n",
    "Si representamos el perceptrón gráficamente para el caso de dos entradas $e_1$ y $e_2$ tenemos:\n",
    "\n",
    "<img src=\"imgs/perceptron.svg\" width=\"60%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje\n",
    "\n",
    "Veamos el proceso de aprendizaje con un ejemplo muy sencillo. Tenemos un *dataset* formado por tres muestras solamente, donde cada muestra tiene dos propiedades $e_1$ y $e_2$ (además de la correspondiente $e_0$ que siempre es $1$). En la siguiente tabla vemos sus valores y en la figura su representación gráfica. Vemos que hay dos clases, una representada con la etiqueta $1$ y la otra con la etiqueta $0$.\n",
    "\n",
    "| $e_0$ | $e_1$ | $e_2$ | label |\n",
    "|-------|-------|-------|-------|\n",
    "| 1     | 1     | 2     | 1     |\n",
    "| 1     | 3     | 1     | 1     |\n",
    "| 1     | 4     | 5     | 0     |\n",
    "\n",
    "<img src=\"imgs/points.png\" width=\"40%\">\n",
    "\n",
    "\n",
    "El objetivo es encontrar los pesos $w_0$, $w_1$ y $w_2$ de una neuronal artificial para que esta pueda clasificar las muestras correctamente. Este proceso va a ser automático e iterativo. Al principio, la neurona se va a inicializar con valores totalmente aleatorios para los pesos y, posteriormente, se verá qué error ha cometido en la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos: [[ 0.39293837]\n",
      " [-0.42772133]\n",
      " [-0.54629709]]\n",
      "Resultado de la neurona: [[0.24464546]\n",
      " [0.1920844 ]\n",
      " [0.01713359]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Recuerda que la primera columna con unos corresponde a la entrada e0.\n",
    "x_data = [[1, 1, 2],\n",
    "          [1, 3, 1],\n",
    "          [1, 4, 5]]\n",
    "\n",
    "x_data = np.matrix(x_data)\n",
    "\n",
    "y_data = [1, 1, 0]\n",
    "\n",
    "np.random.seed(seed=123)\n",
    "weights = np.random.uniform(low=-1, high=1, size=3).reshape((3,1))\n",
    "\n",
    "print(\"Pesos:\", weights)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "output = sigmoid(x_data * weights)\n",
    "\n",
    "print(\"Resultado de la neurona:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de error\n",
    "\n",
    "¿Cómo podemos medir el error cometido por la neurona? Para el primer punto $(1,2)$ el resultado debería ser $1$, para el punto $(3,1)$, también $1$. Y, para el punto $(4,5)$, el resultado debería ser $0$. Una forma de medir el error global cometido sería: \n",
    "\n",
    "$$ error = \\sum_{j=1}^{m} ( \\sum_{i=0}^{n} {w_i  e_i^j} - label_j)^2$$\n",
    "\n",
    "Siendo $m$ el número de muestras que tenemos, en este caso, tres. El error será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2235816444340613\n"
     ]
    }
   ],
   "source": [
    "error = np.sum(np.power((output.reshape(3) - y_data),2))\n",
    "\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy importante entender ahora que **esta función de error está en función de los pesos, no de las muestras**. Las muestras son estáticas, los pesos, no. Los pesos los iremos variando a medida que descendamos por el gradiente. Vemos también que esta función de error es continua y derivable en todo momento. Por eso nos interesaba  prescindir de los anteriores \"mayores\" y \"menores\" y quedarmos con una función como la sigmoide, que es continua y derivable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descendiendo por el gradiente\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=IT2SAiIb7Yk\">(vídeo)</a>\n",
    "\n",
    "Esta función de error decrece cuando los resultados de la neurona se acercan a las etiquetas (*labels*) de cada muestra. Por lo tanto, iremos descenciendo por el gradiente hasta intentar alcanzar el mínimo de esta función. Recordamos que el desceso requiere el cálculo de las derivadas parciales de la función. Vamos a calcularlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamemos $h_{\\vec{w}}$ a la función del perceptrón para un determinado vector (conjunto de pesos) $\\vec{w}$\n",
    "\n",
    "\n",
    "$$h_{\\vec{w}}(\\vec{e}) = Sig(\\sum_{ i=0 }^{ n } w_i e_i)$$\n",
    "\n",
    "\n",
    "donde $n$ es el número de componentes del vector $\\vec{e}$. La salida de $h_{\\vec{w}}$ estará comprendida en el intervalo real $(0,1)$ (debido a la la función sigmoide tiene su rango comprendido en el intervalo $(0,1)$). Definimos el error $J$ en función de un conjunto de pesos $\\vec{w}$ de la siguiente forma:\n",
    "\n",
    "$$J(\\vec{w}) = \\sum _{ i=1 }^{ m } (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)})^2$$\n",
    "\n",
    "Donde $m$ es el número de muestras. \n",
    "\n",
    "El nuevo conjunto de pesos $\\vec{w}$ será actualizado de la siguiente forma\n",
    "\n",
    "$$\\vec{w}_{t+1}  := \\vec{w}_t - \\gamma  \\frac{\\partial{J(\\vec{w})}}{\\partial{\\vec{w}}}$$\n",
    "\n",
    "La constante $\\gamma$ se define como \"tasa de aprendizaje\". Su derivada parcial con respecto a cada componente de $\\vec{w}$ será:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J(\\vec{w})}{\\partial w_j} = \\frac{\\partial}{\\partial w_j}\\sum_{ i=1 }^{ m }  (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)})^2 =$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\frac{\\partial}{\\partial w_j} (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\frac{\\partial}{\\partial w_j} Sig(\\textbf{e}^{(i)} \\cdot \\vec{w}) =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\; Sig' (\\textbf{e}^{(i)} \\cdot \\vec{w}) \\frac{\\partial}{\\partial w_j} \\textbf{e}^{(i)} \\cdot \\vec{w} =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\; Sig' (\\textbf{e}^{(i)} \\cdot \\vec{w}) \\frac{\\partial}{\\partial w_j} \\sum_{k=0}^n e^{(i)}_{k} w_k =\n",
    "$$\n",
    "\n",
    "$$\n",
    "2 \\sum_{ i=1 }^{ m }  (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\; Sig' (\\textbf{e}^{(i)} \\cdot \\vec{w}) e^{(i)}_{j} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código\n",
    "\n",
    "Veamos todo el proceso completo en código. Programaremos la función sigmoide y su derivada e iteraremos un determinado número de veces descendiendo por el gradiente de la función de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [[1, 2],\n",
    "          [3, 1],\n",
    "          [4, 5]]\n",
    "\n",
    "labels = [1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivate(o):\n",
    "    return o * (1.0 - o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "[1, 2] -> 1\n",
      "0.973080122635592\n",
      "-----------------------\n",
      "[3, 1] -> 1\n",
      "0.9855133303924121\n",
      "-----------------------\n",
      "[4, 5] -> 0\n",
      "0.01880063364262751\n",
      "-----------------------\n",
      "Pesos:  7.9886492208294735 -0.6272766337348483 -1.8868855557655697\n"
     ]
    }
   ],
   "source": [
    "def train(x_data, y_data):\n",
    "\n",
    "    np.random.seed(seed=123)\n",
    "    w0, w1, w2 = np.random.rand(3)\n",
    "    lr = 0.1\n",
    "    epochs = 10000\n",
    "\n",
    "    print(\"Training...\")\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        \n",
    "        w0_d = []\n",
    "        w1_d = []\n",
    "        w2_d = []\n",
    "        \n",
    "        for data, label in zip(x_data, y_data):\n",
    "            \n",
    "            e1, e2 = data;\n",
    "            o = sigmoid(w0*1.0 + w1*e1 + w2*e2)\n",
    "            aux = 2.*(o - label) * sigmoid_derivate(o)\n",
    "\n",
    "            w0_d.append(aux * 1.0)\n",
    "            w1_d.append(aux * e1)\n",
    "            w2_d.append(aux * e2)\n",
    "            \n",
    "        w0 = w0 - np.sum(w0_d) * lr\n",
    "        w1 = w1 - np.sum(w1_d) * lr\n",
    "        w2 = w2 - np.sum(w2_d) * lr\n",
    "        \n",
    "        \n",
    "    for data, label in zip(x_data, y_data):\n",
    "        e1, e2 = data;\n",
    "        print(data, \"->\", label)\n",
    "        o = sigmoid(w0*1.0 + w1*e1 + w2*e2)\n",
    "        print(o)\n",
    "        print(\"-----------------------\")\n",
    "    \n",
    "    print(\"Pesos: \", w0, w1, w2)\n",
    "\n",
    "\n",
    "train(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Y, finalmente, vemos cómo la neurona ha ajustado sus pesos hasta llevarlos a unos valores que hacen que sus resultados se aproximen mucho a las etiquetas. Si tomamos los pesos ($w_0 \\approx 8, w_1 \\approx -0.6, w_2 \\approx -2$) y los representamos como la típica recta a la que estamos acostumbrados tendremos:\n",
    "\n",
    "$$ 8e_0 -0.6e_1 -2e_2 = 0 $$\n",
    "\n",
    "cambiamos los nombres ($e_1 = x, e_2=y$, recuerda que $e_0=1$)...\n",
    "\n",
    "$$ 8 -0.6x -2y = 0 $$\n",
    "\n",
    "y despejamos la $y$.\n",
    "\n",
    "$$ y = -0.3x + 4 $$\n",
    "\n",
    "Y si representamos la ecuación gráficamente\n",
    "\n",
    "<img src=\"imgs/recta.png\" width=\"40%\">\n",
    "\n",
    "Por tanto, la neurona es capaz de clasificar todas las muestras correctamente. Realmente, la neurona no va a devolver un $1$ si la etiqueta es un $1$, pero sí un valor muy cercano. Así, si la neurona devuelve un valor mayor que $0.5$ decimos que la muestra la etiqueta como $1$. Y la etiqueta como $0$ en caso contrario.\n",
    "\n",
    "Lo que acabamos de ver se conoce como **aprendizaje supervisado**. Consiste en que un modelo, en este caso una neurona, se adapta (aprende sus pesos) para clasificar un conjunto de datos etiquetados. Ahora, si aparecieran nuevos puntos no etiquetados, la neurona sabría cómo clasificarlos correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "- Una vez tenemos la neurona entrenada, crea un script en Python que calcule el resultado de la clasificación de los puntos $(5,5), (4,2), (0,1)$.\n",
    "\n",
    "- ¿Qué ocurre con los puntos que están cerca de la recta? Como, por ejemplo, el punto $(0,4)$\n",
    "- Compara los pasos de la derivación parcial junto con el descenso por el gradiente e indetifica cada paso del algoritmo con el código en Python.\n",
    "- ¿Qué pasaría si hubiéramos mantenido la función de activación \"mayor que\" en lugar de la función sigmoide? ¿Cómo sería la función de error? Razona la respuesta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

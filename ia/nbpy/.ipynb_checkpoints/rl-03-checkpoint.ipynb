{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por refuerzo\n",
    "\n",
    "Todo lo que hemos visto en los apuntes anteriores corresponden a soluciones basadas en [programación dinámica]( https://es.wikipedia.org/wiki/Programación_dinámica). Su principal característica es que son solamente aplicables cuando se conoce *a priori* todo el entorno en el que el agente se va a desenvolver. Es decir, conocemos cuáles son las recompensas en cada estado y las probabilidades de transición entre estados. Como imaginarás, esto no es siempre posible en situaciones reales, por tanto, necesitamos desarrollar métodos que busquen políticas óptimas mediante la exploración y la exploración. Veremos los métodos basados en estrategias de Monte Carlo y basadas en diferencias temporales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo\n",
    "\n",
    "La **Programación Dinámica** exige un conocimiento *a priori* del comportamiento del entorno $P_{s,s'}^a$ y $R_{s,s'}^a$ para poder evaluar y actualizar las políticas. Pero, ¿qué ocurre cuando el comportamiento del entorno es desconocido? Los métodos de **Monte Carlo** consisten en evaluar una política estudiando las recompensas obtenidas por el agente al actuar sobre el entorno. Para eso es necesario realizar numerosos episodios partiendo de diferentes estados iniciales, lo que permite obtener una estimación del retorno esperado a partir de los diferentes estados.\n",
    "\n",
    "Cada iteración del algoritmo de Monte Carlo supone la realización de un episodio completo siguiendo la política marcada y, a continuación, actualizar la estimación considerando el retorno obtenido en el episodio. El objetivo es aprender la política óptima, lo que se puede realizar a partir de una estimación de la **función de valor** $V(s)$ o de la **función acción-valor** $Q(s,a)$.\n",
    "\n",
    "La actualización de una política a partir de la función de valor es la siguiente:\n",
    "\n",
    "$\\pi_{k+1}(s) = a \\text{,  tal que,  } a = max_a \\left( \\sum_{s'} P_{s,s'}^a [ R_{s,s'}^a + \\gamma \\cdot V^\\pi_{k}(s')] \\right) $ \n",
    "\n",
    "\n",
    "La actualización de una política a partir de la función acción-valor es la siguiente:\n",
    "\n",
    "\n",
    "$\\pi_{k+1}(s) = a \\text{, tal que, } a = max_a  \\left(  Q_{k}(s,a) \\right) $\n",
    "\n",
    "\n",
    "En los métodos de Monte Carlo resulta más adecuado utilizar la función de acción-valor $Q(s,a)$, ya que no requiere almacenar el modelo de comportamiento del entorno ($P_{s,s'}^a$ y $R_{s,s'}^a$).\n",
    "\n",
    "\n",
    "La utilización de la función acción-valor $Q(s,a)$ tiene algunos inconvenientes:\n",
    "\n",
    "- Requiere más memoria que la función de valor $V(s)$, ya que para cada estado debe mantener un valor para cada posible acción.\n",
    "\n",
    "\n",
    "- Una política avariciosa siguiendo una cierta función acción-valor provoca que para cada estado sólo se evalúa una acción, lo que deja gran parte del espacio sin explorar.\n",
    "\n",
    "\n",
    "- Para evitar lo segundo, se utilizan políticas con carácter exploratorio, como $\\epsilon$*-greedy*.\n",
    "\n",
    "\n",
    "#### Algoritmo\n",
    "\n",
    "El algoritmo para estimar la función acción-valor es el siguiente:\n",
    "\n",
    "- a) Generar un episodio siguiendo la política $\\pi$. Cuando se completa un episodio ya se dispone de la información referente a los estados por donde se pasó, las acciones que se tomaron, las recompensas que se recibieron y, principalmente, los retornos desde cada estado por donde se transcurrió.\n",
    "\n",
    "\n",
    "- b) Para cada estado $s$ que aparezca en este episodio\n",
    "        – R = retorno alcanzado desde el estado s\n",
    "        \n",
    "        – a = acción realizada en dicho estado\n",
    "        \n",
    "        – Añadir R a la lista de retornos obtenidos desde s con a\n",
    "        \n",
    "        – Q(s,a) = valor medio de la lista de retornos\n",
    "        \n",
    "\n",
    "- c) Repetir a) y b)\n",
    "\n",
    "\n",
    "Dado que se puede pasar múltiples veces por el mismo estado en un episodio, existen dos formas de estimar la función acción-valor:\n",
    "- 1) Utilizar tan sólo la primera ocurrencia de cada estado (*first-visit*)\n",
    "- 2) Utilizar todas las ocurrencias del estado (*every-visit*).\n",
    "\n",
    "Se puede demostrar que la primera versión converge a la estimación correcta de la función acción-valor. La segunda opción aprovecha más información de cada episodio, pero su convergencia es más lenta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferencias temporales\n",
    "\n",
    "Los métodos de **Monte Carlo** permiten aprender la política óptima sin necesidad de conocer el comportamiento del entorno, pero requiere realizar episodios completos para poder actualizar la estimación de $V(s)$ o de $Q(s,a)$. La **programación dinámica** permite actualizar los valores de $V(s)$ o de $Q(s,a)$ estudiando los estados vecinos, sin necesidad de realizar episodios completos, pero necesita conocer el comportamiento del entorno.\n",
    "\n",
    "El aprendizaje por **diferencias temporales** (TD) es una mezcla de estos dos métodos que consiste en actualizar la función de valor $V(s)$ de los estados por donde se pasa en base a las recompensas y a los estados vecinos (como en la programación dinámica) mientras se llevan a cabo episodios completos siguiendo alguna política $\\pi$ (como en el método Monte Carlo).\n",
    "\n",
    "\n",
    "\n",
    "El algoritmo de diferencias temporales más sencillo es el siguiente:\n",
    "\n",
    "`\n",
    "Tras una transición de s a s’ con recompensa r\n",
    "V[s] = r + V[s’] = V[s] + (r + V[s’] – V[s])\n",
    "`\n",
    "\n",
    "Si consideramos un factor de descuento γ,\n",
    "\n",
    "`\n",
    "V[s] = V[s] + (r + γ·V[s’] – V[s])\n",
    "`\n",
    "    \n",
    "¿Qué sentido tiene esta ecuación? Observemos que `V[s] –  γ·V[s’]` representa la recompensa esperada en la trasición s → s’. Por tanto, `r - (V[s] – γ·V[s’])` representa la diferencia entre la recompensa esperada y la encontrada. Así que la ecuación corresponde a una actualización basada en las diferencias encontradas en cada instante.\n",
    "\n",
    "Para asegurar la convergencia es necesario considerar un factor $\\alpha < 1$\n",
    "\n",
    "`\n",
    "V[s] = V[s] + α·(r + γ·V[s’] – V[s])\n",
    "` \n",
    " \n",
    "Este algoritmo se conoce como **TD(0)**. El factor $\\alpha$ garantiza, además, que el agente se adapte a entornos no estacionarios, es decir, entornos en los que $P_{s,s'}^a$ y $R_{s,s'}^a$ varíen en el tiempo.\n",
    "\n",
    "\n",
    "#### Algoritmo\n",
    "\n",
    "```python\n",
    "while noConvergencia:\n",
    "    Inicializar episodio\n",
    "    while duraEpisodio:\n",
    "        a = Π(s)  # Acción elegida por la política en el estado s\n",
    "        r, s’ = EjecutarAcción(a)  # Recompensa y estado tras la transición\n",
    "        V[s] = V[s] + alpha * ( r + gamma * V(s’) – V(s) )\n",
    "        s = s’\n",
    "```\n",
    "\n",
    "### Sarsa: State–action–reward–state–action  \n",
    "\n",
    "Este algoritmo es similar al anterior, pero haciendo uso de la **función acción-valor** $Q(s,a)$ en lugar de la **función valor** $V(s)$. Para ello, actualizamos $Q$ de la siguiente forma:\n",
    "\n",
    "$$\n",
    "Q(s,a) = Q(s,a) + \\alpha \\cdot [r + \\gamma \\cdot Q(s’,a’) – Q(s,a)]\n",
    "$$\n",
    "\n",
    "\n",
    "####    Algoritmo\n",
    "\n",
    "```python\n",
    "while noConvergencia:\n",
    "    Inicializar episodio\n",
    "    a = Π(s)  # Acción elegida por la política en el estado s\n",
    "    while duraEpisodio:\n",
    "        r, s’ = EjecutarAcción(a)  # Recompensa y estado tras la transición\n",
    "        a’ = Π(s’)  # Acción elegida por la política en el estado s’\n",
    "        Q[s,a] = Q[s,a] + alpha * ( r + gamma * Q(s’,a’) – Q(s,a) )\n",
    "        s = s’\n",
    "        a = a’\n",
    "```\n",
    "\n",
    "\n",
    "### Q-learning\n",
    "\n",
    "El algoritmo Q-learning se basa en no tomar directamente el valor de $Q(s’,a’)$ dada por la política $\\pi$ (como en *Sarsa*), sino escoger el máximo valor de $Q$ en es el estado $s’$.\n",
    "\n",
    "\n",
    "$$\n",
    "Q(s,a) = Q(s,a) + \\alpha \\cdot [r + \\gamma \\cdot max_{a'}(Q(s',a')) - Q(s,a)]\n",
    "$$\n",
    "\n",
    "\n",
    "####    Algoritmo\n",
    "\n",
    "```python\n",
    "while noConvergencia:\n",
    "    Inicializar episodio\n",
    "    while duraEpisodio:\n",
    "        a = Π(s)  # Elegir acción utilizando una política (p.e. ε-greedy)\n",
    "        (r, s’) = EjecutarAcción(a)  # Recompensa y estado tras la transición\n",
    "        Q(s,a) = Q(s,a) + α · [ r + γ · maxa’( Q(s’,a’)) – Q(s,a) ]\n",
    "        s = s’\n",
    "```\n",
    "\n",
    "### TD(1), TD(2),...\n",
    "\n",
    "Los algoritmos TD(0), Sarsa y Q-learning realizan una estimación del retorno esperado $R_t$ de un estado en un único paso:\n",
    "\n",
    "$$\n",
    "R_t(s_t) \\approx r_{t+1} + \\gamma \\cdot V_t(S_{t+1})\n",
    "$$\n",
    "\n",
    "Los métodos de Monte Carlo se basan en estimar el retorno esperado utilizando un episodio completo:\n",
    "\n",
    "$$\n",
    "R_t(s_t) \\approx r_{t+1} + \\gamma \\cdot r_{t+2} + \\gamma^2 \\cdot r_{t+3} + \\gamma^3 \\cdot r_{t+4} + \\dots\n",
    "$$\n",
    "\n",
    "\n",
    "Una modificación interesante es considerar diferencias temporales con un número de pasos mayor:\n",
    "\n",
    "$$\n",
    "R_t(s_t)^{(2)} \\approx r_{t+1} + \\gamma \\cdot r_{t+2} + \\gamma^2 \\cdot V_t(S_{t+2})\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_t(s_t)^{(3)} \\approx r_{t+1} + \\gamma \\cdot r_{t+2} + \\gamma^2 \\cdot r_{t+3} + \\gamma^3 \\cdot V_t(S_{t+3})\n",
    "$$\n",
    "\n",
    "Esto se conoce como algoritmos TD(1), TD(2), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos on-policy y off-policy\n",
    "\n",
    "La filosofía de TD(0) y *Sarsa* es desarrollar muchos episodios para evaluar una política. Una vez evaluada correctamente se actualiza la política y se vuelve a evaluar. Para generar los episodios se utiliza la política a evaluar. Esto se conoce como métodos **on-policy**. Por el contrario, se denominan métodos **off-policy** a aquellos en los que la actualización de los valores no se basa en la política a evaluar sino en una búsqueda directa de la política óptima, como, por ejemplo, el método *Q-Learning*.\n",
    "\n",
    "\n",
    "Los métodos **on-policy** tratan de mejorar la política recogiendo datos a los que llega siguiendo su política actual. Los métodos **off-policy** mejoran la política accediendo a estados que no siempre están elegidos por su política. Expliquemos esto un poco mejor. Hemos visto que los métodos que no usan programación dinámica deben explorar estados para acceder a nueva información. Supongamos que elegimos una política $\\pi$ que permita cierto grado de exploración, como, por ejemplo,  $\\epsilon$-*greedy*. En un algoritmo, como *Q-Learning* o *Sarsa*, con política $\\epsilon$-*greedy* la selección del nuevo estado se lleva a cabo haciendo uso de una probabilidad $\\epsilon$ (por ejemplo, $\\epsilon = 10\\%$). Lo que quiere decir que en el 90% de las ocasiones el nuevo estado $s’$ vendrá dado por la acción $a$ con el máximo valor de $Q$ en el estado $s$ (explotación), y un 10% de las veces la acción $a$ se seleccionará aleatoriamente (exploración). \n",
    "\n",
    "Una vez tenemos clara la política que vamos a utilizar, tenemos que ver cómo esa política se optimiza. La política mejora a medida que vamos actualizando la tabla $Q$, es decir, tomamos mejores decisiones cuanta mayor información nos da $Q$. Recordemos cómo actualiza la tabla $Q$ el algoritmo *Sarsa*:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\cdot [r + \\gamma \\cdot Q(s',a') - Q(s,a)]\n",
    "$$\n",
    "\n",
    "Y ahora veamos cómo lo hace el algoritmo *Q-Learning*:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\cdot [r + \\gamma \\cdot max_{a'}(Q(s',a')) - Q(s,a)]\n",
    "$$\n",
    "\n",
    "La diferencia está en que el algoritmo *Q-Learning* ha actualizado $Q$ haciendo uso de la acción $a’$ sobre $s’$ que mayor valor le ofrece. Por tanto, la acción $a’$ no ha venido seleccionada por la política $\\pi$, (*off-policy*). Sin embargo, en el algoritmo *Sarsa* la actualización se lleva a cabo por $Q(s’,a’)$ en donde $a’$ sí ha venido dada por la política $\\pi$, (*on-policy*).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

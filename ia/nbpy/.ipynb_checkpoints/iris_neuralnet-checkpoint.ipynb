{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "\n",
    "# Redes Neuronales\n",
    "\n",
    "Una red neuronal es un clasificador. Su funcionamiento se basa en la combinación de hiperplanos que acotan regiones de activación del espacio de muestras.\n",
    "\n",
    "\n",
    "### Mini-batch\n",
    "\n",
    "En cada ciclo de actualización de los pesos de la red no es necesario calcular el gradiente de descenso para cada muestra. Basta con escoger un lote variado de muestras (mini-batch) y calcular el gradiente resultante de éste. \n",
    "\n",
    "### Loss\n",
    "\n",
    "La pérdida o *loss* es el valor o error resultante de comparar las predicciones de la red con las etiquetas de las muestras.\n",
    "\n",
    "### One-hot\n",
    "\n",
    "Cuando se clasifica en más de dos categorías es necesario que la red tenga tantas neuronas en la capa de salida como clases haya. Por tanto, la activación de cada una de estas neuronas de salida corresponderá con una clase. Por ejemplo, si la red tiene cuatro salidas y se activa la tercera, el vector resultante podría ser algo como: [0.002,0.008,0.95,0.04]. Como normalmente las etiquetas de las muestras están en un formato numérico entero (0,1,2,3...), no podremos realizar la comparación de las salidas con las etiquetas. Así que transformamos la etiqueta en un vector de todo ceros excepto el lugar correspondiente a la etiqueta, el cual tendrá un valor de 1. Por ejemplo: $2\\rightarrow [0,0,1,0]$. (Recordemos que las etiquetas comienzan en 0). Esta función de transformación se denomina **One-hot**.\n",
    "\n",
    "### Época\n",
    "\n",
    "Cada vez que entrenamos una red con todos los lotes o *mini-batches* de nuestro dataset de entrenamiento decimos que hemos completado una **época**. Durante el proceso de entrenamiento de una red se suelen completar múltiples épocas.\n",
    "\n",
    "### Softmax\n",
    "\n",
    "La función softmax es parecida a la sigmoide, ésta también convierte un valor entre menos infinito y más infinito en un valor entre cero y uno. La diferencia está en que la función softmax no trabaja sobre un valor sino sobre un vector. De esta forma, convierte todas las componentes de un vector en valores entre cero y uno, pero, además, garantiza que la suma de todos estos valores sea uno. La función softmax se emplea en la capa de salida de una red, y convierte de esta forma sus salidas en una distribución de probabilidades.\n",
    "\n",
    "$$ Softmax(\\textbf{v})_i = \\frac{e^{v_i}}{\\sum_{j=1}^{n} e^{v_j}} $$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/red.svg\" width=30%>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

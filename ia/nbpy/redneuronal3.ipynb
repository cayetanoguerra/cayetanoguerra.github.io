{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales 3\n",
    "\n",
    "La derivación es el aspecto algorítmicamente más costoso en la utilización de clasificadores basados en redes neuronales. Calcular las decenas, centenares o centenares de miles de derivadas parciales sería una tarea abrumadora si la tuviéramos que resolver a mano cada vez que construimos una red neuronal. Afortunadamente contamos con herramientas que hacen eficientemente esa labor por nosotros. **Keras** es una de ellas. \n",
    "\n",
    "## Keras\n",
    "\n",
    "[Keras](https://keras.io/) es una API escrita en Python que nos permite de una forma rápida y cómoda configurar y entrenar redes neuronales.\n",
    "\n",
    "<img src=\"imgs/keras-logo.png\" width=\"30%\">\n",
    "\n",
    "Hay otra cosa que Keras hace por nosotros y que es sumamente importante: trasladar el cálculo a la GPU en lugar de hacerlo en la CPU. Todo el cálculo que realiza la red para generar una salida es computacionalmente muy alto. Son muchísimas las multiplicaciones y sumas que se llevan a cabo. Pero, afortunadamente, la inmensa mayoría de estas operaciones son paralelizables. Y, de la misma forma que los juegos actuales utilizan la GPU para poder mover rápidamente una inmensa cantidad de puntos, vértices y polígonos, esta misma arquitectura de computación paralela se adapta perfectamente a las necesidades de cálculo de las redes neuronales.\n",
    "\n",
    "Por supuesto, cuando las redes y sus conjuntos de datos son pequeños no es indispensable disponer de GPU en el ordenador. Pero en cuanto el modelo o los datos crecen el tiempo de cómputo se vuelve crucial.\n",
    "\n",
    "Keras, a su vez, se apoya sobre otras herramientas como [Tensorflow](https://www.tensorflow.org/) y [CUDA](https://developer.nvidia.com/cuda-zone) (si disponemos de GPU).\n",
    "\n",
    "### Instalación\n",
    "\n",
    "Visita [https://keras.io/#installation](https://keras.io/#installation) para instalar Keras. Antes debes [instalar Tensorflow](https://www.tensorflow.org/install). Y si quieres disponer de un espacio con todo ya instalado y con acceso a GPU puedes usar [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true).\n",
    "\n",
    "\n",
    "**Modo rápido:** Si quieres, puedes realizar una instalación limpia de Keras mediante un entorno virtual (recomendable). \n",
    "\n",
    "Instala virtualenv\n",
    "\n",
    "<code>pip install virtualenv</code>\n",
    "\n",
    "Crea el entorno virtual\n",
    "\n",
    "<code>virtualenv nombre_de_tu_entorno -p python3</code>\n",
    "\n",
    "Activa el entorno virtual\n",
    "\n",
    "<code>nombre_de_tu_entorno\\Scripts\\activate.bat</code>\n",
    "\n",
    "Dentro del entorno, instala Tensorflow:\n",
    "\n",
    "<code>pip install tensorflow</code>\n",
    "\n",
    "y luego, Keras:\n",
    "\n",
    "<code>pip install keras</code>\n",
    "\n",
    "Con esto tendrás una instalación de Keras para realizar las prácticas, pero sin GPU. Si tu ordenador no tiene GPU, entonces será la opción adecuada. Recuerda que todo esto también lo puedes hacer desde el Pycharm en el momento de crear un proyecto nuevo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "La estructura principal de Keras es el **modelo**, lo cual es una forma de organizar y conectar capas de neuronas. El tipo de modelo más simple es el **modelo secuencial**, que es una pila lineal de capas. Para arquitecturas más complejas, es necesario utilizar la **API funcional** de Keras, que permite crear capas con conexiones arbitrarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()  # Creamos nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora apilaremos capas con <code>.add()</code>. Keras denomina a las capas neuronales básicas como **densas** <code>Dense</code>, lo que significa que todas las entradas son conectadas a todas las neuronas. Como observamos en la figura siguiente, todas las $n$ entradas se conectan a todas las $m$ neuronas. Veremos más adelante que esto no siempre es así. Hay capas denominadas **convolutivas** que no siguen este patrón de conexión, sino que parte de las entradas se conectan solo a algunas neuronas de la capa. Pero, por ahora, eso es otra historia.\n",
    "\n",
    "<img src=\"imgs/densa.svg\" width=\"30%\">\n",
    "\n",
    "Fíjate que en la instanciación de la primera capa densa tenemos que especificar el número de entradas <code>input_dim=4</code>, pero en la siguiente capa no. Keras sabe que son $5$ entradas puesto que en la capa anterior hay $5$ neuronas <code>units=5</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=5, activation='sigmoid', input_dim=4))  # Son 4 entradas conectadas a 5 neuronas\n",
    "model.add(Dense(units=3, activation='softmax'))  # Son 5 entradas conectadas a 3 neuronas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del entrenamiento\n",
    "\n",
    "Una vez configurado el modelo, especificaremos el proceso de aprendizaje. La **función de error** o **pérdida** (**loss**) que utilizaremos es <code>loss='mse'</code>, lo que significa *mean squared error* o error cuadrático medio. Es parecida a la suma de todos los errores que ya hemos visto pero dividido por el número de muestras sobre las que calculamos el error. Existen otras funciones de error que ya veremos más adelante. En cuando al optimizador, usaremos el clásico descenso por el gradiente <code>optimizer=keras.optimizers.SGD(lr=1)</code> (stochastic gradient descent) al que le aplicamos un *learning rate* o **tasa de aprendizaje** de $1$. \n",
    "\n",
    "\\- Lo de *descenso por el gradiente*, vale. Pero, ¿lo de *estocástico* qué significa? \n",
    "\n",
    "En este contexto, **estocástico** significa que no vamos a calcular el error sobre todo el conjunto de muestras en cada iteración sino sobre un subconjunto **aleatorio** de ellos. Ese subconjunto corresponde al *mini-batch* que ya hemos visto.\n",
    "\n",
    "Durante el proceso de entrenamiento vamos progresivamente descendiendo por la función de error hasta llegar a algún mínimo. Para verificar que vamos avanzando adecuadamente es conveniente ir comprobando cómo el valor de la función de error va bajando (*loss*). Sin embargo, *loss* es un valor que no indica nada en sí, solo que si baja es buena señal. Pero, lo que realmente nos va diciendo cuánto vamos mejorando es el *accuracy* (precisión) <code>metrics=['accuracy']</code>. Este valor se calcula introduciendo las muestras en la red y comprobando qué porcentaje de ellas ha clasificado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=keras.optimizers.SGD(lr=1),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de los datos. Entrenamiento y test\n",
    "\n",
    "Una vez completamente definido el modelo y cómo lo vamos a entrenar es necesario preparar los datos sobre los que vamos a trabajar. Esta vez vamos a dividir nuestro conjunto de muestras en dos subconjuntos: uno  para **entrenar** (*train*) y otro para **verificar** (*test*). Cuando entrenamos una red neuronal con un conjunto de muestras, ¿cómo podemos estar seguros de que esa red es capaz de clasificar correctamente nuevas muestras que nunca haya visto antes? Dicho de otro modo, ¿cómo podemos saber si la red puede **generalizar**?\n",
    "\n",
    "Cuando de pequeños aprendemos a sumar, lo hacemos practicando con muchos ejercicios. Cuando dominamos la técnica somos capaces de realizar cualquier suma. No nos hemos aprendido de memoria el resultado de todos los ejemplos con lo que hemos aprendido. A eso se llama generalizar.\n",
    "\n",
    "Para saber si una red ha aprendido correctamente hacemos esta división. Vamos a entrenar la red con el **conjunto de entrenamiento**. Y una vez entrenada comprobaremos cuántas muestras del **conjunto de test** es capaz de clasificar correctamente. Si el porcentaje de aciertos es satisfactorio concluimos que la red está correctamente entrenada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('data/iris.data', delimiter=\",\")  # Cargamos el dataset iris\n",
    "np.random.shuffle(data)  # Desordenamos el dataset para conseguir minibatchs con suficiente variación de muestras\n",
    "\n",
    "def one_hot(x, n):\n",
    "    if type(x) == list:\n",
    "        x = np.array(x)\n",
    "    x = x.flatten()\n",
    "    o_h = np.zeros((len(x), n))\n",
    "    o_h[np.arange(len(x)), x] = 1\n",
    "    return o_h\n",
    "\n",
    "x_data = data[:, 0:4].astype('f4')  # Muestras\n",
    "y_data = one_hot(data[:, 4].astype(int), 3)  # Etiquetas en formato one-hot\n",
    "\n",
    "# Tomamos el 80% (120) de las muestras para entrenar y el 20% (30) para testear\n",
    "x_train = x_data[:120]\n",
    "x_test = x_data[120:]\n",
    "y_train = y_data[:120]\n",
    "y_test = y_data[120:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Procedemos a hacer el entrenamiento de la red. Para ello solo tenemos que invocar al método <code>fit</code> del modelo y especificarle que queremos entrenar $100$ épocas y que use un tamaño de lote de $15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "120/120 [==============================] - 0s 392us/step - loss: 0.2549 - accuracy: 0.3583\n",
      "Epoch 2/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.2214 - accuracy: 0.4417\n",
      "Epoch 3/200\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.2210 - accuracy: 0.3167\n",
      "Epoch 4/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.2086 - accuracy: 0.5417\n",
      "Epoch 5/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1907 - accuracy: 0.7333\n",
      "Epoch 6/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1719 - accuracy: 0.6833\n",
      "Epoch 7/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1518 - accuracy: 0.6917\n",
      "Epoch 8/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1399 - accuracy: 0.7083\n",
      "Epoch 9/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1318 - accuracy: 0.7333\n",
      "Epoch 10/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1211 - accuracy: 0.7000\n",
      "Epoch 11/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1171 - accuracy: 0.7583\n",
      "Epoch 12/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1131 - accuracy: 0.7917\n",
      "Epoch 13/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1103 - accuracy: 0.7667\n",
      "Epoch 14/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1061 - accuracy: 0.7833\n",
      "Epoch 15/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1139 - accuracy: 0.7000\n",
      "Epoch 16/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0975 - accuracy: 0.8750\n",
      "Epoch 17/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1051 - accuracy: 0.8000\n",
      "Epoch 18/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0938 - accuracy: 0.8417\n",
      "Epoch 19/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0950 - accuracy: 0.8417\n",
      "Epoch 20/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0980 - accuracy: 0.7917\n",
      "Epoch 21/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0858 - accuracy: 0.8500\n",
      "Epoch 22/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0887 - accuracy: 0.8417\n",
      "Epoch 23/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0864 - accuracy: 0.8667\n",
      "Epoch 24/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0889 - accuracy: 0.8833\n",
      "Epoch 25/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0748 - accuracy: 0.8917\n",
      "Epoch 26/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0667 - accuracy: 0.9167\n",
      "Epoch 27/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0920 - accuracy: 0.8000\n",
      "Epoch 28/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0749 - accuracy: 0.8833\n",
      "Epoch 29/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0634 - accuracy: 0.9250\n",
      "Epoch 30/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0631 - accuracy: 0.9333\n",
      "Epoch 31/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0656 - accuracy: 0.8917\n",
      "Epoch 32/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0626 - accuracy: 0.8833\n",
      "Epoch 33/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0632 - accuracy: 0.8917\n",
      "Epoch 34/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0780 - accuracy: 0.8333\n",
      "Epoch 35/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0683 - accuracy: 0.8917\n",
      "Epoch 36/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0610 - accuracy: 0.9083\n",
      "Epoch 37/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0726 - accuracy: 0.8333\n",
      "Epoch 38/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0470 - accuracy: 0.9417\n",
      "Epoch 39/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0504 - accuracy: 0.9250\n",
      "Epoch 40/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0785 - accuracy: 0.8250\n",
      "Epoch 41/200\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.93 - 0s 125us/step - loss: 0.0498 - accuracy: 0.9333\n",
      "Epoch 42/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0373 - accuracy: 0.9750\n",
      "Epoch 43/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0760 - accuracy: 0.8333\n",
      "Epoch 44/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0416 - accuracy: 0.9500\n",
      "Epoch 45/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0568 - accuracy: 0.9000\n",
      "Epoch 46/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0328 - accuracy: 0.9833\n",
      "Epoch 47/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0401 - accuracy: 0.9333\n",
      "Epoch 48/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0921 - accuracy: 0.7917\n",
      "Epoch 49/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0337 - accuracy: 0.9667\n",
      "Epoch 50/200\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0360 - accuracy: 0.9500\n",
      "Epoch 51/200\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0440 - accuracy: 0.9167\n",
      "Epoch 52/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0374 - accuracy: 0.9500\n",
      "Epoch 53/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0966 - accuracy: 0.8000\n",
      "Epoch 54/200\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0403 - accuracy: 0.9417\n",
      "Epoch 55/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0520 - accuracy: 0.9000\n",
      "Epoch 56/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0390 - accuracy: 0.9333\n",
      "Epoch 57/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0462 - accuracy: 0.9250\n",
      "Epoch 58/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0354 - accuracy: 0.9417\n",
      "Epoch 59/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0627 - accuracy: 0.8750\n",
      "Epoch 60/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0561 - accuracy: 0.8917\n",
      "Epoch 61/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0765 - accuracy: 0.8417\n",
      "Epoch 62/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0294 - accuracy: 0.9500\n",
      "Epoch 63/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0251 - accuracy: 0.9667\n",
      "Epoch 64/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0459 - accuracy: 0.9083\n",
      "Epoch 65/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0418 - accuracy: 0.9250\n",
      "Epoch 66/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0301 - accuracy: 0.9500\n",
      "Epoch 67/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0496 - accuracy: 0.9167\n",
      "Epoch 68/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0402 - accuracy: 0.9250\n",
      "Epoch 69/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0366 - accuracy: 0.9500\n",
      "Epoch 70/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0297 - accuracy: 0.9500\n",
      "Epoch 71/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0369 - accuracy: 0.9333\n",
      "Epoch 72/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0226 - accuracy: 0.9667\n",
      "Epoch 73/200\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0355 - accuracy: 0.9417\n",
      "Epoch 74/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0222 - accuracy: 0.9667\n",
      "Epoch 75/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0308 - accuracy: 0.9500\n",
      "Epoch 76/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0308 - accuracy: 0.9417\n",
      "Epoch 77/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0237 - accuracy: 0.9583\n",
      "Epoch 78/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0384 - accuracy: 0.9333\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 117us/step - loss: 0.0233 - accuracy: 0.9583\n",
      "Epoch 80/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0401 - accuracy: 0.9417\n",
      "Epoch 81/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0235 - accuracy: 0.9667\n",
      "Epoch 82/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0236 - accuracy: 0.9667\n",
      "Epoch 83/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0245 - accuracy: 0.9500\n",
      "Epoch 84/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0445 - accuracy: 0.9083\n",
      "Epoch 85/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0802 - accuracy: 0.8250\n",
      "Epoch 86/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0217 - accuracy: 0.9583\n",
      "Epoch 87/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0502 - accuracy: 0.9083\n",
      "Epoch 88/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0244 - accuracy: 0.9583\n",
      "Epoch 89/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0268 - accuracy: 0.9583\n",
      "Epoch 90/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0360 - accuracy: 0.9333\n",
      "Epoch 91/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0156 - accuracy: 0.9833\n",
      "Epoch 92/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0315 - accuracy: 0.9417\n",
      "Epoch 93/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0308 - accuracy: 0.9250\n",
      "Epoch 94/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0246 - accuracy: 0.9500\n",
      "Epoch 95/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0200 - accuracy: 0.9667\n",
      "Epoch 96/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0249 - accuracy: 0.9583\n",
      "Epoch 97/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0402 - accuracy: 0.9250\n",
      "Epoch 98/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0267 - accuracy: 0.9333\n",
      "Epoch 99/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0368 - accuracy: 0.9250\n",
      "Epoch 100/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0331 - accuracy: 0.9417\n",
      "Epoch 101/200\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0484 - accuracy: 0.9083\n",
      "Epoch 102/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0485 - accuracy: 0.9000\n",
      "Epoch 103/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0251 - accuracy: 0.9750\n",
      "Epoch 104/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0350 - accuracy: 0.9250\n",
      "Epoch 105/200\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.93 - 0s 108us/step - loss: 0.0292 - accuracy: 0.9417\n",
      "Epoch 106/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0406 - accuracy: 0.9167\n",
      "Epoch 107/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0260 - accuracy: 0.9417\n",
      "Epoch 108/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0228 - accuracy: 0.9583\n",
      "Epoch 109/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0277 - accuracy: 0.9500\n",
      "Epoch 110/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0241 - accuracy: 0.9667\n",
      "Epoch 111/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0297 - accuracy: 0.9500\n",
      "Epoch 112/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0178 - accuracy: 0.9667\n",
      "Epoch 113/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0430 - accuracy: 0.8917\n",
      "Epoch 114/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0224 - accuracy: 0.9667\n",
      "Epoch 115/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0208 - accuracy: 0.9750\n",
      "Epoch 116/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0195 - accuracy: 0.9667\n",
      "Epoch 117/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0324 - accuracy: 0.9333\n",
      "Epoch 118/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0204 - accuracy: 0.9583\n",
      "Epoch 119/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0163 - accuracy: 0.9833\n",
      "Epoch 120/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0169 - accuracy: 0.9750\n",
      "Epoch 121/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0239 - accuracy: 0.9667\n",
      "Epoch 122/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0330 - accuracy: 0.9333\n",
      "Epoch 123/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0348 - accuracy: 0.9333\n",
      "Epoch 124/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0231 - accuracy: 0.9500\n",
      "Epoch 125/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0165 - accuracy: 0.9750\n",
      "Epoch 126/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0403 - accuracy: 0.9083\n",
      "Epoch 127/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0194 - accuracy: 0.9667\n",
      "Epoch 128/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0203 - accuracy: 0.9667\n",
      "Epoch 129/200\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0327 - accuracy: 0.9417\n",
      "Epoch 130/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0243 - accuracy: 0.9583\n",
      "Epoch 131/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0235 - accuracy: 0.9417\n",
      "Epoch 132/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0157 - accuracy: 0.9750\n",
      "Epoch 133/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0266 - accuracy: 0.9417\n",
      "Epoch 134/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0216 - accuracy: 0.9583\n",
      "Epoch 135/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0214 - accuracy: 0.9583\n",
      "Epoch 136/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0304 - accuracy: 0.9250\n",
      "Epoch 137/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0188 - accuracy: 0.9750\n",
      "Epoch 138/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0209 - accuracy: 0.9583\n",
      "Epoch 139/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0213 - accuracy: 0.9583\n",
      "Epoch 140/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0223 - accuracy: 0.9583\n",
      "Epoch 141/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0162 - accuracy: 0.9583\n",
      "Epoch 142/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0284 - accuracy: 0.9500\n",
      "Epoch 143/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0213 - accuracy: 0.9667\n",
      "Epoch 144/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0218 - accuracy: 0.9750\n",
      "Epoch 145/200\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0205 - accuracy: 0.9583\n",
      "Epoch 146/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0241 - accuracy: 0.9417\n",
      "Epoch 147/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0206 - accuracy: 0.9583\n",
      "Epoch 148/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0145 - accuracy: 0.9750\n",
      "Epoch 149/200\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0189 - accuracy: 0.9583\n",
      "Epoch 150/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0216 - accuracy: 0.9583\n",
      "Epoch 151/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0178 - accuracy: 0.9667\n",
      "Epoch 152/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0209 - accuracy: 0.9583\n",
      "Epoch 153/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0191 - accuracy: 0.9750\n",
      "Epoch 154/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0220 - accuracy: 0.9583\n",
      "Epoch 155/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0186 - accuracy: 0.9667\n",
      "Epoch 156/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0250 - accuracy: 0.9417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0280 - accuracy: 0.9500\n",
      "Epoch 158/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0202 - accuracy: 0.9667\n",
      "Epoch 159/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0194 - accuracy: 0.9583\n",
      "Epoch 160/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0283 - accuracy: 0.9333\n",
      "Epoch 161/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0226 - accuracy: 0.9500\n",
      "Epoch 162/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0188 - accuracy: 0.9667\n",
      "Epoch 163/200\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0221 - accuracy: 0.9667\n",
      "Epoch 164/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0150 - accuracy: 0.9750\n",
      "Epoch 165/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0202 - accuracy: 0.9583\n",
      "Epoch 166/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0173 - accuracy: 0.9750\n",
      "Epoch 167/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0410 - accuracy: 0.9083\n",
      "Epoch 168/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0157 - accuracy: 0.9750\n",
      "Epoch 169/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0251 - accuracy: 0.9417\n",
      "Epoch 170/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0148 - accuracy: 0.9833\n",
      "Epoch 171/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0260 - accuracy: 0.9417\n",
      "Epoch 172/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0225 - accuracy: 0.9583\n",
      "Epoch 173/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0249 - accuracy: 0.9500\n",
      "Epoch 174/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0152 - accuracy: 0.9750\n",
      "Epoch 175/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0179 - accuracy: 0.9667\n",
      "Epoch 176/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0169 - accuracy: 0.9667\n",
      "Epoch 177/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0190 - accuracy: 0.9667\n",
      "Epoch 178/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0247 - accuracy: 0.9417\n",
      "Epoch 179/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0495 - accuracy: 0.9083\n",
      "Epoch 180/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0297 - accuracy: 0.9333\n",
      "Epoch 181/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0237 - accuracy: 0.9583\n",
      "Epoch 182/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0442 - accuracy: 0.9083\n",
      "Epoch 183/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0168 - accuracy: 0.9833\n",
      "Epoch 184/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0159 - accuracy: 0.9667\n",
      "Epoch 185/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0192 - accuracy: 0.9750\n",
      "Epoch 186/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0160 - accuracy: 0.9750\n",
      "Epoch 187/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0208 - accuracy: 0.9667\n",
      "Epoch 188/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0270 - accuracy: 0.9500\n",
      "Epoch 189/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0168 - accuracy: 0.9750\n",
      "Epoch 190/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0197 - accuracy: 0.9500\n",
      "Epoch 191/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0398 - accuracy: 0.9167\n",
      "Epoch 192/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0172 - accuracy: 0.9750\n",
      "Epoch 193/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0203 - accuracy: 0.9667\n",
      "Epoch 194/200\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0148 - accuracy: 0.9750\n",
      "Epoch 195/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0251 - accuracy: 0.9583\n",
      "Epoch 196/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0196 - accuracy: 0.9750\n",
      "Epoch 197/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0367 - accuracy: 0.9250\n",
      "Epoch 198/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0164 - accuracy: 0.9750\n",
      "Epoch 199/200\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0272 - accuracy: 0.9500\n",
      "Epoch 200/200\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0217 - accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=200, batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización\n",
    "\n",
    "El objeto <code>history</code> que nos devuelve el método <code>fit</code> contiene la información acerca del progreso del entrenamiento. Vemos cómo el valor de *loss* va decreciendo mientras el *accuracy* va aproximándose a $1$, lo que representa casi el 100% de las muestras de entramiento bien clasificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "\n",
    "plt.title('Entrenamiento IRIS')\n",
    "plt.xlabel('Épocas')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Durante el entrenamiento *accuracy* nos indica el porcentaje de aciertos sobre el mismo conjunto de entrenamiento. Pero nos interesa conocer el porcentaje de acierto sobre un conjunto no visto antes por la red. Para ello utilizamos el conjunto de test. Sobre este conjunto vemos que el valor de *loss* es $0.0215$ y el *accuracy* (que es el importante) es del $96.7\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "\r",
      "30/30 [==============================] - 0s 400us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.014211722649633884, 0.9666666388511658]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producción\n",
    "\n",
    "Una vez entrenada y testeada la red, podemos ponerla en producción. Keras tiene funciones para guardar tanto el modelo como los pesos ya entrenados. Si queremos hacer una clasifiación invocaremos el método <code>predict</code> del modelo.\n",
    "\n",
    "Vamos a ver qué resultados nos ofrece la red si introducimos el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.023 0.961 0.016] -> [0. 1. 0.]\n",
      "[0.021 0.944 0.035] -> [0. 1. 0.]\n",
      "[0.009 0.351 0.64 ] -> [0. 1. 0.] ✘\n",
      "[0.969 0.028 0.003] -> [1. 0. 0.]\n",
      "[0.978 0.019 0.003] -> [1. 0. 0.]\n",
      "[0.007 0.273 0.719] -> [0. 0. 1.]\n",
      "[0.022 0.938 0.04 ] -> [0. 1. 0.]\n",
      "[0.002 0.08  0.918] -> [0. 0. 1.]\n",
      "[0.024 0.959 0.016] -> [0. 1. 0.]\n",
      "[0.002 0.077 0.921] -> [0. 0. 1.]\n",
      "[0.002 0.077 0.921] -> [0. 0. 1.]\n",
      "[0.974 0.023 0.003] -> [1. 0. 0.]\n",
      "[0.021 0.957 0.022] -> [0. 1. 0.]\n",
      "[0.028 0.954 0.017] -> [0. 1. 0.]\n",
      "[0.984 0.013 0.003] -> [1. 0. 0.]\n",
      "[0.974 0.023 0.003] -> [1. 0. 0.]\n",
      "[0.021 0.888 0.091] -> [0. 1. 0.]\n",
      "[0.982 0.015 0.003] -> [1. 0. 0.]\n",
      "[0.022 0.956 0.022] -> [0. 1. 0.]\n",
      "[0.021 0.961 0.018] -> [0. 1. 0.]\n",
      "[0.003 0.097 0.901] -> [0. 0. 1.]\n",
      "[0.03  0.954 0.016] -> [0. 1. 0.]\n",
      "[0.977 0.02  0.003] -> [1. 0. 0.]\n",
      "[0.962 0.035 0.004] -> [1. 0. 0.]\n",
      "[0.003 0.1   0.897] -> [0. 0. 1.]\n",
      "[0.002 0.07  0.928] -> [0. 0. 1.]\n",
      "[0.002 0.068 0.93 ] -> [0. 0. 1.]\n",
      "[0.002 0.073 0.925] -> [0. 0. 1.]\n",
      "[0.002 0.077 0.921] -> [0. 0. 1.]\n",
      "[0.018 0.753 0.229] -> [0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "for p, l in zip(np.round(predictions, 3), y_test):\n",
    "    if np.argmax(p) == np.argmax(l):\n",
    "        print(p, \"->\", l)\n",
    "    else:\n",
    "        print(p, \"->\", l, \"✘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "- Instala Keras (o usa Google Colab) y crea una proyecto en PyCharm (o Anaconda o Jupyter) para que ejecutes todo el código que se muestra en esta clase.\n",
    "\n",
    "\n",
    "- Varía algunos hiperparámetros (learning rate, tamaño del mini-lote, número de neuronas en la capa oculta, número de épocas) y observa qué ocurre.\n",
    "\n",
    "\n",
    "- **Obligatorio:** Contesta [este formulario](https://forms.gle/yb1jH6mK758UTRFCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

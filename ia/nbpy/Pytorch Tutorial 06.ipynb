{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial de Pytorch 6: Transfer Learning\n",
    "\n",
    "El \"transfer learning\" (aprendizaje por transferencia) es una técnica en el campo del aprendizaje automático y la inteligencia artificial donde un modelo desarrollado para una tarea se reutiliza como punto de partida para un modelo en una segunda tarea. Es una estrategia popular en el aprendizaje profundo debido a que puede entrenar modelos de aprendizaje profundo con relativamente pocos datos. Esto es especialmente útil en casos donde la recopilación de un conjunto de datos grande y completo es difícil o costosa.\n",
    "\n",
    "Hay dos enfoques principales en el transfer learning:\n",
    "\n",
    "**Fine-Tuning (Ajuste Fino):** En este enfoque, se toma un modelo preentrenado (generalmente entrenado en un conjunto de datos grande y diverso, como ImageNet) y se ajusta ligeramente para adaptarlo a una tarea específica. Esto se hace entrenando el modelo en el nuevo conjunto de datos, pero con un ritmo de aprendizaje muy bajo, lo que permite que el modelo ajuste sus pesos finales para la nueva tarea sin olvidar lo que ya ha aprendido. A menudo, solo se reentrenan las últimas capas del modelo, mientras que las primeras capas (que suelen capturar características generales de las imágenes) se mantienen intactas.\n",
    "\n",
    "**Feature Extraction (Extracción de Características):** En este método, se utiliza un modelo preentrenado como un extractor de características fijas. Se pasan los datos a través del modelo y se extraen las características de una de las capas intermedias. Estas características se utilizan luego para entrenar un nuevo clasificador para la nueva tarea. En este caso, los pesos del modelo preentrenado no se modifican durante el entrenamiento del nuevo clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cayetano\\AppData\\Roaming\\Python\\Python37\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "C:\\Users\\Cayetano\\AppData\\Roaming\\Python\\Python37\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\Cayetano/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a57689fe3994a4a8aeedd9e9e68cd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar el modelo VGG16 preentrenado\n",
    "vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos cargado el modelo VGG16. Vamos a ver sus capas de clasificación (las últimas capas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora las capas de extracción de características (las primeras capas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/VGG16_ilu.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase predicha: 69\n"
     ]
    }
   ],
   "source": [
    "vgg16.eval()  # Poner el modelo en modo de evaluación\n",
    "\n",
    "# Función para cargar y preprocesar la imagen\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Añadir una dimensión de lote\n",
    "    return image\n",
    "\n",
    "# Cargar y preprocesar la imagen\n",
    "image_path = 'imgs/trilobite.jpeg'  # Reemplazar con la ruta de tu imagen\n",
    "image = preprocess_image(image_path)\n",
    "\n",
    "# Clasificar la imagen\n",
    "with torch.no_grad():\n",
    "    outputs = vgg16(image)\n",
    "    _, predicted = outputs.max(1)\n",
    "\n",
    "# Obtener la etiqueta de la clase predicha (necesitarás un mapeo de índices a etiquetas de ImageNet)\n",
    "print(f\"Clase predicha: {predicted.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones para las imágenes\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando el conjunto de datos\n",
    "dataset = datasets.ImageFolder('data/Sign-Language', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos en entrenamiento y prueba\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargadores de datos\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del dispositivo (usa GPU si está disponible)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg16.features:\n",
    "    layer.requires_grad = False\n",
    "\n",
    "# Definiendo la red neuronal\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(25088, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 25088)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class MyVGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyVGG16, self).__init__()\n",
    "        self.features = vgg16.features\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Creamos la red neuronal\n",
    "model = MyVGG16().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizador y Función de Pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiendo la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Loss: 23458662.6542\n",
      "Epoch 1, Batch 20, Loss: 67151.9869\n",
      "Epoch 1, Batch 30, Loss: 12279132.6561\n",
      "Epoch 1, Batch 40, Loss: 69155.5146\n",
      "Epoch 1, Batch 50, Loss: 2.3050\n",
      "Epoch 2, Batch 10, Loss: 2.3056\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7bb7a3fbc9e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[0;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 488\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m         )\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()  # modelo en modo de entrenamiento\n",
    "\n",
    "# Entrenamiento\n",
    "num_epochs = 3\n",
    "history = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}, Batch {i}, Loss: {running_loss/10:.4f}')\n",
    "            history.append(running_loss/10)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.plot(history)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test images: 10.17%\n"
     ]
    }
   ],
   "source": [
    "# Evaluación\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()  # Poner el modelo en modo de evaluación\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

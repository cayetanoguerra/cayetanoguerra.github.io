{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"./nlp/imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial de Pytorch 6:  clasificador dataset iris**\n",
    "\n",
    "En este notebook vamos a ver cómo crear un clasificador basado en una red neuronal para el dataset iris. El dataset iris es un dataset muy conocido en el mundo del machine learning y es un dataset que se utiliza para hacer pruebas de clasificación. El dataset iris contiene 150 muestras de flores iris, cada una con 4 características: longitud del sépalo, ancho del sépalo, longitud del pétalo y ancho del pétalo. Las muestras se dividen en tres especies de iris: setosa, versicolor y virginica.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../imgs/iris.png\" width=\"30%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos cargando y preparando el dataset iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5065,  1.2492, -1.5676, -1.3154],\n",
      "        [-0.1737,  3.0908, -1.2834, -1.0522],\n",
      "        [ 1.0380,  0.0982,  0.3649,  0.2641],\n",
      "        [-1.2642,  0.7888, -1.2266, -1.3154],\n",
      "        [-1.7489,  0.3284, -1.3971, -1.3154],\n",
      "        [ 0.5533, -1.2830,  0.7059,  0.9223],\n",
      "        [ 0.6745,  0.3284,  0.4217,  0.3958],\n",
      "        [-0.7795,  1.0190, -1.2834, -1.3154],\n",
      "        [-1.0218,  1.2492, -1.3402, -1.3154],\n",
      "        [-0.7795,  2.4002, -1.2834, -1.4471]])\n",
      "tensor([0, 0, 1, 0, 0, 2, 1, 0, 0, 0])\n",
      "Longitud del conjunto de entrenamiento: 120\n",
      "Longitud del conjunto de test: 30\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos el dataset Iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Normalizamos los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Lo convertimos a tensores de PyTorch\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Dividimos en conjunto de entrenamiento y de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Imprimimos los 10 primeros ejemplos y sus etiquetas\n",
    "print(X_train[:10])\n",
    "print(y_train[:10])\n",
    "print(\"Longitud del conjunto de entrenamiento:\", len(X_train))\n",
    "print(\"Longitud del conjunto de test:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo de red neuronal con cinco neuronas en la capa oculta.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./imgs/rediris.png\" width=\"30%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisNet(\n",
      "  (fc1): Linear(in_features=4, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n",
      "Número de parámetros: 43\n"
     ]
    }
   ],
   "source": [
    "# Definimos el modelo\n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 5) # 4 características de entrada, 5 neuronas en la capa oculta\n",
    "        self.fc2 = nn.Linear(5, 3) # 3 clases de salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x  # Fíjate en que no aplicamos la función de activación softmax, ya que la función de pérdida CrossEntropyLoss lo hace por nosotros\n",
    "\n",
    "model = IrisNet()\n",
    "\n",
    "print(model)\n",
    "print(\"Número de parámetros:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Pregunta\n",
    "\n",
    "1. ¿Por qué son 43 parámetros?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el criterio de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)  # Pasamos el conjunto de entrenamiento completo, rara vez se hace esto en la práctica\n",
    "    loss = criterion(output, y_train)  # Date cuenta de que pasamos las etiquetas originales, no en formato one-hot\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = y_test.size(0)\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct += (predicted == y_test).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test set: {round(100 * correct / total, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Cosas en qué fijarnos**\n",
    "¿Qué nos devuelve el modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6884,  1.6662,  0.1354],\n",
       "        [ 3.0821, -2.4969, -2.7127],\n",
       "        [-7.8466,  0.6208,  6.1597],\n",
       "        [-2.9156,  1.4927,  0.5944],\n",
       "        [-3.6345,  1.6372,  1.1046],\n",
       "        [ 2.3956, -1.9155, -2.5018],\n",
       "        [-1.5794,  1.4062, -0.7765],\n",
       "        [-5.1603,  0.8852,  3.4350],\n",
       "        [-4.4466,  1.8076,  1.8326],\n",
       "        [-2.2002,  1.8953, -0.4881],\n",
       "        [-4.0717,  0.9560,  2.2465],\n",
       "        [ 2.3428, -1.8511, -2.5544],\n",
       "        [ 2.9082, -2.3366, -2.7049],\n",
       "        [ 2.4489, -1.9432, -2.5794],\n",
       "        [ 3.3549, -2.7168, -2.8357],\n",
       "        [-2.6529,  1.2593,  0.5293],\n",
       "        [-5.3344,  0.6769,  3.7309],\n",
       "        [-2.2739,  1.9836, -0.5126],\n",
       "        [-2.5004,  1.6002,  0.0594],\n",
       "        [-5.5386,  0.8060,  3.8413],\n",
       "        [ 2.5278, -2.0166, -2.5806],\n",
       "        [-3.6611,  1.1528,  1.6581],\n",
       "        [ 2.5048, -2.0087, -2.5330],\n",
       "        [-5.3313,  0.8670,  3.5614],\n",
       "        [-4.6422,  0.5316,  3.0221],\n",
       "        [-5.2905,  0.8549,  3.5914],\n",
       "        [-5.6369,  1.1563,  3.5222],\n",
       "        [-5.4068,  0.5569,  3.9078],\n",
       "        [ 2.0617, -1.6234, -2.4320],\n",
       "        [ 2.3044, -1.8273, -2.5121]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **with torch.no_grad():**\n",
    "\n",
    "En PyTorch, <code>with torch.no_grad():</code> es un contexto que se utiliza para desactivar el cálculo y almacenamiento de gradientes que normalmente se realiza en operaciones sobre tensores. Esto es útil en situaciones donde no necesitas realizar backpropagation, es decir, no necesitas calcular los gradientes para la optimización de los parámetros del modelo. Esto suele hacerse durante la fase de evaluación o inferencia del modelo, cuando solo estás realizando predicciones o evaluando el rendimiento del modelo y no quieres modificar sus parámetros.\n",
    "\n",
    "Es muy común realizar evaluaciones del modelo durante su entrenamiento, por lo que es importante desactivar el cálculo de gradientes en estas situaciones para evitar que se acumulen en la memoria y ralenticen el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1203858852386475\n",
      "Accuracy of the model on the test set: 23.33%\n",
      "Epoch 11, Loss: 0.9088758230209351\n",
      "Accuracy of the model on the test set: 63.33%\n",
      "Epoch 21, Loss: 0.7178919315338135\n",
      "Accuracy of the model on the test set: 93.33%\n",
      "Epoch 31, Loss: 0.5924195051193237\n",
      "Accuracy of the model on the test set: 93.33%\n",
      "Epoch 41, Loss: 0.49193066358566284\n",
      "Accuracy of the model on the test set: 86.67%\n",
      "Epoch 51, Loss: 0.419420063495636\n",
      "Accuracy of the model on the test set: 86.67%\n",
      "Epoch 61, Loss: 0.36532923579216003\n",
      "Accuracy of the model on the test set: 86.67%\n",
      "Epoch 71, Loss: 0.3160657584667206\n",
      "Accuracy of the model on the test set: 93.33%\n",
      "Epoch 81, Loss: 0.27905404567718506\n",
      "Accuracy of the model on the test set: 93.33%\n",
      "Epoch 91, Loss: 0.24549810588359833\n",
      "Accuracy of the model on the test set: 93.33%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)  # Date cuenta de que pasamos las etiquetas originales, no en formato one-hot\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "        # Evaluamos el modelo con el conjunto de test\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = y_test.size(0)\n",
    "            outputs = model(X_test)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == y_test).sum().item()\n",
    "        print(f'Accuracy of the model on the test set: {round(100 * correct / total, 2)}%')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **torchsummary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages (1.5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torchsummary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [-1, 5]              25\n",
      "            Linear-2                    [-1, 3]              18\n",
      "================================================================\n",
      "Total params: 43\n",
      "Trainable params: 43\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [10, 5]              25\n",
      "            Linear-2                    [10, 3]              18\n",
      "================================================================\n",
      "Total params: 43\n",
      "Trainable params: 43\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (4,), batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales 4\n",
    "\n",
    "Vamos a crear una red neuronal para tratar ahora con un conjunto mucho más grande, el *dataset* MNIST. Verás que, a medida que tratamos con conjuntos mayores, el tiempo de procesamiento se incrementa y la necesidad de contar con una GPU crece al mismo ritmo. Échale un vistazo a [este vídeo](https://www.youtube.com/watch?v=qcOjR-sJkUY) para que te vayas familiarizando con Google Colab, por si necesitas usarlo.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Conjunto MNIST\n",
    "\n",
    "El [conjunto MNIST](https://en.wikipedia.org/wiki/MNIST_database) está formado por 70.000 imágenes de dígitos manuscritos del 0 al 9 con un tamaño de 28x28 en escala de grises. A su vez, el conjunto se divide en 60.000 imágenes para entrenamiento y 10.000 para test.\"https://www.youtube.com/embed/S_f2qV2_U00?rel=0&amp;controls=0&amp;showinfo=0\" frame\n",
    "\n",
    "\n",
    "<img src=\"imgs/mnist.jpg\" width=\"80%\">\n",
    "\n",
    "Vamos a utilizar este *dataset* para entrenar una red y ver qué precisión obtenemos al clasificar el conjunto de test. Piensa bien lo que pretendemos lograr: **hacer una red que será capaz de “ver”**, aunque, por ahora, solo sean imágenes de dos dimensiones.\n",
    "\n",
    "\n",
    "El conjunto MNIST es muy popular y se utiliza mucho para aprender y probar redes, así que Keras ya lo incluye como parte de la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Veamos la forma tiene x_train\n",
    "print(\"Shape:\", x_train.shape)  # 60.000 imágenes de 28x28\n",
    "\n",
    "# Veamos una imagen cualquiera, por ejemplo, con el índice 125\n",
    "image = np.array(x_train[125], dtype='float')\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"Label:\", y_train[125])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es necesario saber en qué rango de valores se mueven nuestras muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: 255\n",
      "Min value: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Max value:\", max(x_train[125].reshape(784)))\n",
    "print(\"Min value:\", min(x_train[125].reshape(784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que cada pixel es un byte con un rango de valores que va desde el 0 hasta el 255 en formato entero. Esta escala no es muy adecuada para la red. Podemos facilitar mucho el trabajo de entrenamiento si transformamos esta escala en otra centrada en el 0 y con un rango de valores entre -0.5 y 0.5. Y, por supuesto, en formato real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: 0.5\n",
      "Min value: -0.5\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255  # Escalamos a un rango entre 0 y 1\n",
    "x_test /= 255\n",
    "\n",
    "x_train -= 0.5  # desplazamos el rango a -0.5 y 0.5\n",
    "x_test -= 0.5\n",
    "\n",
    "print(\"Max value:\", max(x_train[125].reshape(784)))\n",
    "print(\"Min value:\", min(x_train[125].reshape(784)))\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora preparamos las etiquetas transformándolas a formato *one_hot*. Keras tiene funciones para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)  # 10 clases\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(\"Label:\", y_train[125])  # Recordemos que esta muestra tenía valor 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del modelo\n",
    "\n",
    "Tenemos que clasificar imágenes en diez categorías distintas, luego la capa final tendrá diez salidas. En cuanto a la capa de entrada, tenemos una matriz de $28 \\times 28$. Lo que vamos a hacer es transformarla en un vector de $784$ componentes (\n",
    "$28\\times28=784$). Simplemente tomaremos cada fila de la matriz y las iremos colocando secuencialmente.\n",
    "\n",
    "Para la capa o capas ocultas vamos a probar primero con una capa oculta con 20 neuronas. Recuerda que estos son los **hiperparámetros** de los que ya hemos hablado. No hay forma de saber *a priori* cuál es el número óptimo de capas ni de neuronas por capa oculta.\n",
    "\n",
    "Como funciones de activación utilizaremos sigmoides y, en la capa final, softmax.\n",
    "\n",
    "En lugar de definir la red de nuevo con el modelo secuencial de Keras, vamos a hacerlo con la [API funcional](https://keras.io/getting-started/functional-api-guide/). Esta API nos sirve para poder definir modelos más complejos que los simplemente creados a partir de acumular capas apiladas. Hay redes que tienen arquitecturas donde hay capas compartidas, las salidas de una capa pueden ir a capas separadas varias capas, etc. Hay una gran variedad. \n",
    "\n",
    "Por supuesto, la red que vamos a hacer ahora la podríamos hacer perfectamente con el modelo secuencial (y hasta nos sería más fácil), pero vamos a aprender a utilizar esta API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(784,))  # Capa de entrada\n",
    "output_h = Dense(units=20, activation='sigmoid')(inputs)  # Capa oculta\n",
    "predictions = Dense(10, activation='softmax')(output_h)  # Capa de salida\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si te has fijado, la API funcional usa la función <code>Dense(units=20, activation='sigmoid')(inputs)</code> para crear una capa en lugar del método <code>model.add(Dense(units=20, activation='sigmoid', input_dim=784))</code> del modelo. Esto nos da libertad para asignar la salida de una capa a la entrada de la capa que queramos, no obligatoriamente a la siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer=keras.optimizers.SGD(lr=1),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el entrenamiento vamos a ir viendo también cómo va evolucionando el *accuracy* del conjunto de test. Cuando usamos el conjunto de test de esta manera se suele llamar **conjunto de validación**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0476 - accuracy: 0.6893 - val_loss: 0.0210 - val_accuracy: 0.8853\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0181 - accuracy: 0.8927 - val_loss: 0.0150 - val_accuracy: 0.9089\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0147 - accuracy: 0.9081 - val_loss: 0.0131 - val_accuracy: 0.9187\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0132 - accuracy: 0.9173 - val_loss: 0.0122 - val_accuracy: 0.9233\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0122 - accuracy: 0.9230 - val_loss: 0.0114 - val_accuracy: 0.9281\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0115 - accuracy: 0.9274 - val_loss: 0.0109 - val_accuracy: 0.9300\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0109 - accuracy: 0.9306 - val_loss: 0.0105 - val_accuracy: 0.9347\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0105 - accuracy: 0.9345 - val_loss: 0.0103 - val_accuracy: 0.9347\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0101 - accuracy: 0.9368 - val_loss: 0.0098 - val_accuracy: 0.9378\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0097 - accuracy: 0.9388 - val_loss: 0.0098 - val_accuracy: 0.9363\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0094 - accuracy: 0.9419 - val_loss: 0.0095 - val_accuracy: 0.9398\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0092 - accuracy: 0.9428 - val_loss: 0.0093 - val_accuracy: 0.9406\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0089 - accuracy: 0.9445 - val_loss: 0.0090 - val_accuracy: 0.9430\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0087 - accuracy: 0.9459 - val_loss: 0.0090 - val_accuracy: 0.9430\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0085 - accuracy: 0.9474 - val_loss: 0.0087 - val_accuracy: 0.9455\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0083 - accuracy: 0.9490 - val_loss: 0.0087 - val_accuracy: 0.9445\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0082 - accuracy: 0.9496 - val_loss: 0.0087 - val_accuracy: 0.9458\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0080 - accuracy: 0.9506 - val_loss: 0.0083 - val_accuracy: 0.9466\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0078 - accuracy: 0.9518 - val_loss: 0.0083 - val_accuracy: 0.9464\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0077 - accuracy: 0.9526 - val_loss: 0.0084 - val_accuracy: 0.9477\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0076 - accuracy: 0.9534 - val_loss: 0.0082 - val_accuracy: 0.9476\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0075 - accuracy: 0.9539 - val_loss: 0.0081 - val_accuracy: 0.9480\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0074 - accuracy: 0.9545 - val_loss: 0.0080 - val_accuracy: 0.9486\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0072 - accuracy: 0.9553 - val_loss: 0.0081 - val_accuracy: 0.9475\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0072 - accuracy: 0.9557 - val_loss: 0.0079 - val_accuracy: 0.9481\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0070 - accuracy: 0.9572 - val_loss: 0.0077 - val_accuracy: 0.9495\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0069 - accuracy: 0.9579 - val_loss: 0.0077 - val_accuracy: 0.9490\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0068 - accuracy: 0.9582 - val_loss: 0.0077 - val_accuracy: 0.9507\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0068 - accuracy: 0.9586 - val_loss: 0.0076 - val_accuracy: 0.9502\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0067 - accuracy: 0.9596 - val_loss: 0.0077 - val_accuracy: 0.9504\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0066 - accuracy: 0.9596 - val_loss: 0.0075 - val_accuracy: 0.9508\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0065 - accuracy: 0.9602 - val_loss: 0.0075 - val_accuracy: 0.9508\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0065 - accuracy: 0.9614 - val_loss: 0.0074 - val_accuracy: 0.9521\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0064 - accuracy: 0.9612 - val_loss: 0.0075 - val_accuracy: 0.9495\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0063 - accuracy: 0.9625 - val_loss: 0.0073 - val_accuracy: 0.9524\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0062 - accuracy: 0.9624 - val_loss: 0.0073 - val_accuracy: 0.9526\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0062 - accuracy: 0.9627 - val_loss: 0.0074 - val_accuracy: 0.9519\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0061 - accuracy: 0.9628 - val_loss: 0.0072 - val_accuracy: 0.9526\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0061 - accuracy: 0.9636 - val_loss: 0.0074 - val_accuracy: 0.9521\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0060 - accuracy: 0.9637 - val_loss: 0.0074 - val_accuracy: 0.9513\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0060 - accuracy: 0.9647 - val_loss: 0.0072 - val_accuracy: 0.9537\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0059 - accuracy: 0.9648 - val_loss: 0.0072 - val_accuracy: 0.9522\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0059 - accuracy: 0.9652 - val_loss: 0.0071 - val_accuracy: 0.9532\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0058 - accuracy: 0.9657 - val_loss: 0.0070 - val_accuracy: 0.9555\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0057 - accuracy: 0.9658 - val_loss: 0.0071 - val_accuracy: 0.9540\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0057 - accuracy: 0.9657 - val_loss: 0.0070 - val_accuracy: 0.9546\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0057 - accuracy: 0.9661 - val_loss: 0.0071 - val_accuracy: 0.9529\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0056 - accuracy: 0.9669 - val_loss: 0.0071 - val_accuracy: 0.9546\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0056 - accuracy: 0.9667 - val_loss: 0.0073 - val_accuracy: 0.9520\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0055 - accuracy: 0.9673 - val_loss: 0.0070 - val_accuracy: 0.9550\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=50, batch_size=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "\n",
    "plt.title('Entrenamiento MNIST')\n",
    "plt.xlabel('Épocas')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjuntos de entrenamiento, validación y test\n",
    "\n",
    "Si nos enfrentáramos a un problema de clasificación con responsabilildad deberíamos ser capaces de asegurar que el rendimiento que decimos que tiene nuestra red es el que realmente tiene (y no necesariamente una red, sino a cualquier modelo de clasificación que utilicemos. No solo de redes vive el experto en *machine learning*).\n",
    "\n",
    "Para ello, hasta ahora hemos hecho uso del conjunto de test. Pero, cuando entrenamos una red hacemos muchas pruebas, muchos cambios en su configuración (los hiperparámetros) buscando una de ellas que nos dé los mejores resultados. Llegará un momento en el que hemos hecho tantas modificaciones en la red que nuestro conjunto de test logrará un buen *accuracy*. Sin embargo, ¿cómo podemos estar seguros de que la red funcionaría bien para un nuevo conjunto de test? Es decir, quizá hayamos involuntariamente optimizado la red para que funcione bien sobre el conjunto de test.\n",
    "\n",
    "La forma de asegurar que hemos entrenado una red que **generaliza** correctamente es disponer de tres conjuntos: **entrenamiento**, **validación** y **test**. Con el de entrenamiento, entrenamos, y utilizaremos el conjunto de validación para comprobar el nivel de *accuracy* logrado en ese modelo. Al final de todas las pruebas que hayamos hecho, dispondremos de nuestro modelo final. En ese momento tomaremos nuestro conjunto de test (que previamente habíamos guardado bajo llave para evitar la tentación de utilizarlo antes) y lo pasaremos por la red. El *accuracy* que nos devuelva este conjunto de test será nuestro resultado final.\n",
    "\n",
    "Nosotros en las clases no nos vamos a preocupar mucho de esto, y utilizaremos el conjunto de test también como conjunto de validación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "- Utiliza el *dataset* [FashionMNIST](https://www.kaggle.com/zalando-research/fashionmnist ) para sustituir al *dataset* MNIST de esta clase. Haz los cambios en los hiperparámetros que consideres oportunos para obtener un buen *accuracy*. [Aquí tienes una copia](data/fashionmnist.zip) del *dataset* FashionMNIST.\n",
    "\n",
    "\n",
    "- **Obligatorio:** Contesta [este formulario](https://forms.gle/cysReRAroaUoT8gg6) (fecha límite para contestarlo: 10 de abril de 2020 a las 11:59 - hora canaria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

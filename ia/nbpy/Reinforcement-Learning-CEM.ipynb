{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (Cross Entropy Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos ejecutando algunos ejemplos de interacción con el entorno elegiendo acciones al azar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_random(env):\n",
    "    for i_episode in range(20):\n",
    "        observation = env.reset()\n",
    "        for t in range(100):\n",
    "            env.render()\n",
    "            #print(observation)\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                break\n",
    "run_random(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes descomentar la impresión de las observaciones para ver qué sale. Corresponden a la posición del coche, la velocidad del coche, el ángulo del péndulo y velocidad de rotación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = [0., 0., 0., 0.]  # first means\n",
    "sigma = [1., 1., 1., 1.]  # first standard deviations\n",
    "episodies = 100\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función que ejecutará cada episodio (desde el momento inicial hasta que llegamos a los `max_reward` pasos o hasta que el péndulo se nos caiga.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episode(env, parameters, render=False, max_reward=200):\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in xrange(max_reward):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = 0 if np.matmul(parameters, observation) < 0 else 1  # this line is our agent\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos las iteraciones del método de entropía cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iteration in xrange(iterations):\n",
    "\n",
    "    parameters = []\n",
    "    for i in xrange(4):\n",
    "        parameters.append(np.random.normal(mu[i], sigma[i], episodies))\n",
    "    parameters = np.transpose(parameters)\n",
    "\n",
    "    rewards = []\n",
    "    number_of_goals = 0\n",
    "\n",
    "    for i in xrange(episodies):\n",
    "        r = run_episode(env, parameters[i], max_reward=500)\n",
    "        rewards.append(r)\n",
    "        if r == 500:\n",
    "            number_of_goals += 1\n",
    "\n",
    "    # We combine in a list parameteres+rewards and sort it by rewards.\n",
    "    # To do that we use a lambda function\n",
    "    l = sorted(zip(parameters, rewards), key=lambda pair: pair[1])\n",
    "    # We get the last ten (they will be those with the higher reward), but\n",
    "    # only first component (parameters) is needed. \n",
    "    l = list(zip(*l[-10:])[0])\n",
    "\n",
    "    mu = np.mean(l, 0)\n",
    "    sigma = np.std(l, 0)\n",
    "\n",
    "    print \"------------\"\n",
    "    print \"Iteration:\", iteration\n",
    "    print \"Mean:\", mu\n",
    "    print \"Standard deviation:\", sigma\n",
    "    print \"# goals:\", number_of_goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos un episodio con 1500 pasos a ver qué tal va."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(env, mu, render=True, max_reward=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aprendizaje por refuerzo**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Q-Learning (DQL)**\n",
    "\n",
    "A partir de la ecuación de Bellman, podemos definir la ecuación de actualización de Q-Learning:\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right]$$\n",
    "\n",
    "Ahora convertimos la tabla Q en una red neuronal $\\hat{Q}_{\\theta}$, donde la entrada es el estado $s$ y la salida es el valor Q para cada acción $a$. Pero para poder abordar el problema necesitamos tener un objetivo (target). Para esto, utilizamos una red neuronal adicional, llamada red objetivo $Q_{\\theta}$, que se utiliza para calcular el valor Q objetivo para cada acción. La red objetivo se actualiza periódicamente con los parámetros de la red principal. \n",
    "\n",
    "\n",
    "#### **Target**\n",
    "\n",
    "$$ y_i = r_i + \\gamma \\max_{a'} Q(s', a'; \\theta^-) $$\n",
    "\n",
    "#### **Loss**\n",
    "\n",
    "$$ \\mathcal{L(\\theta)} = \\mathbb{E} [ ( \\hat{Q}(s, a; \\theta_i) - y_i )^2 ] $$\n",
    "\n",
    "\n",
    "### **Componentes de la Ecuación de Pérdida:**\n",
    "\n",
    "1. $Q(s, a; \\theta)$: Es el valor Q predicho por la red principal para el estado actual $s$ y la acción $a$ utilizando los parámetros actuales $\\theta$.\n",
    "\n",
    "2. $\\hat{Q}(s', a'; \\theta^-)$: Es el valor Q calculado por la red objetivo para el próximo estado $s'$ y una posible acción $a'$, usando los parámetros $\\theta^-$.\n",
    "\n",
    "3. $r$: Es la recompensa inmediata recibida después de tomar la acción $a$ en el estado $s$.\n",
    "\n",
    "4. $\\gamma$: Es el factor de descuento que determina la importancia de las recompensas futuras.\n",
    "\n",
    "5. $\\mathcal{L}(\\theta)$: Es la función de pérdida, que mide la discrepancia entre la predicción de la red principal y el valor objetivo calculado usando la red objetivo.\n",
    "\n",
    "6. $\\mathbb{E}$: Denota la expectativa matemática, indicando que estamos tomando el promedio de la pérdida sobre muchas transiciones, generalmente tomadas de un minibatch del buffer de repetición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Policy Gradient**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Actor-Critic**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Optimal Policy Gradient**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Reinforment Learning from Human Feedback**\n",
    "\n",
    "\n",
    "Claro, puedo proporcionarte una explicación detallada sobre cómo obtener y optimizar la función de valor en el contexto del aprendizaje por refuerzo. Primero, comenzaremos con una breve introducción al aprendizaje por refuerzo y luego discutiremos cómo se puede obtener y optimizar la función de valor.\n",
    "\n",
    "### Introducción al Aprendizaje por Refuerzo\n",
    "\n",
    "El aprendizaje por refuerzo es un tipo de aprendizaje automático en el que un agente aprende a interactuar con su entorno para maximizar alguna noción de recompensa acumulativa. La función de valor es una herramienta crítica en este proceso, que se utiliza para estimar el valor esperado de las recompensas futuras que un agente puede obtener a partir de un estado dado o una pareja estado-acción.\n",
    "\n",
    "### Definición de Función de Valor\n",
    "\n",
    "En general, hay dos tipos de funciones de valor que podrías estar interesado en calcular:\n",
    "\n",
    "1. **Función de valor de estado (V(s))**: Esta función da el valor esperado de las recompensas futuras que un agente puede obtener comenzando desde el estado $ s $ y siguiendo una política $ \\pi $.\n",
    "   \n",
    "   $[V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_t \\mid S_0 = s \\right]]$\n",
    "\n",
    "2. **Función de valor de acción (Q(s, a))**: Esta función da el valor esperado de las recompensas futuras que un agente puede obtener comenzando desde el estado $ s $, tomando una acción $ a $ y luego siguiendo una política $ \\pi $.\n",
    "   \n",
    "   \\[Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_t \\mid S_0 = s, A_0 = a \\right]\\]\n",
    "\n",
    "Donde:\n",
    "- $ \\gamma $ es el factor de descuento, que determina cuánto valora el agente las recompensas futuras en comparación con las recompensas inmediatas.\n",
    "- $ R_t $ es la recompensa en el tiempo $ t $.\n",
    "\n",
    "### Obtener y Optimizar la Función de Valor\n",
    "\n",
    "Para obtener y optimizar la función de valor, puedes seguir estos pasos:\n",
    "\n",
    "1. **Inicialización**: Inicializa la función de valor arbitrariamente. Por lo general, esto se hace asignando valores iniciales aleatorios o ceros a cada estado o pareja estado-acción.\n",
    "\n",
    "2. **Política**: Define una política $ \\pi $, que es una estrategia que el agente seguirá para seleccionar acciones en cada estado.\n",
    "\n",
    "3. **Ecuaciones de Bellman**: Utiliza las ecuaciones de Bellman para actualizar iterativamente las estimaciones de la función de valor. Las ecuaciones de Bellman son herramientas matemáticas que relacionan el valor de un estado con el valor de sus estados sucesores. Las ecuaciones son:\n",
    "\n",
    "   Para $ V(s) $:\n",
    "   \\[V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s', r|s, a) \\left[ r + \\gamma V^\\pi(s') \\right]\\]\n",
    "\n",
    "   Para $ Q(s, a) $:\n",
    "   \\[Q^\\pi(s, a) = \\sum_{s',r} p(s', r|s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s', a') \\right]\\]\n",
    "\n",
    "4. **Métodos de Solución**: Usa métodos como la Iteración de Valor o la Iteración de Política para resolver las ecuaciones de Bellman y obtener una función de valor optimizada.\n",
    "\n",
    "5. **Optimización de la Política**: Una vez que tienes una estimación de la función de valor, puedes usarla para mejorar tu política, por ejemplo, usando una política greedy con respecto a la función de valor estimada.\n",
    "\n",
    "6. **Algoritmos de Aprendizaje por Refuerzo**: También puedes usar algoritmos más avanzados de aprendizaje por refuerzo como Q-learning o SARSA para aprender directamente una función de valor óptima sin necesidad de una política explícita.\n",
    "\n",
    "### Implementación\n",
    "\n",
    "Para implementar este proceso, puedes utilizar bibliotecas de Python como OpenAI Gym para simular el entorno y TensorFlow o PyTorch para implementar y optimizar las funciones de valor y la política.\n",
    "\n",
    "Espero que esto te dé una buena idea de cómo obtener y optimizar una función de valor en el aprendizaje por refuerzo. ¡Buena suerte!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos on-policy y off-policy\n",
    "\n",
    "La filosofía de TD(0) y *Sarsa* es desarrollar muchos episodios para evaluar una política. Una vez evaluada correctamente se actualiza la política y se vuelve a evaluar. Para generar los episodios se utiliza la política a evaluar. Esto se conoce como métodos **on-policy**. Por el contrario, se denominan métodos **off-policy** a aquellos en los que la actualización de los valores no se basa en la política a evaluar sino en una búsqueda directa de la política óptima, como, por ejemplo, el método *Q-Learning*.\n",
    "\n",
    "\n",
    "Los métodos **on-policy** tratan de mejorar la política recogiendo datos a los que llega siguiendo su política actual. Los métodos **off-policy** mejoran la política accediendo a estados que no siempre están elegidos por su política. Expliquemos esto un poco mejor. Hemos visto que los métodos que no usan programación dinámica deben explorar estados para acceder a nueva información. Supongamos que elegimos una política $\\pi$ que permita cierto grado de exploración, como, por ejemplo,  $\\epsilon$-*greedy*. En un algoritmo, como *Q-Learning* o *Sarsa*, con política $\\epsilon$-*greedy* la selección del nuevo estado se lleva a cabo haciendo uso de una probabilidad $\\epsilon$ (por ejemplo, $\\epsilon = 10\\%$). Lo que quiere decir que en el 90% de las ocasiones el nuevo estado $s’$ vendrá dado por la acción $a$ con el máximo valor de $Q$ en el estado $s$ (explotación), y un 10% de las veces la acción $a$ se seleccionará aleatoriamente (exploración). \n",
    "\n",
    "Una vez tenemos clara la política que vamos a utilizar, tenemos que ver cómo esa política se optimiza. La política mejora a medida que vamos actualizando la tabla $Q$, es decir, tomamos mejores decisiones cuanta mayor información nos da $Q$. Recordemos cómo actualiza la tabla $Q$ el algoritmo *Sarsa*:\n",
    "\n",
    "$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\cdot [r + \\gamma \\cdot Q(s',a') - Q(s,a)]\n",
    "$\n",
    "\n",
    "Y ahora veamos cómo lo hace el algoritmo *Q-Learning*:\n",
    "\n",
    "$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\cdot [r + \\gamma \\cdot max_{a'}(Q(s',a')) - Q(s,a)]\n",
    "$\n",
    "\n",
    "La diferencia está en que el algoritmo *Q-Learning* ha actualizado $Q$ haciendo uso de la acción $a’$ sobre $s’$ que mayor valor le ofrece. Por tanto, la acción $a’$ no ha venido seleccionada por la política $\\pi$, (*off-policy*). Sin embargo, en el algoritmo *Sarsa* la actualización se lleva a cabo por $Q(s’,a’)$ en donde $a’$ sí ha venido dada por la política $\\pi$, (*on-policy*).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aprendizaje por refuerzo**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Q-Learning (DQL)**\n",
    "\n",
    "A partir de la ecuación de Bellman, podemos definir la ecuación de actualización de Q-Learning:\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right]$$\n",
    "\n",
    "Ahora convertimos la tabla Q en una red neuronal $\\hat{Q}_{\\theta}$, donde la entrada es el estado $s$ y la salida es el valor Q para cada acción $a$. Pero para poder abordar el problema necesitamos tener un objetivo (target). Para esto, utilizamos una red neuronal adicional, llamada red objetivo $Q_{\\theta}$, que se utiliza para calcular el valor Q objetivo para cada acción. La red objetivo se actualiza periódicamente con los parámetros de la red principal. \n",
    "\n",
    "\n",
    "#### **Target**\n",
    "\n",
    "$$ y_i = r_i + \\gamma \\max_{a'} Q(s', a'; \\theta^-) $$\n",
    "\n",
    "#### **Loss**\n",
    "\n",
    "$$ \\mathcal{L(\\theta)} = \\mathbb{E} [ ( \\hat{Q}(s, a; \\theta_i) - y_i )^2 ] $$\n",
    "\n",
    "\n",
    "### **Componentes de la Ecuación de Pérdida:**\n",
    "\n",
    "1. $Q(s, a; \\theta)$: Es el valor Q predicho por la red principal para el estado actual $s$ y la acción $a$ utilizando los parámetros actuales $\\theta$.\n",
    "\n",
    "2. $\\hat{Q}(s', a'; \\theta^-)$: Es el valor Q calculado por la red objetivo para el próximo estado $s'$ y una posible acción $a'$, usando los parámetros $\\theta^-$.\n",
    "\n",
    "3. $r$: Es la recompensa inmediata recibida después de tomar la acción $a$ en el estado $s$.\n",
    "\n",
    "4. $\\gamma$: Es el factor de descuento que determina la importancia de las recompensas futuras.\n",
    "\n",
    "5. $\\mathcal{L}(\\theta)$: Es la función de pérdida, que mide la discrepancia entre la predicción de la red principal y el valor objetivo calculado usando la red objetivo.\n",
    "\n",
    "6. $\\mathbb{E}$: Denota la expectativa matemática, indicando que estamos tomando el promedio de la pérdida sobre muchas transiciones, generalmente tomadas de un minibatch del buffer de repetición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://karpathy.github.io/2016/05/31/rl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

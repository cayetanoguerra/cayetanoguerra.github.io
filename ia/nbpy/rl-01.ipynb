{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por refuerzo\n",
    "\n",
    "El **aprendizaje por refuerzo** agrupa al conjunto de métodos del **aprendizaje automático** en el que un **agente** aprende a tomar decisiones sobre las **acciones** a ejecutar en un **entorno** que lo llevan a maximizar una **recompensa**. Lo hace mediante la **exploración** de nuevas vías y soluciones y la **explotación** del conocimiento que adquiere mediante repetidas pruebas.\n",
    "\n",
    "<img src=\"./imgs/rl.svg\" width=60%>\n",
    "\n",
    "Se asume que el comportamiento del sistema es discreto, es decir, que está formado por una secuencia de pasos. El caso continuo se trataría con intervalos temporales. Cada paso consiste en estudiar el estado $s$ del entorno y seleccionar una acción $a$. El entorno responde con un nuevo estado $s'$ y una recompensa $r$. El comportamiento del entorno es, en general, desconocido y puede ser estocástico, es decir, que la evolución del entorno y la recompensa generada pueden obedecer a una cierta función de probabilidad.\n",
    "\n",
    "El hecho fundamental del aprendizaje por refuerzo es que la recompensa puede tener un cierto retardo. Es decir, la idoneidad de la acción tomada por el agente puede que no se refleje hasta un cierto número de pasos posteriores.\n",
    "\n",
    "Comenzaremos nuestro estudio con el algoritmo Q-Learning básico. Este algoritmo nos permitirá familiarizarnos con los conceptos fundamentales del aprendizaje por refuerzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Supongamos que tenemos un robot cuyo objetivo es aprender a salir de una casa, como la que muestra el plano de la figura siguiente. Para ello, va a realizar una serie de múltiples intentos, obteniendo **recompensa** únicamente cuando consiga salir. En cada intento el robot partirá desde alguna habitación aleatoria. A estos \"intentos\" le daremos el nombre de \"episodios\". Denominaremos **episodio** al conjunto de **acciones** que el robot toma desde que parte inicialmente de una habitación hasta que consigue salir.\n",
    "\n",
    "<img src=\"imgs/planocasa.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las puertas que dan directamente al exterior tienen una recompensa de 100. El resto de puertas no tienen recompensa. El plano de la casa puede ser visto como un grafo (figura siguiente). Cuando el robot llega al **estado** número 5 del grafo el **episodio** finaliza.\n",
    "\n",
    "<img src=\"imgs/graph.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este grafo puede ser representado también por una matriz donde las filas representan los **estados** y las columnas las **acciones** que se pueden tomar. En este caso en particular, las acciones corresponden a los estados a los que se puede ir. Así que, en este caso, la matriz es cuadrada.\n",
    "Vamos a llamar a esta matriz $R$, **matriz de recompensas**. En este caso, vamos a denotar con el valor $-1$ a una acción que no es posible ejecutar para un determinado estado. Ojo, en otro tipo de problemas el valor $-1$ puede corresponder con una recompensa negativa (o castigo).\n",
    "\n",
    "\n",
    "$$R = \\begin{pmatrix}\n",
    "-1 & -1 & -1 & -1 & 0 & -1 \\\\\n",
    "-1 & -1 & -1 & 0 & -1 & 100 \\\\\n",
    "-1 & -1 & -1 & 0 & -1 & -1 \\\\\n",
    "-1 & 0 & 0 & -1 & 0 & -1 \\\\\n",
    "0 & -1 & -1 & 0 & -1 & 100 \\\\\n",
    "-1 & 0 & -1 & -1 & 0 & 100\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Este algoritmo de aprendizaje por refuerzo almacena en una tabla el \"conocimiento\" que va adquiriendo, que llamaremos **tabla $Q$**. Inicialmente estará vacía. \n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "A medida que el robot vaya deambulando por la casa y completando episodios irá acumulando recompensas. \n",
    "\n",
    "Supongamos que el robot parte de la habitación (estado) 1. En ese momento, el robot no tiene ningún tipo de conocimiento de la casa, no sabe qué puerta elegir para llegar antes a la salida. Por supuesto, no tiene acceso a la matriz de recompensas. En esas condiciones, el robot solo puede hacer una elección aleatoria de una de las dos puertas, supongamos que elige la inferior (véase el plano de la casa). Es decir, elige la acción “ir al estado 3”. \n",
    "\n",
    "Una vez en la habitación 3 descubre que no recibe ninguna recompensa y que vuelve a encontrarse en la misma situación, ¿qué puerta elegir? Todas, incluso ir de nuevo al estado 1, son para el robot elecciones aceptables, puesto que todas le proporcionan la misma incertidumbre. De nuevo, mediante una selección totalmente aleatoria, elige la puerta izquierda, “ir al estado 4”.\n",
    "\n",
    "Ya en el estado 4 la situación se repite. De nuevo, no recibe ninguna recompensa. Otra vez de forma aleatoria, elige la puerta inferior “ir al estado 5”.\n",
    "\n",
    "Llegado al estado 5, el robot descubre que recibe 100 puntos (puntos, dinero, gallifantes... cualquier cosa vale) de recompensa. En ese momento el robot actualizará su tabla $Q$, dado que hay algo de información nueva. Actualizará, por tanto, la entrada $(4,5)$ con el valor $100$. Démonos cuenta de que $4$ representa el estado en el que se encontraba el robot y $5$ es la acción que tomó (\"ir al estado 5\"). En otras palabras significaría que debemos apuntar en una libreta que si estamos en la habitación 4 y vamos por la puerta inferior obtendremos 100 puntos de recompensa. La próxima vez que estemos en la habitación $4$, ya sabremos qué puerta elegir para obtener alguna recompensa. Ten en cuenta que no hemos explorado otras puertas de la habitación 4, luego no sabemos si esas otras puertas nos llevan a recompensas mayores.\n",
    "\n",
    "\n",
    "Finalmente, como el robot ya ha salida de la casa, el episodio termina.\n",
    "\n",
    "La tabla $Q$ actualizada será:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "Empezamos, por tanto, un nuevo episodio. Supongamos que, por una cuestión aleatoria, el robot parte de la habitación 3. Y, de nuevo, el azar nos lleva a seleccionar la puerta de la izquierda, (ir al estado 4).\n",
    "Tengo que confesarte que, por simplificar, en el episodio anterior no expliqué completamente cómo se actualiza la tabla $Q$. Ahora sí que la vamos a ir actualizando correctamente. Para ello, vamos a hacer uso de esta fórmula, denominada **ecuación de Bellman**:\n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma max[Q(s',a')]$$\n",
    "\n",
    "Significa lo siguiente, cuando el robot se encuentra en el estado $s$ y toma la acción $a$ pasa al estado $s’$. Una vez en el estado $s’$ podemos consultar la tabla $Q$ para ver qué acción $a’$ es la que tiene la recompensa máxima, $max[Q(s’,a’)]$. Por tanto, la actualización de $Q(s,a)$ se compone de dos partes. En primer lugar, la recompensa directa por haber pasado de $s$ a $s’$ mediante la acción $a$, que en este caso es $R(3,4) = 0$. Y, en segundo lugar, la recompensa máxima que se puede obtener desde $s’$ tomando la acción $a’$ adecuada. El factor $\\gamma$ debe tener un valor mayor que $0$ y menor que $1$, pongámosle $0.8$. Su cometido es rebajar proporcionalmente la recompensa que está dos pasos más allá del estado $s$. Por tanto, la nueva actualización de $Q(3,4)$ será:\n",
    "\n",
    "$$Q(3,4) = R(3,4) + \\gamma \\cdot max[Q(4,0),Q(4,3),Q(4,5)]$$\n",
    "\n",
    "Que es:\n",
    "\n",
    "$$Q(3,4) = 0 + 0.8 \\cdot max[0, 0, 100]$$\n",
    "\n",
    "Los nuevos valores de $Q$ serán:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 80 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Por tanto, ¿qué almacena la tabla $Q$? Almacena la recompensa que podría obtenerse a partir de cada estado, pero disminuida exponencialmente en función de la lejanía a la que la recompensa se encuentra. Desde un determinado estado podríamos tener dos opciones con el mismo valor de Q, yendo por un camino podríamos conseguir una recompensa pequeña pero cercana, mientras que por el otro podríamos conseguir una recompensa mucho mayor pero nos obligaría a dar muchos pasos intermedios.\n",
    "\n",
    "Aún no hemos terminado este segundo episodio. Nos encontramos en el estado 4. Si el robot consulta la tabla $Q$ puede ver que si elige la acción “ir al estado 5” obtendrá mayor recompensa que si toma cualquiera de las otras opciones, que, por el momento, están a $0$. Supongamos que elige “ir al estado 5” y el episodio termina.\n",
    "\n",
    "Comencemos con el tercer episodio. El robot parte de la habitación 1 (por azar). Si escoge la acción “ir al estado 3” deberá actualizar la tabla $Q$ de la siguiente forma:\n",
    "\n",
    "$$Q(1,3) = R(1,3) + \\gamma \\cdot max[Q(3,1), Q(3,2), Q(3,4)]$$\n",
    "\n",
    "Lo cual es:\n",
    "\n",
    "$$Q(1,3) = 64  = 0 + 0.8 \\cdot max[0, 0, 80] $$\n",
    "\n",
    "Con lo que la tabla $Q$ quedaría:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 64 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 80 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "A partir de aquí el robot podría ir utilizando la información de la tabla $Q$ para guiar su toma de decisiones, pasando del estado $3$ al $4$ y del $4$ al $5$, finalizando el tercer episodio.\n",
    "\n",
    "\n",
    "Supongamos ahora que en un cuarto episodio el robot parte de nuevo desde la habitación 1. Si se guía por la información de la tabla $Q$ podría llegar a la salida en tres pasos. Sin embargo, si elige la salida superior (por supuesto, la puerta está cerrada y no sabe a dónde lleva) llegaría a la salida en un solo paso, lo cual es mucho mejor. A medida que el robot recaba nueva información, puede reutilizarla en episodios posteriores, es decir, puede **explotar** la información que ya tiene. O, por el contrario, puede aventurarse a descubrir nuevos caminos, es decir, **explorar** nuevas vías que, en ocasiones, pueden llevarle a mucho mejores resultados.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Explotación vs. exploración\n",
    "\n",
    "El algoritmo Q-Learning debe acompasar una estrategia que combine cierta **explotación** con cierta **exploración**. Al principio, es evidente que lo único que se puede hacer es explorar, puesto que nuestra tabla $Q$ está vacía, no hay información. Pero, a medida que vamos completando episodios, debemos explotar esta información para obtener recompensas seguras.\n",
    "\n",
    "La forma de combinar exploración y explotación es lo que se denomina **política**.\n",
    "\n",
    "### Convergencia\n",
    "\n",
    "¿Cuándo termina el algoritmo Q-Learning? La primera respuesta sería: cuando la tabla $Q$ converja. Esto significa que cuando hayamos hecho los suficientes episodios, la tabla $Q$ ya no modificará más sus valores, a esta tabla la llamaremos $Q^*$. Esto ocurre fácilmente en casos como el de nuestro ejemplo. Pero en casos complejos, la tabla puede ser muy grande y sería necesario mucho tiempo (más del disponible) para que la tabla llegue a converger. Por tanto, la segunda respuesta es que no termina nunca. Siempre se estará ejecutando una determinada política que alterne, de la manera más eficiente posible, explotación y exploración.\n",
    "\n",
    "### Implementación del algoritmo\n",
    "\n",
    "Establecemos los parámetros del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "discount = 0.8 # gamma\n",
    "final_state = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la tabla de recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [[-1., -1., -1., -1., 0., -1.],\n",
    "           [-1., -1., -1., 0., -1., 100.],\n",
    "           [-1., -1., -1., 0., -1., -1.],\n",
    "           [-1., 0., 0., -1., 0., -1.],\n",
    "           [0., -1., -1., 0., -1., 100.],\n",
    "           [-1., 0., -1., -1., 0., 100.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la tabla $Q$ a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "Q = [[0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "import random\n",
    "        \n",
    "from tabulate import tabulate\n",
    "print(tabulate(Q, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fórmula de actualización de la matriz $Q$ \n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma max[Q(s',a')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(s, a):\n",
    "    Q[s][a] = rewards[s][a] + discount * max(Q[a])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   |  0 | 80 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   | 64 |  0 | 100 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   | 64 |  0 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 | 80 | 51.2 |  0 | 80 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "| 64 |  0 |  0   | 64 |  0 | 100 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   |  0 |  0 |   0 |\n",
      "+----+----+------+----+----+-----+\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "\n",
    "    s = random.randint(0, 4)\n",
    "   \n",
    "    keep = True\n",
    "    while keep:\n",
    "        a = random.randint(0, 5)\n",
    "        while rewards[s][a] == -1:\n",
    "            a = random.randint(0, 5)\n",
    "        qlearning(s, a)\n",
    "        s = a\n",
    "        if s == final_state:\n",
    "            keep = False \n",
    "            \n",
    "print(tabulate(Q, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ejercicios prácticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Actualiza la tabla $Q$ realizando a mano un episodio con el siguiente recorrido $0\\rightarrow4\\rightarrow3\\rightarrow1\\rightarrow5$.\n",
    "\n",
    "\n",
    "* Crea una función que, a partir de la matriz $Q^*$, nos lleve a la salida por el camino óptimo desde cualquier habitación.\n",
    "\n",
    "\n",
    "* ¿Qué pasaría si el factor $\\gamma$ fuera $1$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

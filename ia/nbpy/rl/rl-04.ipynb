{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "  <img src=\"./imgs/EII-ULPGC-logo.jpeg\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aprendizaje por refuerzo 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Q-Learning (DQL)**\n",
    "\n",
    "Partimos de la **ecuación de Bellman**, que define cómo actualizamos el valor $ Q $ en el algoritmo Q-Learning:\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right]$$\n",
    "\n",
    "En lugar de usar una tabla $ Q $, en **Deep Q-Learning** utilizamos una red neuronal $ \\hat{Q}_{\\theta} $, donde la entrada es el estado $ s $ y la salida son los valores $ Q $ para cada posible acción $ a $. Sin embargo, para que el entrenamiento sea estable y efectivo, necesitamos un **objetivo (target)** con el cual comparar nuestras predicciones. Para ello, utilizamos una **red neuronal adicional**, conocida como **red objetivo** $ Q_{\\theta^-} $, que genera los valores Q objetivos. Esta red objetivo se actualiza periódicamente con los parámetros de la red principal.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./imgs/deep.jpg\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "En el **Q-Learning tradicional**, la tabla Q es una matriz que almacena los valores Q para cada par de estado-acción posible. Cada celda de esta tabla representa el valor esperado de realizar una acción específica en un estado dado, es decir, qué tan buena es esa acción en ese estado en términos de la recompensa futura esperada. Este enfoque funciona bien cuando el espacio de estados y acciones es pequeño, ya que se puede mantener una tabla explícita para todas las combinaciones posibles. Sin embargo, cuando el espacio de estados crece (como en problemas con muchas variables o en entornos continuos), la tabla Q se vuelve impráctica o imposible de manejar debido a su tamaño.\n",
    "\n",
    "Aquí es donde entra en juego el **Deep Q-Learning**. En lugar de utilizar una tabla para almacenar los valores Q, se emplea una **red neuronal profunda** para aproximar la función Q. La red toma como entrada un estado y devuelve los valores Q para todas las acciones posibles. Esto tiene varias implicaciones clave:\n",
    "\n",
    "1. **Generalización sobre el espacio de estados**: En lugar de almacenar valores para cada estado específico (como en la tabla Q), la red neuronal aprende patrones a partir de los estados, permitiendo que generalice a estados que no ha visto antes. Esto es especialmente útil cuando los estados no son discretos sino continuos o muy numerosos.\n",
    "   \n",
    "2. **Reducción del almacenamiento**: En vez de mantener una tabla que puede volverse extremadamente grande, la red neuronal compacta esta información en los pesos y conexiones entre las capas de la red. Esto hace que el enfoque sea más escalable para problemas complejos.\n",
    "\n",
    "3. **Entrenamiento basado en experiencias**: La red neuronal se entrena a partir de ejemplos de pares de estado-acción y sus correspondientes valores Q (o más bien, sus aproximaciones), lo que le permite ajustar sus parámetros para aproximar mejor la función Q en todo el espacio de estados.\n",
    "\n",
    "4. **Aproximación continua**: La red neuronal puede manejar de forma natural espacios de estado continuos, donde sería imposible definir una tabla Q, ya que no se pueden enumerar todos los estados posibles.\n",
    "\n",
    "\n",
    "#### **Cálculo del Target**\n",
    "\n",
    "El objetivo, o **target**, se calcula con la siguiente fórmula:\n",
    "\n",
    "$$ y_i = r_i + \\gamma \\max_{a'} Q(s', a'; \\theta^-) $$\n",
    "\n",
    "Donde:\n",
    "- $ r_i $ es la recompensa inmediata tras tomar la acción.\n",
    "- $ \\gamma $ es el factor de descuento que pondera las recompensas futuras.\n",
    "- $ Q(s', a'; \\theta^-) $ es el valor Q para el siguiente estado $ s' $, calculado por la red objetivo.\n",
    "\n",
    "#### **Función de Pérdida (Loss)**\n",
    "\n",
    "Para entrenar la red neuronal, minimizamos la siguiente función de pérdida:\n",
    "\n",
    "$$ \\mathcal{L(\\theta)} = \\mathbb{E} \\left[ ( \\hat{Q}(s, a; \\theta_i) - y_i )^2 \\right] $$\n",
    "\n",
    "Esta pérdida mide la diferencia entre la predicción de la red principal y el valor objetivo calculado por la red objetivo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Componentes de la Ecuación de Pérdida:**\n",
    "\n",
    "1. **$ \\hat{Q}(s, a; \\theta) $**: Es el valor Q predicho por la red principal para el estado actual $ s $ y la acción $ a $, utilizando los parámetros actuales $ \\theta $.\n",
    "   \n",
    "2. **$ Q(s', a'; \\theta^-) $**: Es el valor Q calculado por la red objetivo para el siguiente estado $ s' $ y la mejor acción posible $ a' $, utilizando los parámetros $ \\theta^- $.\n",
    "   \n",
    "3. **$ r $**: Es la recompensa inmediata recibida tras tomar la acción $ a $ en el estado $ s $.\n",
    "   \n",
    "4. **$ \\gamma $**: Es el factor de descuento que controla cuánto valoramos las recompensas futuras.\n",
    "   \n",
    "5. **$ \\mathcal{L}(\\theta) $**: Es la función de pérdida que mide la discrepancia entre el valor predicho por la red principal y el valor objetivo.\n",
    "   \n",
    "6. **$ \\mathbb{E} $**: Denota la expectativa matemática, lo que indica que estamos tomando el promedio de la pérdida sobre varias transiciones. Estas transiciones generalmente se seleccionan de un **minibatch** almacenado en el **replay buffer**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Replay Buffer (Memoria de Repetición)**\n",
    "\n",
    "El **replay buffer** es un componente clave en el entrenamiento de Deep Q-Learning. Es una memoria donde el agente almacena las transiciones $ (s, a, r, s') $ experimentadas durante el juego. Este enfoque permite romper la correlación entre transiciones consecutivas, ya que durante el entrenamiento, seleccionamos un minibatch aleatorio del buffer. Esto ayuda a estabilizar el proceso de entrenamiento y mejora la convergencia del modelo.\n",
    "\n",
    "### **Actualización de la Red Objetivo**\n",
    "\n",
    "Para evitar que la red se vuelva inestable debido a constantes actualizaciones, los parámetros de la **red objetivo** $ \\theta^- $ se actualizan copiando los parámetros de la **red principal** $ \\theta $ solo cada cierto número de pasos $ N $. Esto desacopla las predicciones del futuro (calculadas por la red objetivo) de las actualizaciones constantes de la red principal, proporcionando una referencia estable durante el entrenamiento.\n",
    "\n",
    "### **Exploración vs. Explotación**\n",
    "\n",
    "El agente utiliza una estrategia de **$ \\epsilon $-greedy**, lo que significa que, en cada paso, tomará una acción aleatoria con probabilidad $ \\epsilon $ (exploración) o seleccionará la acción con el mayor valor Q (explotación). Este parámetro $ \\epsilon $ empieza siendo alto para fomentar la exploración, pero se va reduciendo con el tiempo para que el agente se enfoque en explotar las mejores acciones aprendidas.\n",
    "\n",
    "La fórmula para el decaimiento de $ \\epsilon $ es:\n",
    "\n",
    "$$\n",
    "\\epsilon_t = \\epsilon_{\\text{final}} + (\\epsilon_{\\text{initial}} - \\epsilon_{\\text{final}}) \\cdot e^{-t / \\lambda}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $ \\epsilon_t $ es el valor de exploración en el tiempo $ t $.\n",
    "- $ \\lambda $ es la tasa de decaimiento.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pong**\n",
    "\n",
    "A continución se muestra un ejemplo de implementación de un agente DQL para jugar al Pong de Atari. Puedes decargar el código completo desde [este enlace](./staff/DQN.zip).\n",
    "\n",
    "El siguiente vídeo muestra el agente en funcionamiento antes de que comience el entrenamiento:\n",
    "\n",
    "<div align=\"center\">\n",
    "<video controls>\n",
    "<source src=\"./imgs/principio_entrenamiento.mp4\">\n",
    "</video>\n",
    "</div>\n",
    "\n",
    "Y este vídeo muestra el agente después del entrenamiento:\n",
    "\n",
    "<div align=\"center\">\n",
    "<video controls>\n",
    "<source src=\"./imgs/final_entrenamiento.mp4\">\n",
    "</video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://karpathy.github.io/2016/05/31/rl/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

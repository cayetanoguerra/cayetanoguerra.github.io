{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "En el algoritmo Q-Learning, La \"Q\" de su nombre proviene de *quality* y tiene su raz√≥n de ser porque cada valor de la tabla $Q$ indica la calidad de cada acci√≥n para llegar a una recompensa futura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "\n",
    "Supongamos que tenemos un robot cuyo objetivo es aprender a salir de una casa, como la que muestra el plano de la figura siguiente. Para ello, va a realizar una serie de m√∫ltiples intentos, obteniendo recompensa √∫nicamente cuando consiga salir. En cada intento el robot partir√° desde alguna habitaci√≥n aleatoria. A estos \"intentos\" le daremos el nombre de \"episodios\". Denominaremos **episodio** al conjunto de acciones que el robot toma desde que parte inicialmente de una habitaci√≥n hasta que consigue salir.\n",
    "\n",
    "<img src=\"imgs/plano.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las puertas que dan directamente al exterior tienen una recompensa de 100. El resto de puertas no tienen recompensa. El plano de la casa puede ser visto como un grafo (figura siguiente). Cuando el robot llega al estado n√∫mero 5 del grafo el intento se da por finalizado.\n",
    "\n",
    "<img src=\"imgs/grafo.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este grafo puede ser representado tambi√©n por una matriz donde las filas representan los estados y las columnas las acciones que se pueden tomar. En este caso en particular, las acciones corresponden a los estados a los que se puede ir. As√≠ que, en este caso, la matriz es cuadrada.\n",
    "Vamos a llamar a esta matriz $R$, **matriz de recompensas**.  El valor $-1$ significa que una determinada acci√≥n no es posible para un determinado estado.\n",
    "\n",
    "\n",
    "$$R = \\begin{pmatrix}\n",
    "-1 & -1 & -1 & -1 & 0 & -1 \\\\\n",
    "-1 & -1 & -1 & 0 & -1 & 100 \\\\\n",
    "-1 & -1 & -1 & 0 & -1 & -1 \\\\\n",
    "-1 & 0 & 0 & -1 & 0 & -1 \\\\\n",
    "0 & -1 & -1 & 0 & -1 & 100 \\\\\n",
    "-1 & 0 & -1 & -1 & 0 & 100\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Este algoritmo de aprendizaje por refuerzo almacena en una tabla el conocimiento que va adquiriendo, que llamaremos **tabla $Q$**. Inicialmente estar√° vac√≠a. \n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "A medida que el robot vaya deambulando por la casa y completando episodios ir√° acumulando recompensas. \n",
    "\n",
    "Supongamos que el robot parte de la habitaci√≥n (estado) 1. En ese momento, el robot no tiene ning√∫n tipo de conocimiento de la casa, no sabe qu√© puerta elegir para llegar antes a la salida. Por supuesto, no tiene acceso a la matriz de recompensas. En esas condiciones, el robot solo puede hacer una elecci√≥n aleatoria de una de las dos puertas, supongamos que elige la inferior (v√©ase el plano de la casa). Es decir, elige la acci√≥n ‚Äúir al estado 3‚Äù. \n",
    "\n",
    "Una vez en la habitaci√≥n 3 descubre que no recibe ninguna recompensa y que vuelve a encontrarse en la misma situaci√≥n, ¬øqu√© puerta elegir? Todas, incluso ir de nuevo al estado 1, son para el robot elecciones aceptables, puesto que todas le proporcionan la misma incertidumbre. De nuevo, mediante una selecci√≥n totalmente aleatoria, selecciona la puerta izquierda, ‚Äúir al estado 4‚Äù.\n",
    "\n",
    "Ya en el estado 4 la situaci√≥n se repite. De nuevo, no recibe ninguna recompensa. Otra vez de forma aleatoria, elige la puerta inferior ‚Äúir al estado 5‚Äù.\n",
    "\n",
    "Llegado al estado 5, el robot descubre que recibe 100 puntos (puntos, dinero, gallifantes... cualquier cosa vale) de recompensa. En ese momento el robot actualizar√° su tabla $Q$, dado que hay algo de informaci√≥n nueva. Actualizar√°, por tanto, la entrada $(4,5)$ con el valor $100$. D√©monos cuenta de que $4$ representa el estado en el que se encontraba el robot y $5$ es la acci√≥n que tom√≥ (\"ir al estado 5\"). En otras palabras significar√≠a que debemos apuntar en una libreta que si estamos en la habitaci√≥n 4 y vamos por la puerta inferior obtendremos 100 puntos de recompensa. La pr√≥xima vez que estemos en la habitaci√≥n $4$, ya sabremos qu√© elegir.\n",
    "\n",
    "\n",
    "Finalmente, como el robot ya ha salida de la casa, el episodio termina.\n",
    "\n",
    "La tabla $Q$ actualizada ser√°:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "Empezamos, por tanto, un nuevo episodio. Supongamos que, por una cuesti√≥n aleatoria, el robot parte de la habitaci√≥n 3. Y, de nuevo, el azar nos lleva a seleccionar la puerta de la izquierda, (ir al estado 4).\n",
    "Tengo que confesarte que, por simplificar, en el episodio anterior no expliqu√© completamente c√≥mo se actualiza la tabla $Q$. Ahora s√≠ que la vamos a ir actualizando correctamente. Para ello, vamos a hacer uso de esta f√≥rmula, denominada **ecuaci√≥n de Bellman**:\n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma max[Q(s',a')]$$\n",
    "\n",
    "Significa lo siguiente, cuando el robot se encuentra en el estado $s$ y toma la acci√≥n $a$ pasa al estado $s‚Äô$. Una vez en el estado $s‚Äô$ podemos consultar la tabla $Q$ para ver qu√© acci√≥n $a‚Äô$ es la que tiene la recompensa m√°xima, $max[Q(s‚Äô,a‚Äô)]$. Por tanto, la actualizaci√≥n de $Q(s,a)$ se compone de dos partes. En primer lugar, la recompensa directa por haber pasado de $s$ a $s‚Äô$ mediante la acci√≥n $a$, que en este caso es $R(3,4) = 0$. Y, en segundo lugar, la recompensa m√°xima que se puede obtener desde $s‚Äô$ tomando la acci√≥n $a‚Äô$ adecuada. El factor $\\gamma$ debe tener un valor mayor que $0$ y menor que $1$, pong√°mosle $0.8$. Su cometido es rebajar proporcionalmente la recompensa que est√° dos pasos m√°s all√° del estado $s$. Por tanto, la nueva actualizaci√≥n de $Q(3,4)$ ser√°:\n",
    "\n",
    "$$Q(3,4) = R(3,4) + \\gamma \\cdot max[Q(4,0),Q(4,3),Q(4,5)]$$\n",
    "\n",
    "Que es:\n",
    "\n",
    "$$Q(3,4) = 0 + 0.8 \\cdot max[0, 0, 100]$$\n",
    "\n",
    "Los nuevos valores de $Q$ ser√°n:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 80 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Por tanto, ¬øqu√© almacena la tabla $Q$? Realmente, no almacena recompensas, sino informaci√≥n. Los valores de la tabla $Q$ nos dan una referencia sobre la cercan√≠a o lejan√≠a a la que la recompensa real est√°.\n",
    "\n",
    "A√∫n no hemos terminado este segundo episodio. Nos encontramos en el estado 4. Si el robot consulta la tabla $Q$ puede ver que si elige la acci√≥n ‚Äúir al estado 5‚Äù obtendr√° mayor recompensa que si toma cualquiera de las otras opciones, que, por el momento, est√°n a $0$. Supongamos que elige ‚Äúir al estado 5‚Äù y el episodio termina.\n",
    "\n",
    "Comencemos con el tercer episodio. El robot parte de la habitaci√≥n 1 (por azar). Si escoge la acci√≥n ‚Äúir al estado 3‚Äù deber√° actualizar la tabla $Q$ de la siguiente forma:\n",
    "\n",
    "$$Q(1,3) = R(1,3) + \\gamma \\cdot max[Q(3,1), Q(3,2), Q(3,4)]$$\n",
    "\n",
    "Lo cual es:\n",
    "\n",
    "$$Q(1,3) = 64  = 0 + 0.8 \\cdot max[0, 0, 80] $$\n",
    "\n",
    "Con lo que la tabla $Q$ quedar√≠a:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 64 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 80 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "A partir de aqu√≠ el robot podr√≠a ir utilizando la informaci√≥n de la tabla $Q$ para guiar su toma de decisiones, pasando del estado $3$ al $4$ y del $4$ al $5$, finalizando el tercer episodio.\n",
    "\n",
    "\n",
    "Supongamos ahora que en un cuarto episodio el robot parte de nuevo desde la habitaci√≥n 1. Si se gu√≠a por la informaci√≥n de la tabla $Q$ podr√≠a llegar a la salida en tres pasos. Sin embargo, si elige la salida superior (por supuesto, la puerta est√° cerrada y no sabe a d√≥nde lleva) llegar√≠a a la salida en un solo paso, lo cual es mucho mejor. A medida que el robot recaba nueva informaci√≥n, puede reutilizarla en episodios posteriores, es decir, puede **explotar** la informaci√≥n que ya tiene. O, por el contrario, puede aventurarse a descubrir nuevos caminos, es decir, **explorar** nuevas v√≠as que, en ocasiones, pueden llevarle a mucho mejores resultados.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Explotaci√≥n vs. exploraci√≥n\n",
    "\n",
    "El algoritmo Q-Learning debe acompasar una estrategia que combine cierta explotaci√≥n con cierta exploraci√≥n. Al principio, es evidente que lo √∫nico que se puede hacer es explorar, puesto que nuestra tabla ùëÑQ est√° vac√≠a, no hay informaci√≥n. Pero, a medida que vamos completando episodios, debemos explotar esta informaci√≥n para obtener recompensas seguras.\n",
    "\n",
    "Al tipo de combinaci√≥n que hagamos sobre exploraci√≥n y explotaci√≥n es lo que se denomina **pol√≠tica**.\n",
    "\n",
    "### Convergencia\n",
    "\n",
    "¬øCu√°ndo termina el algoritmo Q-Learning? La primera respuesta ser√≠a: cuando la tabla $Q$ converja. Esto significa que cuando hayamos hecho los suficientes episodios, la tabla $Q$ ya no modificar√° m√°s sus valores, a esta tabla la llamaremos $Q^*$. Esto ocurre f√°cilmente en casos como el de nuestro ejemplo. Pero en casos complejos, la tabla puede ser muy grande y ser√≠a necesario mucho tiempo (m√°s del disponible) para que la tabla llegue a converger. Por tanto, la segunda respuesta es que no termina nunca. Siempre se estar√° ejecutando una determinada pol√≠tica que alterne, de la manera m√°s eficiente posible, explotaci√≥n y exploraci√≥n.\n",
    "\n",
    "### Implementaci√≥n del algoritmo\n",
    "\n",
    "Establecemos los par√°metros del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "discount = 0.8 # gamma\n",
    "learning_rate = 0.5 # alfa\n",
    "final_state = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la tabla de recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [[-1., -1., -1., -1., 0., -1.],\n",
    "           [-1., -1., -1., 0., -1., 100.],\n",
    "           [-1., -1., -1., 0., -1., -1.],\n",
    "           [-1., 0., 0., -1., 0., -1.],\n",
    "           [0., -1., -1., 0., -1., 100.],\n",
    "           [-1., 0., -1., -1., 0., 100.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la tabla $Q$ a cero o cualquier valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "Q = [[0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "import random\n",
    "        \n",
    "from tabulate import tabulate\n",
    "print(tabulate(Q, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√≥rmula de actualizaci√≥n de la matriz $Q$ \n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma max[Q(s',a')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning1(s, a):\n",
    "    Q[s][a] = rewards[s][a] + discount * max(Q[a])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   |  0 | 80 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   | 64 |  0 | 100 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   | 64 |  0 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 | 80 | 51.2 |  0 | 80 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "| 64 |  0 |  0   | 64 |  0 | 100 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   |  0 |  0 |   0 |\n",
      "+----+----+------+----+----+-----+\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "\n",
    "    s = random.randint(0, 5)\n",
    "    while s == final_state:\n",
    "        s = random.randint(0, 5)\n",
    "\n",
    "    keep = True\n",
    "    while keep:\n",
    "        a = random.randint(0, 5)\n",
    "        while rewards[s][a] == -1:\n",
    "            a = random.randint(0, 5)\n",
    "        qlearning1(s, a)\n",
    "        s = a\n",
    "        if s == final_state:\n",
    "            keep = False \n",
    "            \n",
    "print(tabulate(Q, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ejercicios pr√°cticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Actualiza la tabla $Q$ realizando a mano un episodio con el siguiente recorrido $0\\rightarrow4\\rightarrow3\\rightarrow1\\rightarrow5$.\n",
    "\n",
    "\n",
    "* Crea una funci√≥n que, a partir de la matriz $Q^*$, nos lleve a la salida por el camino √≥ptimo desde cualquier habitaci√≥n.\n",
    "\n",
    "\n",
    "* ¬øQu√© pasar√≠a si el factor $\\gamma$ fuera $1$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

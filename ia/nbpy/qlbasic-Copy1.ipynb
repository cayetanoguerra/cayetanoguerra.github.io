{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "\n",
    "Supongamos que tenemos un robot cuyo objetivo es aprender a salir de una casa, como la que muestra el plano de la figura siguiente. Para ello, va a realizar una serie de múltiples intentos, obteniendo recompensa únicamente cuando consiga salir. En cada intento el robot partirá desde alguna habitación aleatoria. Denominaremos “intento” al conjunto de acciones que el robot toma desde que parte de una habitación hasta que consigue salir.\n",
    "\n",
    "<img src=\"imgs/plano.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las puertas que dan directamente al exterior tienen una recompensa de 100. El resto de puertas no tienen recompensa. El plano de la casa puede ser visto como un grafo (figura siguiente). Cuando el robot llega al estado número 5 del grafo el intento se da por finalizado.\n",
    "\n",
    "<img src=\"imgs/grafo.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este grafo puede ser representado también por una matriz donde las filas representan los estados y las columnas las acciones que se pueden tomar. En este caso en particular, las acciones corresponden a los estados a los que se puede ir. Así que, en este caso, la matriz es cuadrada.\n",
    "Vamos a llamar a esta matriz $R$, **matriz de recompensas**.  El valor $-1$ significa que una determinada acción no es posible para un determinado estado.\n",
    "\n",
    "\n",
    "$$R = \\begin{pmatrix}\n",
    "-1 & -1 & -1 & -1 & 0 & -1 \\\\\n",
    "-1 & -1 & -1 & 0 & -1 & 100 \\\\\n",
    "-1 & -1 & -1 & 0 & -1 & -1 \\\\\n",
    "-1 & 0 & 0 & -1 & 0 & -1 \\\\\n",
    "0 & -1 & -1 & 0 & -1 & 100 \\\\\n",
    "-1 & 0 & -1 & -1 & 0 & 100\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Este algoritmo de aprendizaje por refuerzo almacena en una tabla el conocimiento que va adquiriendo, que llamaremos **tabla $Q$**.\n",
    "\n",
    "$Q$ es nuestra tabla de conocimiento del entorno. Inicialmente estará vacía. Ten presente que comenzamos los índices de las tablas $R$ y $Q$ en $0$, no en $1$.\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Representa el valor que tiene un par (estado, acción). Su actualización la haremos con la siguiente fórmula:\n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma max[Q(s',a')]$$\n",
    "\n",
    "Supongamos que comenzamos en la habitación número 1 y, por azar, seleccionamos la acción \"ir a 5\". La actualización de $Q$ será:\n",
    "\n",
    "$$Q(1,5) = R(1,5) + \\gamma max[Q(5,1),Q(5,4),Q(5,5)]$$\n",
    "\n",
    "El parámetro $\\gamma$ es $0.8$. Por tanto, el nuevo valor de la celda (1,5) será:\n",
    "\n",
    "$$Q(1,5) = 100 + 0.8 \\times max[0,0,0] = 100$$\n",
    "\n",
    "Nuestra tabla $Q$ se actualizará de esta forma:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Como el siguiente estado es el 5, hemos llegado al final del episodio. Supongamos ahora que partimos del estado 3. Tenemos tres posibles acciones a realizar: ir al estado 1, al 2 o al 4. Supongamos que, también por azar, seleccionamos ir al estado 1.\n",
    "\n",
    "$$Q(3,1) = R(3,1) + \\gamma max[Q(1,3),Q(1,5)]$$\n",
    "\n",
    "Cuyos valores son:\n",
    "\n",
    "$$Q(3,1) = 0 + 0.8 \\times max[0,100] = 80$$\n",
    "\n",
    "Nuestra tabla $Q$ queda:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 80 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Hemos llegado de nuevo al estado 5, así que terminamos el episodio y empezaríamos uno nuevo. Si continuamos durante más episodios veremos que la tabla $Q$ se irá actualizando hasta llegar a converger en la matriz $Q^*$. Alcanzada esta convergencia, diremos que nuestro agente ha aprendido a desenvolverse por este entorno.\n",
    "\n",
    "### Implementación del algoritmo\n",
    "\n",
    "Establecemos los parámetros del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "discount = 0.8 # gamma\n",
    "learning_rate = 0.5 # alfa\n",
    "final_state = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la tabla de recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = [[-1., -1., -1., -1., 0., -1.],\n",
    "           [-1., -1., -1., 0., -1., 100.],\n",
    "           [-1., -1., -1., 0., -1., -1.],\n",
    "           [-1., 0., 0., -1., 0., -1.],\n",
    "           [0., -1., -1., 0., -1., 100.],\n",
    "           [-1., 0., -1., -1., 0., 100.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la tabla $Q$ a cero o cualquier valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "Q = [[0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "import random\n",
    "#for j in xrange(len(rewards)):\n",
    "#    for i in xrange(len(rewards[0])):\n",
    "#        Q[j][i] = round(random.random()*100, 2)\n",
    "        \n",
    "from tabulate import tabulate\n",
    "print tabulate(Q, tablefmt=\"grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fórmula de actualización de la matriz $Q$ \n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma max[Q(s',a')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qlearning1(s, a):\n",
    "    Q[s][a] = rewards[s][a] + discount * max(Q[a])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(s,a) = \\alpha Q(s,a) + (1-\\alpha)[R(s,a) + \\gamma max[Q(s',a')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qlearning2(s, a):\n",
    "    Q[s][a] = learning_rate * Q[s][a] + (1.0-learning_rate) * (rewards[s][a] + discount * max(Q[a]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   |  0 | 80 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   | 64 |  0 | 100 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   | 64 |  0 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 | 80 | 51.2 |  0 | 80 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "| 64 |  0 |  0   | 64 |  0 | 100 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   |  0 |  0 |   0 |\n",
      "+----+----+------+----+----+-----+\n"
     ]
    }
   ],
   "source": [
    "for _ in xrange(100):\n",
    "\n",
    "    s = random.randint(0, 5)\n",
    "    while s == final_state:\n",
    "        s = random.randint(0, 5)\n",
    "\n",
    "    keep = True\n",
    "    while keep:\n",
    "        a = random.randint(0, 5)\n",
    "        while rewards[s][a] == -1:\n",
    "            a = random.randint(0, 5)\n",
    "        qlearning1(s, a)\n",
    "        s = a\n",
    "        if s == final_state:\n",
    "            keep = False \n",
    "            \n",
    "print tabulate(Q, tablefmt=\"grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ejercicios prácticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Modifica el código para imprimir los estados por donde se pasa y los valores de las celdas de $Q$ que se modifican durante el aprendizaje.\n",
    "* Actualiza la tabla $Q$ realizando a mano un episodio con el siguiente recorrido $0\\rightarrow4\\rightarrow3\\rightarrow1\\rightarrow5$.\n",
    "* Sustituye la primera fórmula del cálculo de $Q$ por la segunda y describe qué diferencias observas.\n",
    "* ¿Qué ventajas o desventajas ofrece la primera fórmula de cálculo de $Q$ con respecto a la segunda?\n",
    "* Crea una función que, a partir de la matriz $Q^*$, nos lleve a la salida por el camino óptimo desde cualquier habitación.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

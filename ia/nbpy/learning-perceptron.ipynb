{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales artificiales (RNA) constituyen un paradigma de computación inspirado en las <a href=\"https://es.wikipedia.org/wiki/Neurona\">neuronas</a> biológicas y su interconexión. Las neuronas bilógicas son células compuestas principalmente de tres partes: soma (cuerpo celular), dendritas (canales de entrada) y axón (canal de salida). Descrito de una forma muy simplificada, las neuronas transmiten información a través de procesos electroquímicos. Cuando una neurona recibe, a través de las denritas, una cantidad de estímulos mayor a un cierto umbral, ésta se despolariza excitando, a través del axón, a otras neuronas próximas conectadas a través de las sinapsis.\n",
    "\n",
    "<img src=\"imgs/neurona.jpg\" width=\"70%\">\n",
    "\n",
    "\n",
    "Inspirados por esta idea se concibió el modelo de <a href=\"https://es.wikipedia.org/wiki/Neurona_de_McCulloch-Pitts\">neurona artificial</a>. Fundamentalmente, consiste en una unidad de cálculo que admite entradas $\\vec{e}$ que suma de forma ponderada por unos pesos $\\vec{w}$ y, si esta suma supera un cierto umbral $\\theta$, genera un cierto valor de salida. A su vez, esta salida puede ser la entrada a una función no lineal $g(x)$ que generará el valor de salida final de la neurona. En este caso decimos que la neurona se activa.\n",
    "\n",
    "<img src=\"imgs/model.svg\" width=\"70%\">\n",
    "\n",
    "Una expresión común de la neurona artificial es la siguiente:\n",
    "\n",
    "$$\n",
    "f(\\textbf{e}) = \\begin{cases} \\textrm{1, si} \\sum_{i=1}^{n} {w_i  e_i} - \\theta \\geq 0 \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "Donde la función no lineal $g(x)$ tiene la forma \"1 si $x\\geq 0$ y $0$ si $x<0$\". En este caso $x=\\sum_{i=1}^{n} {w_i  e_i} - \\theta$. Si estudiamos bien esta fórmula vemos que se trata de un discriminador lineal. \n",
    "\n",
    "Supongamos que tenemos un conjunto de puntos ${a,b,c,d,e}$ en un espacio $R^2$ tal como muestra la figura.\n",
    "\n",
    "<img src=\"imgs/espacios.svg\" width=\"50%\">\n",
    "\n",
    "Algunos de ellos ($a,b,c$) pertencen a una clase (clase 1) y los otros a otra (clase 2). Estas dos regiones están delimitadas por una recta. Nótese que la recta que separa ambas clases no es única, puede ser cualquiera que satisfaga la condición de separación de las clases. Por tanto, tenemos la función de una recta con la ecuación genérica:\n",
    "$$\n",
    "y = mx+b \n",
    "$$\n",
    "\n",
    "Podemos concretar esta recta como la recta del ejemplo:\n",
    "$$\n",
    " y = \\frac{1}{2} x +1 \n",
    "$$\n",
    "\n",
    "Esta recta corresponde al conjunto de todos los puntos $(x,y)$ que satisfacen la ecuación. Por ejemplo, el punto $a(2,2)$. Vemos que los puntos $b$,$c$,$d$ y $e$ no satisfacen la ecuación. Sin embargo, los puntos $a$,$b$ y $c$ sí satisfarían la inecuación:\n",
    "\n",
    "$$\n",
    "\ty \\geq \\frac{1}{2} x +1 \n",
    "$$\n",
    "\n",
    "Operando un poco sobre esta inecuación tendríamos:\n",
    "$$\n",
    "\t-\\frac{1}{2} x + y \\geq 1 \n",
    "$$\n",
    "\n",
    "Y cambiando la nomenclatura. Es decir, cambiando $x$ por $e_{1}$ e $y$ por $e_{2}$ tenemos:\n",
    "$$\n",
    "\t-\\frac{1}{2} e_{1} + e_{2} \\geq 1 \n",
    "$$\n",
    "\n",
    "Con lo cual obtenemos que $w_1 = -\\frac{1}{2}$, $w_2 = 1$ y $\\theta=1$, que es, justamente, la neurona que actuaría de discriminador lineal de nuestro ejemplo.\n",
    "\n",
    "El verdadero potencial de la neuronal artificial no está en calcular a mano sus pesos y umbral sino en dejar que ella misma \"aprenda\" esos valores. Para ello en necesario contar con un conjunto de muestras de ambos clases e iniciar el proceso de aprendizaje.\n",
    "\n",
    "\n",
    "## Aprendizaje\n",
    "\n",
    "Para llevar a cabo el aprendizaje vamos a utilizar la función sigmoide como función de activación ya que ofrece una venjata importante: es derivable.\n",
    "\n",
    "La función sigmoide tiene la siguiente forma: \n",
    "\n",
    "$$Sig(x)=\\frac{ 1 }{1+{ e }^{ -x }}$$ \n",
    "\n",
    "Su derivada es:\n",
    "\n",
    "$$Sig'(x)=\\frac { 1 }{ (1+e^{ x })} -\\frac { 1 }{ (1+e^{ x })^{ 2 }  } $$\n",
    "\n",
    "$$Sig'(x)=\\frac { 1 }{ (1+e^{ x }) } \\left[ 1-\\frac { 1 }{ (1+e^{ x }) }  \\right] =\\frac { 1 }{ (1+e^{ -x }) } \\left[ 1-\\frac { 1 }{ (1+e^{ -x }) }  \\right] =Sig(x)\\cdot \\left[ 1-Sig(x) \\right] $$\n",
    "\n",
    "\n",
    "\n",
    "La función del perceptrón tendrá la forma, para un determinado $\\vec{w}$\n",
    "\n",
    "<img src=\"imgs/perceptron.svg\" width=\"60%\">\n",
    "\n",
    "$$h_{\\vec{w}}(\\vec{e}) = Sig(\\sum_{ i=0 }^{ n } w_i e_i)$$\n",
    "\n",
    "\n",
    "donde $n$ es el número de componentes del vector $\\vec{e}$. La salida de $h_{\\vec{w}}$ estará ahora comprendida en el intervalo real $(0,1)$. Definimos el error $J$ en función de un conjunto de pesos $\\vec{w}$ de la siguiente forma:\n",
    "\n",
    "$$J(\\vec{w}) = \\sum _{ i=1 }^{ m } (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)})^2$$\n",
    "\n",
    "Donde $m$ es el número de muestras o cardinal del conjunto $E$. \n",
    "\n",
    "El nuevo conjunto de pesos $\\vec{w}$ será actualizado de la siguiente forma\n",
    "\n",
    "$$\\vec{w}_{t+1}  := \\vec{w}_t - \\gamma  \\frac{\\partial{J(\\vec{w})}}{\\partial{\\vec{w}}}$$\n",
    "\n",
    "La constante $\\gamma$ se define como \"ritmo de aprendizaje\". Su derivada parcial con respecto a cada componente de $\\vec{w}$ será:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J(\\vec{w})}{\\partial w_j} = \\frac{\\partial}{\\partial w_j}\\sum_{ i=1 }^{ m }  (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)})^2 =$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\frac{\\partial}{\\partial w_j} (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\frac{\\partial}{\\partial w_j} Sig(\\textbf{e}^{(i)} \\cdot \\vec{w}) =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\; Sig' (\\textbf{e}^{(i)} \\cdot \\vec{w}) \\frac{\\partial}{\\partial w_j} \\textbf{e}^{(i)} \\cdot \\vec{w} =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\; Sig' (\\textbf{e}^{(i)} \\cdot \\vec{w}) \\frac{\\partial}{\\partial w_j} \\sum_{k=0}^n e^{(i)}_{k} w_k =\n",
    "$$\n",
    "\n",
    "$$\n",
    "2 \\sum_{ i=1 }^{ m }  (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\; Sig' (\\textbf{e}^{(i)} \\cdot \\vec{w}) e^{(i)}_{j} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [[0, 0],\n",
    "          [10, 0],\n",
    "          [0, 10],\n",
    "          [10, 10]]\n",
    "\n",
    "y_data = [1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivate(o):\n",
    "    return o * (1.0 - o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "[0, 0] -> 1\n",
      "0.98366122091\n",
      "-----------------------\n",
      "[10, 0] -> 1\n",
      "0.997805415877\n",
      "-----------------------\n",
      "[0, 10] -> 0\n",
      "0.000294994115965\n",
      "-----------------------\n",
      "[10, 10] -> 0\n",
      "0.00222352706624\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def train(x_data, y_data):\n",
    "\n",
    "    w0, w1, w2 = np.random.rand(3)\n",
    "    lr = 0.1\n",
    "    epochs = 10000\n",
    "\n",
    "    print \"Training...\"\n",
    "\n",
    "    for _ in xrange(epochs):\n",
    "        \n",
    "        w0_d = []\n",
    "        w1_d = []\n",
    "        w2_d = []\n",
    "        \n",
    "        for data, label in zip(x_data, y_data):\n",
    "\n",
    "            o = sigmoid(w0*1.0 + w1*data[0] + w2*data[1])\n",
    "            error = 2.*(o - label) * sigmoid_derivate(o)\n",
    "\n",
    "            w0_d.append(error * 1.0)\n",
    "            w1_d.append(error * data[0])\n",
    "            w2_d.append(error * data[1])\n",
    "            \n",
    "        w0 = w0 - np.sum(w0_d) * lr\n",
    "        w1 = w1 - np.sum(w1_d) * lr\n",
    "        w2 = w2 - np.sum(w2_d) * lr\n",
    "        \n",
    "        \n",
    "    for data, label in zip(x_data, y_data):\n",
    "        print data, \"->\", label\n",
    "        o = sigmoid(w0*1.0 + w1*data[0] + w2*data[1])\n",
    "        print o\n",
    "        print \"-----------------------\"\n",
    "\n",
    "\n",
    "train(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

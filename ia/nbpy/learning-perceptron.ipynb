{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de aprendizaje del perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales artificiales (RNA) constituyen un paradigma de computación inspirado en las neuronas biológicas y su interconexión. Se incluyen dentro de los métodos de aprendizaje supervisado.\n",
    "\n",
    "<img src=\"imgs/neurona.jpg\" width=\"70%\">\n",
    "\n",
    "La función de clasificación tendrá la forma:\n",
    "\n",
    "$$\n",
    "f(\\textbf{e}) = \\sum_{i=1}^{n} {w_i  e_i} - \\theta \\geq 0\n",
    "$$\n",
    "\n",
    "Donde $\\theta$ será un valor umbral y $\\textbf{w}$ un conjunto de pesos, ambos a definir en el problema. En la figura \\ref{fig_espacios} los puntos $a$, $b$ y $c$ pertenecen a la clase 1, mientras que los puntos $d$ y $e$ pertecen a la clase 2 con el perceptrón de la figura \n",
    "\n",
    "## Perceptrón\n",
    "\n",
    "Se conoce como \"Perceptrón\" al modelo matemático de la neurona biológica cuya función es la de actuar como discriminador lineal en un espacio $R^n$. Esto significa que, dado un conjunto de puntos $E = \\{ \\textbf{e}^{(1)},  \\textbf{e}^{(2)},  \\cdots, \\textbf{e}^{(m)}  \\}$   en ese espacio, etiquetará cada punto como perteneciente a una clase u otra, siempre y cuando el conjunto $E$ sea linealmente separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función sigmoide\n",
    "\n",
    "La función sigmoide es la que usaremos como función de activación de cada neurona. \n",
    "\n",
    "$$Sig(x)=\\frac{ 1 }{1+{ e }^{ -x }}$$ \n",
    "\n",
    "Su derivada es:\n",
    "\n",
    "$$Sig'(x)=\\frac { 1 }{ (1+e^{ x })} -\\frac { 1 }{ (1+e^{ x })^{ 2 }  } $$\n",
    "\n",
    "$$Sig'(x)=\\frac { 1 }{ (1+e^{ x }) } \\left[ 1-\\frac { 1 }{ (1+e^{ x }) }  \\right] =\\frac { 1 }{ (1+e^{ -x }) } \\left[ 1-\\frac { 1 }{ (1+e^{ -x }) }  \\right] =Sig(x)\\cdot \\left[ 1-Sig(x) \\right] $$\n",
    "\n",
    "## Aprendizaje\n",
    "\n",
    "La función del perceptrón tendrá la forma, para un determinado $\\vec{w}$\n",
    "\n",
    "<img src=\"imgs/perceptronlearning.svg\" width=\"60%\">\n",
    "\n",
    "$$h_{\\vec{w}}(\\vec{e}) = Sig(\\sum_{ i=0 }^{ n } w_i e_i)$$\n",
    "\n",
    "\n",
    "donde $n$ es el número de componentes del vector $\\vec{e}$. La salida de $h_{\\vec{w}}$ estará ahora comprendida en el intervalo real $(0,1)$. Definimos el error $J$ en función de un conjunto de pesos $\\vec{w}$ de la siguiente forma:\n",
    "\n",
    "$$J(\\vec{w}) = \\sum _{ i=1 }^{ m } (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)})^2$$\n",
    "\n",
    "Donde $m$ es el número de muestras o cardinal del conjunto $E$. \n",
    "\n",
    "El nuevo conjunto de pesos $\\vec{w}$ será actualizado de la siguiente forma\n",
    "\n",
    "$$\\vec{w}_{t+1}  := \\vec{w}_t - \\gamma  \\frac{\\partial{J(\\vec{w})}}{\\partial{\\vec{w}}}$$\n",
    "\n",
    "La constante $\\gamma$ se define como \"ritmo de aprendizaje\". Su derivada parcial con respecto a cada componente de $\\vec{w}$ será:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J(\\vec{w})}{\\partial w_j} = \\frac{\\partial}{\\partial w_j}\\sum_{ i=1 }^{ m }  (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)})^2 $$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\frac{\\partial}{\\partial w_j} (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\frac{\\partial}{\\partial w_j} Sig(\\textbf{e}^{(i)} \\cdot \\vec{w})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\; Sig' (\\textbf{e}^{(i)} \\cdot \\vec{w}) \\frac{\\partial}{\\partial w_j} \\textbf{e}^{(i)} \\cdot \\vec{w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{ i=1 }^{ m }  2(h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\; Sig' (\\textbf{e}^{(i)} \\cdot \\vec{w}) \\frac{\\partial}{\\partial w_j} \\sum_{k=0}^n e^{(i)}_{k} w_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "2 \\sum_{ i=1 }^{ m }  (h_{\\vec{w}}(\\textbf{e}^{(i)}) - l^{(i)}) \\; Sig' (\\textbf{e}^{(i)} \\cdot \\vec{w}) e^{(i)}_{j} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [[0, 0],\n",
    "          [10, 0],\n",
    "          [0, 10],\n",
    "          [10, 10]]\n",
    "\n",
    "y_data = [1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivate(o):\n",
    "    return o * (1.0 - o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "[0, 0] -> 1\n",
      "0.98366122091\n",
      "-----------------------\n",
      "[10, 0] -> 1\n",
      "0.997805415877\n",
      "-----------------------\n",
      "[0, 10] -> 0\n",
      "0.000294994115965\n",
      "-----------------------\n",
      "[10, 10] -> 0\n",
      "0.00222352706624\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def train(x_data, y_data):\n",
    "\n",
    "    w0, w1, w2 = np.random.rand(3)\n",
    "    lr = 0.1\n",
    "    epochs = 10000\n",
    "\n",
    "    print \"Training...\"\n",
    "\n",
    "    for _ in xrange(epochs):\n",
    "        \n",
    "        w0_d = []\n",
    "        w1_d = []\n",
    "        w2_d = []\n",
    "        \n",
    "        for data, label in zip(x_data, y_data):\n",
    "\n",
    "            o = sigmoid(w0*1.0 + w1*data[0] + w2*data[1])\n",
    "            error = 2.*(o - label) * sigmoid_derivate(o)\n",
    "\n",
    "            w0_d.append(error * 1.0)\n",
    "            w1_d.append(error * data[0])\n",
    "            w2_d.append(error * data[1])\n",
    "            \n",
    "        w0 = w0 - np.sum(w0_d) * lr\n",
    "        w1 = w1 - np.sum(w1_d) * lr\n",
    "        w2 = w2 - np.sum(w2_d) * lr\n",
    "        \n",
    "        \n",
    "    for data, label in zip(x_data, y_data):\n",
    "        print data, \"->\", label\n",
    "        o = sigmoid(w0*1.0 + w1*data[0] + w2*data[1])\n",
    "        print o\n",
    "        print \"-----------------------\"\n",
    "\n",
    "\n",
    "train(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

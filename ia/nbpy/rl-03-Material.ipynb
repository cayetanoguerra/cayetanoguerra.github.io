{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por refuerzo\n",
    "\n",
    "Hemos visto ya cómo funciona el algoritmo **Q-Learning** en su versión simplificada. Ahora dotaremos de mayor formalismo al aprendizaje por refuerzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propiedad de Markov\n",
    "\n",
    "Consiste en que la evolución de un sistema dependa exclusivamente de su estado y de la acción realizada. Es decir, su evolución no depende de los estados anteriores ni de las acciones anteriores. Se dice que el sistema \"no tiene memoria\". Matemáticamente se expresaría así:\n",
    "\n",
    "\n",
    "$$ P[s_{t+1} | s_t] = P[s_{t+1} | s_t, s_{t-1}, \\cdots ,s_1 ] $$\n",
    "\n",
    "> El futuro depende únicamente del presente, no del pasado.\n",
    "\n",
    "Por tanto, un sistema con la **propiedad de Markov** puede ser representado mediante una serie de estados conectados con ciertas probabilidades de transición entre un estado y otro. Por ejemplo, el tiempo atmosférico de día a día podría ser simulado de esta forma:\n",
    "\n",
    "<img src=\"./imgs/markov.svg\" width=50%>\n",
    "\n",
    "Esto quiere decir, por ejemplo, que si el martes está nublado hay un 10% de probabilidades de que el miércoles esté soleado. Las probabiliades las podemos representar también mediante una **tabla de probabilidades de transición**, donde la suma de cada fila debe ser 1.\n",
    "\n",
    "|          | soleado | nublado | lluvioso |\n",
    "|----------|---------|---------|----------|\n",
    "| **soleado**  | 0\\.6    | 0\\.3    | 0\\.1     |\n",
    "| **nublado**  | 0\\.1    | 0\\.5    | 0\\.4     |\n",
    "| **lluvioso** | 0\\.6    | 0\\.2    | 0\\.2     |\n",
    "\n",
    "\n",
    "Esto puede generar cadenas de estados, como por ejemplo \"$soleado \\rightarrow nublado \\rightarrow nublado \\rightarrow lluvioso$\" con una cierta probabilidad de ocurrencia. A estas cadenas las denominamos **cadenas de Markov**.\n",
    "\n",
    "\n",
    "## Recompensa y retorno\n",
    "\n",
    "Las recompensas son los valores numéricos que recibe el agente al realizar alguna acción en algunos estados del entorno. El valor numérico puede ser positivo o negativo en función de las acciones del agente. En el aprendizaje por refuerzo, nos preocupamos por maximizar la recompensa acumulada (todas las recompensas que el agente recibe del entorno) en lugar de las recompensas que recibe en el estado actual (también llamada recompensa inmediata). Esta suma total de recompensas que el agente recibe del entorno los denominaremos **retorno**.\n",
    "\n",
    "Por lo tanto, definiremos **retorno** como:\n",
    "\n",
    "$$ G_t = r_{t+1} + r_{t+2} + \\cdots + r_{T} $$\n",
    "\n",
    "donde $r_{t+1}$ es la recompensa recibida por el agente e $t[0]$ al realizar la acción $a$. $T$ es el instante final. El objetivo del aprendizaje por refuerzo es maximizar el retorno esperado.\n",
    "\n",
    "\n",
    "### Retorno con descuento\n",
    "\n",
    "El **factor de descuento $\\gamma$** determina cuánta importancia se debe dar a la recompensa inmediata y cuánta a las recompensas futuras. Básicamente, esto nos ayuda a evitar un valor de recompensa infinito en tareas continuas. Debe tener un valor mayor que 0 y menor que 1. Un valor cercano al 0 significa que se le da más importancia a la recompensa inmediata y un valor cercano a 1 significa que se le da más importancia a las recompensas futuras. Por lo tanto, los valores comunes para el factor de descuento se encuentran entre 0.2 y 0.8. Por tanto, definimos el **retorno** con **factor de descuento** como: \n",
    "\n",
    "$$ G_t = r_{t+1} + \\gamma \\cdot r_{t+2} + \\gamma^2 \\cdot r_{t+3} + \\cdots$$\n",
    "\n",
    "Supongamos un sistema trivial formado por un solo estado $s$ cuya probabilidad de pasar del estado $s$ a sí mismo es del 100%. La recompensa por hacer esa transición es de, por ejemplo, 27. \n",
    "\n",
    "<img src=\"./imgs/unestado.svg\" width=20%>\n",
    "\n",
    "¿Cuál será la recompensa total acumulada (**retorno**) al cabo de $n$ transiciones? Pues será la recompensa de ese momento (27) más un porcentaje ($\\gamma$) de las futuras recompensas. Si suponemos que $\\gamma$ vale 0.8, la recompensa total acumulada en 40 pasos será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno: 134.9820554220569\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "r = 27\n",
    "gamma = 0.8\n",
    "G = 0\n",
    "hist = []\n",
    "steps = 40\n",
    "\n",
    "for step in range(steps):\n",
    "    G = G + gamma**step * r\n",
    "    hist.append(G)\n",
    "    \n",
    "print(\"Retorno:\", G)\n",
    "plt.plot(hist)\n",
    "plt.xlabel(\"Número de pasos\")\n",
    "plt.ylabel(\"Retorno\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que el factor $\\gamma$ hace que el retorno converja a un valor concreto. ¿Qué pasaría si el valor de $\\gamma$ fuera $1.1$? Pruébalo.\n",
    "\n",
    "Este proceso iterativo pretende calcular el valor $V$ del único estado $s$ al cabo de $n$ iteraciones, lo cual lo podemos expresar como:\n",
    "\n",
    "$$\n",
    "V_{t+1}(s) = R + \\gamma \\cdot V_{t}(s)\n",
    "$$\n",
    "\n",
    "Pero, podríamos llegar mucho antes al valor exacto con un simple cálculo:\n",
    "\n",
    "\n",
    "$$V = 27 + 0.8 \\cdot V$$\n",
    "\n",
    "$$V(1-0.8) = 27 $$\n",
    "\n",
    "$$V= \\frac{27}{0.2} = 135 $$\n",
    "\n",
    "\n",
    "### Problema episódico\n",
    "\n",
    "Problema en el que existe un estado final. Por ejemplo, un juego en el que se gana o se pierde. Un **episodio** consiste en una secuencia de pasos desde un estado inicial a un estado final.\n",
    "\n",
    "\n",
    "### Problema continuo\n",
    "\n",
    "Problema en el que no existe un estado final. El agente continúa su funcionamiento de forma ilimitada $(T \\rightarrow \\infty)$.\n",
    "\n",
    "Para unificar el tratamiento de los problemas continuos y episódicos se asume que a partir del estado final, todas las transiciones conducen al estado final y tienen recompensa 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno medio para el estado 0 : 90.95110629090061\n",
      "Retorno medio para el estado 1 : 108.20844548983852\n",
      "Retorno medio para el estado 2 : 57.60614596515345\n",
      "Retorno medio para el estado 3 : 57.74389486364507\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matriz de transiciones\n",
    "P = [[0,0.98,0.01,0.01], # a\n",
    "      [0.95,0,0.05,0],  # b\n",
    "      [0,0,0,1.0], # c\n",
    "      [0.1,0,0,0.9]] # d\n",
    "\n",
    "P = np.array(P)\n",
    "\n",
    "# Vector de recompensas\n",
    "R = [5,37,11,9]\n",
    "\n",
    "\n",
    "states = [0,1,2,3]\n",
    "steps = 50\n",
    "episodes = 2000\n",
    "gamma = 0.8\n",
    "\n",
    "for s in states:\n",
    "    GE = 0\n",
    "    hist = []\n",
    "    for episode in range(episodes):\n",
    "        G = 0\n",
    "        state = s\n",
    "        for step in range(steps):\n",
    "            new_state = np.random.choice(states,p=P[state])\n",
    "            G += gamma**step * R[state]\n",
    "            state = new_state\n",
    "        GE += G\n",
    "        hist.append(GE/(episode+1))\n",
    "\n",
    "    print(\"Retorno medio para el estado\",s,\":\", hist[-1])\n",
    "    #plt.plot(hist)\n",
    "    #plt.xlabel(\"Número de episodios\")\n",
    "    #plt.ylabel(\"Retorno medio\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceso de recompensa de Markov\n",
    "\n",
    "Hasta ahora hemos visto cómo las **cadenas de Markov** definen la dinámica de un entorno utilizando un conjunto de estados $S$ y una **matriz de probabilidad de transición** $P$. Pero sabemos que el **aprendizaje por refuerzo** tiene que ver con el objetivo de maximizar la recompensa, así que añadamos recompensas a nuestra cadena de Markov. Esto nos da el **proceso de recompensa de Markov**.\n",
    "\n",
    "$$ R_s = \\mathbb E[R_{t+1} | S_t] $$\n",
    "\n",
    "\n",
    "Esta fórmula quiere decir que el valor de una determinada recompensa viene dada por la [esperanza matemática](https://es.wikipedia.org/wiki/Esperanza_matem%C3%A1tica). $R_s$ será el retorno promedio ponderado por la probabilidad de cada transición del estado. \n",
    "\n",
    "Supongamos que tenemos un estado $s$ del que parten acciones para ir a los estados $s_1$, $s_2$ y $s_3$, con las respectivas probabilidades $p_1$, $p_2$ y $p_3$. Las recompensas asociadas a estas transiciones son $r_1$, $r_2$ y $r_3$. Por lo tanto, el valor de recompensa esperado es:\n",
    "\n",
    "\n",
    "$$ R_s = \\sum_{i=1}^3 p_i \\cdot r_i = p_1r_1 + p_2r_2 + p_3r_3 = \\mathbb E[r]$$\n",
    "\n",
    "\n",
    "Matemáticamente, definimos el Proceso de Recompensa de Markov como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}v_1 \\\\ \\vdots \\\\v_n  \\end{bmatrix} \\leftarrow \\begin{bmatrix}r_1 \\\\ \\vdots \\\\r_n  \\end{bmatrix} + \\gamma \n",
    "\\begin{bmatrix}\n",
    "p_{1,1} & \\cdots & p_{1,n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "p_{n,1} & \\cdots & p_{n,n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}v_1 \\\\ \\vdots \\\\v_n  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Esto también lo podríamos expresar como:\n",
    "\n",
    "$$\n",
    "V_{t+1}(s) \\leftarrow R + \\gamma \\cdot \\mathbb E[ V_t(S_{t+1} | S_t=s)] \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V = [ 90.80343965 108.29467228  57.40289979  58.02570534]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matriz de probabilidades de transición\n",
    "P = [[0,0.98,0.01,0.01], # a\n",
    "      [0.95,0,0.05,0],  # b\n",
    "      [0,0,0,1.0], # c\n",
    "      [0.1,0,0,0.9]] # d\n",
    "\n",
    "P = np.array(P)\n",
    "\n",
    "# Vector de recompensas\n",
    "R = np.array([5,37,11,9])\n",
    "\n",
    "states = [0,1,2,3]\n",
    "gamma = 0.8\n",
    "\n",
    "V = np.zeros(len(states), dtype=float)\n",
    "\n",
    "while True:\n",
    "    V_new =  R + gamma * np.dot(P,V)\n",
    "    conv = np.abs(V - V_new).sum()\n",
    "    # print(\"Convergencia:\", conv)\n",
    "    V = V_new\n",
    "    if conv < 0.1:\n",
    "        break\n",
    "\n",
    "print(\"V =\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método nos permite calcular el retorno esperado para cada estado del grafo. Por ejemplo, $V(0)$ corresponde al retorno promedio para una cadena de Markov con recompensa que comienza en el estado 0.\n",
    "\n",
    "\n",
    "Reflexionemos un poco sobre lo que acabamos de ver. ¿Qué cometido tiene aquí la matriz de probabilidad de transiciones? Esta matriz guía el movimiento del agente a través de los estados. Pero, ¿hace esta matriz que los valores $V$ obtenidos sean los máximos posibles? Si fuera así, nuestro problema estaría resuelto.\n",
    "Esta matriz, que pudo haber sido inicializada de manera totalmente aleatoria, nos ha servido para **modelar** simplemente el problema de obtención de los valores $V$. A partir de ahora, prescindiremos de ella y nos moveremos entre estados con el único objetivo de maximizar el **retorno**. Para ello vamos a utilizar una función que llamaremos **política**, y la denotaremos con el símbolo $\\pi$.\n",
    "\n",
    "\n",
    "> La **política** es una función que otorga, a partir de un estado, probabilidades de elección sobre las acciones a tomar por el agente.\n",
    "\n",
    "Por tanto, $\\pi(a|s)$ nos indica la probabilidad de que el agente tome la acción $a$ en el estado $s$. Por ejemplo, supongamos que el agente está en el estado $s$ y hay tres posibles acciones a tomar: $a_1$, $a_2$ y $a_3$. La función $\\pi$ nos podría devolver: $\\pi(a_1|s)$ = 0.9,  $\\pi(a_2|s)$ = 0.0 y $\\pi(a_3|s)$ = 0.1. Esto nos permite calcular la esperanza matemática en función de $\\pi$ y, por tanto, tener: $\\mathbb E_\\pi$.\n",
    "\n",
    "$$\n",
    "V_\\pi(s) \\leftarrow R + \\gamma \\cdot \\mathbb E_\\pi[ V_\\pi(S_{t+1}) | S_t=s] \n",
    "$$\n",
    "\n",
    "Ahora, $V_\\pi$ simplemente significa que los valores son calculados a partir de la función $\\pi$.\n",
    "\n",
    "Digamos que $\\mathbb E_\\pi$ prescinde de la anterior matriz de probabilidades de transición e  incorpora nuestra nueva función $\\pi(a|s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de recompensas\n",
    "\n",
    "Hasta ahora hemos utilizado el vector de recompensas $R$, lo cual indica que llegados a un estado $s$ obtendremos la recompensa $R(s)$. A partir de ahora generalizaremos las recompensas con la matriz $R^a_{s,s'}$ que indica que la recompensa vendrá dada por el estado $s$ del que se parte, por la acción $a$ que se tome y por el estado $s'$ al que llegue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función $V$ y funcion $Q$\n",
    "\n",
    "Introducimos un nuevo concepto: la **función de valor de estado-acción**, que llamaremos $Q$. La función $V$ ya la hemos visto, y la llamaremos **función de valor de estado**. Como ya hemos visto, $V(s)$ representa el retorno esperado que tendríamos a partir de un estado $s$. $Q(s,a)$ es el retorno esperado que tendríamos a partir del estado $s$ si ejecutamos la acción $a$. Por tanto:\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\sum_a \\pi(a|s) \\cdot Q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "Y, a su vez, si partimos del estado $s$ y mediante la acción $a$ llegamos al estado $s'$:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = R^a_{s,s'} + \\gamma \\cdot V_\\pi(s')\n",
    "$$\n",
    "\n",
    "Veámoslo un poco más claro con la figura siguiente. \n",
    "\n",
    "\n",
    "<img src=\"./imgs/VyQ.svg\" width=70%>\n",
    "\n",
    "\n",
    "Tenemos tres estados $s_1$, $s_2$ y $s_3$. Cada uno tiene su valor $V_\\pi(s_1)$, $V_\\pi(s_2)$ y $V_\\pi(s_3)$ . $s_1$ está conectado a $s_2$ mediante la acción $a’$ y a $s_3$ mediante la acción $a’’$. Estos enlaces tienen asociadas sus respectivas recompensas $R_{s_1,s_2}^{a'}$ y $R_{s_1,s_3}^{a'’}$. Ahora es importante recordar que el agente escogerá ir desde $s_1$ a $s_2$ o a $s_3$ en función de las probabilidades que genere la función $\\pi$ o, dicho de otra forma, la política $\\pi$. Supongamos que escoger la acción $a’$ tiene una probabilidad del $80\\%$ y escoger la acción $a’’$ el restante $20\\%$.\n",
    "\n",
    "$$\n",
    "V_\\pi(s_1) = 0.8 \\cdot [R_{s_1,s_2}^{a'} +  \\gamma \\cdot V_\\pi(s_2)] + 0.2 \\cdot [R_{s_1,s_3}^{a''} +  \\gamma \\cdot V_\\pi(s_3)]\n",
    "$$ \n",
    "\n",
    "o lo que es lo mismo:\n",
    "\n",
    "$$\n",
    "V_\\pi(s_1) = 0.8 \\cdot Q_\\pi(s_1, a') + 0.2 \\cdot Q_\\pi(s_1, a'') = \\sum_{a\\in(a',a'')} \\pi(a|s_1) \\cdot Q_\\pi(s_1,a)\n",
    "$$ \n",
    "\n",
    "\n",
    "#### Transiciones no deterministas\n",
    "\n",
    "Hasta ahora hemos supuesto que al estar en el estado $s$ y ejecutar la acción $a$ nos vamos al estado $s’$. Pero esto no siempre tiene por qué ser así. Si estamos programando un agente para que aprenda a jugar al ajedrez y le decimos que ejecute la acción “mover el peón ‘x’ una casilla hacia adelante”, pasaremos de un estado del juego a otro de una forma totalmente determinista. Pero si estamos enseñando al agente a jugar al parchís y ejecutamos la acción “tirar el dado” podemos irnos a seis estados distintos de una manera estocástica. Por lo tanto, en muchos contextos distintos, ejecutar la acción $a$ no nos garantiza llegar al estado $s’$, sino solo una probabilidad de llegar a ese estado.\n",
    "\n",
    "\n",
    "Para manejar esta nueva situación vamos a hacer uso de la matriz $P_{s,s'}^a$, la cual nos dará la probabilidad de que partiendo del estado $s$ y ejecutando la acción $a$ lleguemos al estado $s’$. Por tanto, $Q_\\pi(s,a)$ corresponderá al promediado mediante $P_{s,s’}^a$ de todos los estados a los que se llegue a partir del estado $s$ ejecutando la acción $a$.\n",
    "\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = \\sum_{s'} P_{s,s'}^a [ R^a_{s,s'} + \\gamma \\cdot V_\\pi(s') ]\n",
    " $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Ha llegado el momento de concretar nuestra política $\\pi$. ¿Cómo vamos a seleccionar las acciones a realizar para que el retorno sea máximo desde cada estado? Teniendo presente lo que acabamos de ver sobre las transiciones no deterministas, volvamos a retomar, por sencillez, la funcion $V$ y $Q$ con transiciones deterministas.\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) =  R^a_{s,s'} + \\gamma \\cdot V_\\pi(s')\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\sum_a \\pi(a|s) \\cdot Q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "En \"pseudopython\", vamos a implementar el algoritmo de aprendizaje **value interation**, que será parecido al algoritmo de Bellman Expectation Equation, pero no contaremos con una matriz de probabilidades de transición.\n",
    "\n",
    "```python\n",
    "1 Initialize V to arbitrary values\n",
    "2 while V not converge:\n",
    "3    for s in S:\n",
    "4        for a in A:\n",
    "5            Q[s,a] = R[s,a,s_] + gamma * V[s_]  # s_ corresponde al estado al que se llega\n",
    "6        V[s] = max(Q[s,:])\n",
    "```\n",
    "\n",
    "Como vemos, recorremos el conjunto $S$ de todos los estados y, en cada estado $s$, analizamos cada acción $a$ posible desde $s$. Vemos a qué **valor de estado** $V(s')$ nos lleva (valor de retorno esperado), lo multiplicamos por el factor $\\gamma$ y le sumamos la recompensa por ir de $s$ a $s'$. Eso nos va generando para cada estado $s$ la fila de la matriz $Q[s,a_1], \\dots, Q[s,a_n]$. Una vez calculada la fila, nos quedamos con el máximo valor `max(Q[s,:])` para actualizar $V(s)$. Si nos fijamos bien, podríamos fusionar las líneas 5 y 6 de esta forma:\n",
    "\n",
    "```python\n",
    "1 Initialize V to arbitrary values\n",
    "2 while V not converge:\n",
    "3    for s in S:\n",
    "4        for a in A:\n",
    "5            Q[s,a] = R[s,a,s_] + gamma * max(Q[s,:])\n",
    "```\n",
    "\n",
    "Lo que nos debe recordar mucho al algoritmo **Q-Learning** ya visto. \n",
    "\n",
    "\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "```python\n",
    "1 Initialize a policy π´arbitrarily\n",
    "2 while π != π:\n",
    "3    π ← π´\n",
    "4    V[s] = R[s,π(s),s´] + gamma * V[s´]\n",
    "5    π´(s) ← argmax_a(R[s,a] + gamma * V(s´)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0 17.0\n",
      "1 10.76 22.92\n",
      "2 16.4176 29.499200000000002\n",
      "3 21.459776 34.320192000000006\n",
      "4 25.369653760000002 38.283153920000004\n",
      "5 28.527283097600005 41.42804305920001\n",
      "6 31.046252056576005 43.95006966579201\n",
      "7 33.06313950642176 45.96622328020993\n",
      "8 34.67623851845878 47.57949841274963\n",
      "9 35.9668163555699 48.8700339809401\n",
      "10 36.99925495466323 49.902482724574384\n",
      "11 37.82521151488083 50.72843685010215\n",
      "12 38.48597539962861 51.38920131917549\n",
      "13 39.014586834649144 51.91781261395789\n",
      "14 39.437475904132214 52.34070171709811\n",
      "15 39.775787178566674 52.67901298345486\n",
      "16 40.04643619359072 52.94966200041756\n",
      "17 40.26295540669561 53.166181213057165\n",
      "18 40.43617077691896 53.33939658339219\n",
      "19 40.57474307316018 53.4779688796066\n",
      "20 40.68560091013814 53.588826716590994\n",
      "21 40.77428717972411 53.67751298617543\n",
      "22 40.845236195392026 53.74846200184371\n",
      "23 40.90199540792656 53.805221214378165\n",
      "24 40.947402777954146 53.85062858440576\n",
      "25 40.98372867397622 53.88695448042784\n",
      "26 41.012789390793884 53.9160151972455\n",
      "27 41.03603796424801 53.939263770699625\n",
      "28 41.054636823011315 53.95786262946293\n",
      "29 41.06951591002196 53.97274171647357\n",
      "30 41.08141917963047 53.98464498608208\n",
      "31 41.090941795317285 53.99416760176889\n",
      "32 41.09855988786673 54.00178569431835\n",
      "33 41.10465436190629 54.00788016835791\n",
      "34 41.109529941137936 54.01275574758955\n",
      "35 41.11343040452326 54.016656210974865\n",
      "36 41.1165507752315 54.01977658168312\n",
      "37 41.11904707179811 54.02227287824972\n",
      "38 41.12104410905139 54.024269915503005\n",
      "39 41.122641738854014 54.02586754530563\n",
      "40 41.12391984269612 54.027145649147734\n",
      "41 41.1249423257698 54.02816813222142\n",
      "42 41.12576031222875 54.02898611868036\n",
      "43 41.1264147013959 54.02964050784751\n",
      "44 41.12693821272963 54.030164019181235\n",
      "45 41.1273570217966 54.030582828248214\n",
      "46 41.127692069050184 54.0309178755018\n",
      "47 41.12796010685305 54.03118591330467\n",
      "48 41.12817453709535 54.03140034354696\n",
      "49 41.12834608128918 54.031571887740796\n"
     ]
    }
   ],
   "source": [
    "X=0\n",
    "Y=0\n",
    "\n",
    "for i in range(50):\n",
    "    X_ = 1. + 0.8 * (0.3*X + 0.7*Y)\n",
    "    Y_ = 17. + 0.8 * (0.4*Y + 0.6*X) \n",
    "    #print(abs(X-X_)+abs(Y-Y_))\n",
    "    X = X_\n",
    "    Y = Y_\n",
    "    print(i, X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proceso de Decisión de Markov (MDP)\n",
    "\n",
    "Es un problema de aprendizaje por refuerzo en el que el entorno cumple la propiedad de Markov.\n",
    "\n",
    "### Proceso de Decisión de Markov finito\n",
    "\n",
    "Es un MDP en el que el espacio de estados y de acciones es finito. Un MDP finito se caracteriza por el comportamiento del entorno y por el calculo de las recompensas:\n",
    "\n",
    "$P^a_s,s'$  (probabilidad de alcanzar $s'$ a partir de $s$ por la acción $a$)\n",
    "\n",
    "$R^a_s,s'$ (recompensa asociada a la transición $s \\rightarrow  s$ por la acción $a$) \n",
    "\n",
    "\n",
    "<img src=\"./imgs/1_mUyxMUpzQWX4GNTd7TT4nA.gif\">\n",
    "\n",
    "\n",
    "### Política ($\\Pi$)\n",
    "\n",
    "Expresa el comportamiento del agente. Refleja la probabilidad de escoger la acción $a$ al leer el estado $s$, lo expresamos como: $\\Pi(s,a)$. El objetivo del aprendizaje por refuerzo es modificar la **política** para maximizar la recompensa recibida a largo plazo. \n",
    "\n",
    "### Retorno ($R_t$)\n",
    "\n",
    "Es la recompensa acumulada a partir del paso $t$.\n",
    "\n",
    "$$ R_t = r_{t+1} + r_{t+2} + \\cdots + r_{T} $$\n",
    "\n",
    "donde $T$ es el instante final. El objetivo del aprendizaje por refuerzo es maximizar el retorno esperado.\n",
    "\n",
    "### Problema episódico\n",
    "\n",
    "Problema en el que existe un estado final. Por ejemplo, un juego en el que se gana o se pierde. Un **episodio** consiste en una secuencia de pasos desde un estado inicial a un estado final.\n",
    "\n",
    "### Problema continuo\n",
    "\n",
    "Problema en el que no existe un estado final. El agente continúa su funcionamiento de forma ilimitada $(T \\rightarrow \\infty)$.\n",
    "\n",
    "### Retorno con descuento\n",
    "\n",
    "Recompensa calculada con un factor de descuento $\\gamma \\in[0,1]$. El factor de descuento se introduce para evitar que los problemas continuos manejen recompensas infinitas. \n",
    "\n",
    "$$ R_t = r_{t+1} + \\gamma \\cdot r_{t+2} + \\gamma^2 \\cdot r_{t+3} + \\cdots$$\n",
    "\n",
    "Para unificar el tratamiento de los problemas continuos y episódicos se asume que a partir del estado final, todas las transiciones conducen al estado final y tienen recompensa 0.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Función de Valor $V^\\Pi(s)$\n",
    "\n",
    "Retorno esperado para el estado $s$ siguiendo la política $\\Pi$.\n",
    "\n",
    "\n",
    "### Función de Acción-Valor $Q^\\Pi(s,a)$\n",
    "\n",
    "Retorno esperado para el estado $s$ si se toma la acción $a$ y se sigue la política $\\Pi$.\n",
    "\n",
    "Estas funciones están relacionadas:\n",
    "\n",
    "$$ V^\\Pi(s) = \\sum_a \\Pi(s,a) \\cdot Q^\\Pi(s,a)$$\n",
    "\n",
    "$$ Q^\\Pi(s,a) = \\sum_{s'} P^a_{s,s'} [R^a_{s,s'} + \\gamma \\cdot V^{\\Pi}(s')]$$\n",
    "\n",
    "\n",
    "### Ecuación de Bellman para la función de valor\n",
    "\n",
    "$$ V^\\Pi(s) = \\sum_a \\Pi(s,a) \\cdot \\sum_{s'} P^a_{s,s'} [ R^a_{s,s'} + \\gamma \\cdot V^{\\Pi}(s')] $$\n",
    "\n",
    "\n",
    "### Ecuación de Bellman para la función de acción-valor\n",
    "\n",
    "$$ Q^\\Pi(s,a) = \\sum_{s'} P^a_{s,s'} [R^a_{s,s'} + \\gamma \\cdot \\sum_{a'} \\Pi(s',a') \\cdot Q^\\Pi(s',a')]$$\n",
    "\n",
    "\n",
    "Políticas óptimas son aquellas que consiguen el mayor retorno.\n",
    "\n",
    "$$ V^*(s) = max_\\Pi V^\\Pi(s) $$\n",
    "\n",
    "$$ Q^*(s,a) = max_\\Pi Q^\\Pi(s,a) $$\n",
    "\n",
    "\n",
    "### Ecuaciones de optimalidad de Bellman\n",
    "\n",
    "$$ V^*(s) = max_a \\sum_{s'} P^a_{s,s'} [R^a_{s,s'} + \\gamma \\cdot V^*(s')] $$\n",
    "\n",
    "$$ Q^*(s,a) = \\sum_{s'} P^a_{s,s'} [R^a_{s,s'} + \\gamma \\cdot max_{a'} Q^*(s',a')] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema de los n bandidos con arma\n",
    "\n",
    "Vamos a empezar nuestro estudio del aprendizaje por refuerzo con un ejemplo que nos permitirá darnos cuenta de los elementos que conforman este método de aprendizaje máquina.\n",
    "\n",
    "En Las Vegas, un bandido con un arma (*bandit with an arm*) se refiere a una máquina tragaperras. Se trata de un juego de palabras (*arm* significa \"arma\" o \"brazo\" y hace referencia a la palanca de estas máquinas).\n",
    "\n",
    "<img src=\"./imgs/tragaperras.jpg\" width=30%>\n",
    "\n",
    "Una máquina tragaperras es un objeto sobre el que se puede realizar una acción. En este caso, tirar de la palanca y ver si tenemos premio (recompensa). Bien, pues nos encontramos de turismo en Las Vegas y vamos a pasar un par de horas visitando un casino. Nuestro problema consiste en que tenemos un número $n$ de estas máquinas tragaperras y sospechamos que cada máquina ofrece premios con distintas probabilidades, pero no sabemos qué máquinas son las que dan premios más frecuentemente. ¿Qué estrategia podemos seguir para saberlo? Si tuviéramos todo el tiempo del mundo podríamos probar muchísimas veces cada máquina y sacar el promedio de premios de cada máquina. Pero nuestro tiempo es limitado. ¿Qué se te ocurre hacer?\n",
    "\n",
    "Este problema de aprendizaje por refuerzo consiste en maximizar las recompensas (premios) seleccionando la acción (tirar de una de las $n$ palancas) más adecuada. Si escogimos una determinada máquina al azar ($a_i$) y, en un determinado momento $t$, hemos realizamos sobre ella $k$ intentos, la estimación de recompensa promedio sería:\n",
    "\n",
    "$$ Q_t(a_i) = \\frac{r_1 + r_2 + \\cdots + r_k}{k} $$\n",
    "\n",
    "$Q_t(a_i)$ representa el conocimiento (recompensa media) que tenemos de la máquina (acción) $a_i$ en un momento dado $t$. Las $r$ son las distintas recompensas obtenidas en cada intento. A medida que vayamos haciendo más intentos podremos ir actualizando la información:\n",
    "\n",
    "$$  Q_{t+1}(a_i) = Q_t(a_i)+\\frac{1}{k+1} \\cdot [r_{k+1} - Q_t(a_i)]  $$ \n",
    "\n",
    "\n",
    "Ya tenemos una forma de saber el promedio de premios que nos está devolviendo cada máquina. Parece lógico que al principio vayamos seleccionando máquinas al azar para ir viendo cómo va evolucionando la información de promedios de cada máquina (**exploración**). Pero, tras un cierto tiempo, podríamos ir dando mayor probabilidad de selección a las máquinas que mejor promedio vayan ofreciendo a fin de maximizar nuestra recompensa (**explotación**). Esta idea es el origen de las estrategias que vamos a estudiar a continuación.\n",
    "\n",
    "#### Estrategia avariciosa (greedy)\n",
    "\n",
    "Consiste en escoger siempre la acción con mayor recompensa promedio estimada. Por tanto, pretende explotar al máximo las estimaciones, pero a costa de explorar poco el comportamiento de las acciones.\n",
    "\n",
    "#### Estrategia ε-greedy\n",
    "\n",
    "Consiste en asumir una probabilidad $(1- \\epsilon)$ con la que escoger la acción con mayor estimación, y una probabilidad $\\epsilon$ de escoger aleatoriamenteentre entre todas las acciones. Esta estrategia aumenta la exploración en el comportamiento de las acciones, lo que permite que las estimaciones de la recompensa promedio sean mejores y a largo plazo se consiga mejor recompensa.\n",
    "\n",
    "#### Estrategia softmax\n",
    "\n",
    "Se basa en escoger con mayor probabilidad a las acciones con una estimación más alta. Para ello, utiliza la función de distribución de Boltzmann:\n",
    "\n",
    "$$\n",
    "\\Pi(a_i) = \\frac{e^{Q(a_i)/\\tau}}{\\sum_j e^{Q(a_j)/\\tau}}\n",
    "$$\n",
    "\n",
    "$\\Pi(a_i)$ es la probabilidad de ser seleccionada la acción $a_i$, y  $\\tau$ es una variable de control que en los sistemas físicos representa a la temperatura. Si la temperatura se aproxima a cero se obtiene la estrategia *greedy*. Al aumentar la temperatura se aumenta la exploración. Se puede utilizar una temperatura constante, o bien una temperatura inicial alta (mucha exploración inicial) que vaya disminuyendo (mayor explotación al final). Esto se conoce como enfriamiento simulado.\n",
    "\n",
    "\n",
    "#### Estrategia avariciosa con valores iniciales optimistas\n",
    "\n",
    "Se basa en partir de unos valores estimados mucho mayores que las recompensas medias reales. De esta forma, al seleccionar una acción se obtiene una recompensa menor que la estimada y se reduce la estimación. Esto hace que la siguiente iteración no escoja esta acción (que dejaría de tener la estimación más alta) lo que favorece la exploración. Esta estrategia consigue una alta exploración inicial seguida deuna fuerte explotación final.\n",
    "\n",
    "\n",
    "\n",
    "#### Estrategia de comparación por refuerzo (*reinforcement comparison*)\n",
    "\n",
    "Se basa en comparar la estimación de una acción con la media de todas. Utiliza la regla de selección softmax pero usando la preferencia de la acción $p(a_i)$ en lugar de la estimación de la recompensa media $Q(a_i)$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\Pi(a_i) = \\frac{e^{p(a_i)/\\tau}}{\\sum_j e^{p(a_j)/\\tau}}\n",
    "$$\n",
    "\n",
    "La preferencia de cada acción se actualiza según la ecuación:\n",
    "\n",
    "$$\n",
    "p_{t+1}(a_i) = p_{t}(a_i) + \\beta \\cdot (r_t - \\overline{r}_t) \n",
    "$$\n",
    "\n",
    "\n",
    "Donde $r_t$ es la recompensa obtenida en la última selección, $\\overline{r}_t$ es la recompensa media de todas las evaluaciones realizadas y $\\beta$ es una constante menor que 1.\n",
    "\n",
    "\n",
    "#### Métodos de persecución (*pursuit*)\n",
    "\n",
    "Se basa en almacenar tanto la estimación de la recompensa como la probabilidad de elección de cada acción. La estimación de cada acción $a$ se actualiza siguiendo el método habitual: \n",
    "\n",
    "$$\n",
    "Q_{t+1}(a) = Q_t(a) + \\frac{1}{k+1} · [r_k+1 –Q_t(a) ]\n",
    "$$\n",
    "\n",
    "La probabilidad de cada acción se actualiza de manera que se potencie la acción con mayor estimación $a^*$.\n",
    "\n",
    "$$ \\Pi_{t+1}(a^*) = \\Pi_t(a^*)  + \\beta \\cdot [1-\\Pi_t(a^*)] $$\n",
    "\n",
    "$$ \\Pi_{t+1}(a) = \\Pi_t(a)  + \\beta \\cdot [0 - \\Pi_t(a^*)]; a \\neq a^* $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locations: [1.63819033 3.41082852 3.73422331 2.96701699 3.22323819 2.28648222\n",
      " 2.64336969 3.63441651 1.26999667 1.66387249]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f3cf06bb48>]"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class E_greedy:\n",
    "    \n",
    "    def __init__(self, mu, sigma, bandits=10, eps=0.1):\n",
    "        self.eps = eps\n",
    "        self.Q = np.zeros(bandits)\n",
    "        self.K = np.zeros(bandits)\n",
    "        self.mu = mu.copy()\n",
    "        self.sigma = sigma.copy()\n",
    "        self.best_action = np.argmax(self.mu)\n",
    "        self.best_action_acc = 0\n",
    "        self.best_action_acc_time = []\n",
    "        self.k = 1\n",
    "        return\n",
    "        \n",
    "    def action_and_reward(self):\n",
    "        e = np.random.uniform(0, 1)\n",
    "        if e<self.eps:\n",
    "            a = np.random.randint(0, len(self.Q))\n",
    "        else:\n",
    "            a = np.argmax(self.Q)\n",
    "            \n",
    "        if a==self.best_action:\n",
    "            self.best_action_acc += 1\n",
    "            \n",
    "        self.best_action_acc_time.append(float(self.best_action_acc/(self.k)))\n",
    "        self.k += 1\n",
    "            \n",
    "        r = np.random.normal(self.mu[a],self.sigma[a])\n",
    "        \n",
    "        self.K[a] += 1\n",
    "        self.Q[a] += (1/self.K[a] + 1) * (r - self.Q[a])\n",
    "        return a, r\n",
    "    \n",
    "    \n",
    "    \n",
    "class Estrategia_Softmax:\n",
    "    \n",
    "    def __init__(self, mu, sigma, bandits=10, eps=0.1):\n",
    "        self.eps = eps\n",
    "        self.Q = np.zeros(bandits)\n",
    "        self.K = np.zeros(bandits)\n",
    "        self.mu = mu.copy()\n",
    "        self.sigma = sigma.copy()\n",
    "        self.best_action = np.argmax(self.mu)\n",
    "        self.best_action_acc = 0\n",
    "        self.best_action_acc_time = []\n",
    "        self.k = 1\n",
    "        return\n",
    "        \n",
    "    def __boltzmann(self, tau=1):\n",
    "        P = np.empty_like(self.Q)\n",
    "        aux = 0.\n",
    "        for q in self.Q:\n",
    "            aux += np.exp(q/tau)\n",
    "        for i, q in enumerate(self.Q):\n",
    "            P[i] = (np.exp(q/tau)) / aux\n",
    "        return P\n",
    "\n",
    "    def action_and_reward(self, tau=1):\n",
    "    \n",
    "        P = self.__boltzmann(tau)\n",
    "        a = np.random.choice([i for i,_ in enumerate(self.Q)], p=P)\n",
    "           \n",
    "        if a==self.best_action:\n",
    "            self.best_action_acc += 1\n",
    "            \n",
    "        self.best_action_acc_time.append(float(self.best_action_acc/(self.k)))\n",
    "        self.k += 1\n",
    "            \n",
    "        r = np.random.normal(self.mu[a],self.sigma[a])\n",
    "        \n",
    "        self.K[a] += 1\n",
    "        self.Q[a] += (1/self.K[a] + 1) * (r - self.Q[a])\n",
    "        return a, r\n",
    "        \n",
    "    \n",
    "\n",
    "# Inicializamos 10 máquinas tragaperras como distinas funciones normales de probabilidad\n",
    "mu = np.random.uniform(low=1, high=4, size=10)\n",
    "print(\"Locations:\", mu)\n",
    "sigma = np.ones(10)\n",
    "\n",
    "e_greedy = E_greedy(mu=mu, sigma=sigma, eps=0.1)\n",
    "estrategia_softmax = Estrategia_Softmax(mu=mu, sigma=sigma)\n",
    "\n",
    "episodes = 30000\n",
    "\n",
    "for k in range(episodes):\n",
    "    \n",
    "    e_greedy.action_and_reward()\n",
    "    \n",
    "    if k<20000:\n",
    "        tau = 25\n",
    "    elif k<25000:\n",
    "        tau = 1.5\n",
    "    elif k<28000:\n",
    "        tau = 0.2\n",
    "    estrategia_softmax.action_and_reward(tau=tau)\n",
    "    \n",
    "    \n",
    "plt.plot(e_greedy.best_action_acc_time[0:])\n",
    "plt.plot(estrategia_softmax.best_action_acc_time[0:])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[666, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a=[1,2,3]\n",
    "b=a.copy()\n",
    "b[0]=666\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Agente\n",
    "\n",
    "Es el sujeto del aprendizaje por refuerzo. Su funcionamiento consiste en leer el estado del entorno, realizar acciones sobre el entorno y leer las recompensas que producen estas acciones.\n",
    "\n",
    "\n",
    "#### Entorno\n",
    "\n",
    "Es el objeto sobre el que opera el agente. El entorno recibe las acciones del agente y evoluciona. Su comportamiento suele ser desconocido y estocástico. Es el responsable de generar las recompensas asociadas a las acciones y cambios de estado.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Política\n",
    "\n",
    "Define el comportamiento del agente. Puede verse como un mapeo de estado a acción, es decir, establece las reglas de asociación entre el estado del entorno y la acción a tomar. Puede ser estocástica.\n",
    "\n",
    "\t\n",
    "### Función de refuerzo\n",
    "\n",
    "Establece la recompensa a generar en función del estado del entorno y la acción realizada sobre él. Puede ser\n",
    "estocástica. El objetivo del aprendizaje por refuerzo es maximizar la recompensa total obtenida a largo plazo.\n",
    "\n",
    "### Función de evaluación (función de valor)\n",
    "\n",
    "Refleja una estimación de la recompensa que se va a recibir partiendo de un cierto estado y siguiendo una cierta política. Esta función sirve de base para escoger la acción a realizar (aquella que conduzca al estado con mayor\n",
    "valor). El objetivo de los algoritmos de aprendizaje por refuerzo es construir esta función.\n",
    "\n",
    "### Modelo del entorno\n",
    "\n",
    "Permite predecir el comportamiento del entorno y aprovechar esta información para resolver el problema.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"./imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Algoritmo de Viola-Jones**\n",
    "\n",
    "https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf\n",
    "\n",
    "El **algoritmo de Viola–Jones** (2001–2004) es un método clásico de **detección rápida de objetos** —famosamente, **caras frontales**— en imágenes. Su éxito se basa en combinar tres ideas:\n",
    "\n",
    "1. **Características tipo Haar (Haar-like features)**\n",
    "   Rectángulos blancos/negros que miden diferencias de intensidad (bordes, líneas, centros más claros/osc.) dentro de una ventana. Cada característica es:\n",
    "\n",
    "   $$\n",
    "   f(x)=\\sum_{\\text{blanco}} I - \\sum_{\\text{negro}} I\n",
    "   $$\n",
    "\n",
    "   donde $I$ es la imagen.\n",
    "\n",
    "2. **Imagen integral (integral image / summed-area table)**\n",
    "   Estructura que permite calcular la suma de intensidades de cualquier rectángulo en O(1), haciendo viable evaluar miles de características por ventana muy rápido.\n",
    "\n",
    "3. **AdaBoost + cascada de clasificadores**\n",
    "\n",
    "   * **AdaBoost** selecciona, de entre decenas de miles de características, unas pocas muy discriminativas y construye un **clasificador fuerte** como combinación ponderada de **clasificadores débiles** (umbrales sobre una sola característica).\n",
    "   * **Cascada**: organiza varios clasificadores de menor a mayor complejidad. La mayoría de ventanas se descartan en etapas tempranas con coste mínimo; sólo las candidatas pasan a etapas más estrictas. Esto permite tiempo real en CPUs modestas.\n",
    "\n",
    "## **¿Cómo detecta?**\n",
    "\n",
    "* Se recorre la imagen con una **ventana deslizante** a **múltiples escalas**.\n",
    "* Para cada ventana:\n",
    "\n",
    "  1. Se calculan rápidamente las características con la imagen integral.\n",
    "  2. Se evalúa etapa a etapa la **cascada**; si la ventana falla en alguna, se descarta.\n",
    "  3. Las ventanas que superan todas las etapas son detecciones; luego se aplica **Non-Maximum Suppression (NMS)** para unificar solapes.\n",
    "\n",
    "## **Entrenamiento (resumen)**\n",
    "\n",
    "* Conjuntos de **positivos** (caras) y **negativos** (no-cara).\n",
    "* AdaBoost itera: elige la mejor característica+umbral (débil) que minimiza el error ponderado; actualiza pesos; combina débiles → **fuerte**.\n",
    "* Se apila una serie de clasificadores fuertes en **cascada**, ajustando cada etapa para alcanzar alta detección con bajos falsos positivos acumulados.\n",
    "\n",
    "## **Ventajas**\n",
    "\n",
    "* **Rápido** y apto para **tiempo real** sin GPU.\n",
    "* Implementaciones maduras (p. ej., `cv2.CascadeClassifier`).\n",
    "\n",
    "## **Limitaciones**\n",
    "\n",
    "* Funciona mejor para caras frontales con buena iluminación.\n",
    "* Sensible a variaciones de pose** (perfil, inclinación), oclusiones y cambios fuertes de iluminación.\n",
    "* Hoy está superado por CNNs (HOG+SVM, DPM y, sobre todo, detectores modernos tipo Faster/Mask R-CNN, SSD, YOLO), que son más robustos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 19:42:12.561 Python[1743:129736994] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# face_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "# face_cascade = cv2.CascadeClassifier('haarcascade_fullbody.xml')\n",
    "\n",
    "\n",
    "# cap = cv2.VideoCapture(\"people_walking2.mp4\")\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while(1):\n",
    " \n",
    "    # Take each frame\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (400, 300))\n",
    "     \n",
    "    # Convert to HSV for simpler calculations\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(frame)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible descargar más detectores en:\n",
    "\n",
    "https://github.com/opencv/opencv/tree/master/data/haarcascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/face-detection-with-haar-cascade-727f68dafd08\n",
    "\n",
    "https://www.youtube.com/watch?v=LsK-xG1cLYA\n",
    "\n",
    "Se escogen diferentes patrones que se intentarán buscar en la imagen en diferentes posiciones y escalas. Cada patrón se considera como un clasificador débil. La unión de muchos de estos clasificadores mediante boosting convierte a este método en un clasificador robusto.\n",
    "\n",
    "<img src=\"haar8.png\" width=\"30%\">\n",
    "\n",
    "<img src=\"haar9.jpeg\" width=\"30%\">\n",
    "\n",
    "Una vez aplicado el patrón en un lugar concreto de la imagen, se lleva a cabo el cálculo de la comparación.\n",
    "\n",
    "<img src=\"haar2.webp\" width=\"100%\">\n",
    "\n",
    "Cada patrón se buscar a lo largo y ancho de la imagen.\n",
    "\n",
    "<img src=\"haar3.gif\" width=\"40%\">\n",
    "\n",
    "Este proceso es muy costoso computacionalmente así que se crea una iamgen integral que reducirá muchísimo los cálculos.\n",
    "\n",
    "<img src=\"haar4.gif\" width=\"100%\">\n",
    "\n",
    "<img src=\"haar5.webp\" width=\"100%\">\n",
    "\n",
    "Otro truco para mejorar la velocidad consiste en pasar los patrones en diferentes estapas, el algoritmo original usa 38 etapas. Cuando una no responde positivamente la subventana se descarta y no se considera que contenga una cara.\n",
    "\n",
    "<img src=\"haar6.gif\" width=\"70%\">\n",
    "\n",
    "Cuando una subventana pasa todos los patrones se considera que ha detectado una cara.\n",
    "\n",
    "<img src=\"haar7.gif\" width=\"40%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicios**\n",
    "\n",
    "- Adapta el algoritmo de detección de caras a detección de personas (cuerpo entero). Usa el vídeo <code>people_walking.mp4</code> para probarlo.\n",
    "\n",
    "- Desarrolla un sistema de detección de caras robusto que mantenga la atención sobre una cara sin perder su posición. Para ello, combina la función de detección de caras con la función de tracking de manera que, cuando la detección de caras falle, el tracking pueda seguir encuadrando la cara."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

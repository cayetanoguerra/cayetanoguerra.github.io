{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM Normal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 20])\n",
      "torch.Size([1, 4, 20])\n",
      "torch.Size([1, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "lstm = nn.LSTM(10, 20, batch_first=True)  # input_size, hidden_size\n",
    "input = torch.randn(4, 32, 10)  # batch_size, seq_len, input_size\n",
    "\n",
    "(output, (hn, cn)) = lstm(input)  # output: (batch, seq_len, hidden_size), hn: (num_layers, batch, hidden_size)\n",
    "\n",
    "print(output.shape)  # (batch, seq_len, hidden_size)\n",
    "print(hn.shape)  # (num_layers, batch, hidden_size)\n",
    "print(cn.shape)  # (num_layers, batch, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 10])\n",
      "torch.Size([80, 20])\n"
     ]
    }
   ],
   "source": [
    "print(lstm.state_dict()['weight_ih_l0'].shape)  # (4 * hidden_size, input_size)\n",
    "\n",
    "print(lstm.state_dict()['weight_hh_l0'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight_ih_l0',\n",
       "              tensor([[-5.7216e-02, -1.9367e-01,  9.2481e-02, -1.7623e-01, -1.6725e-01,\n",
       "                       -1.6577e-01,  1.8495e-01, -6.5760e-02, -1.1191e-01, -1.8601e-01],\n",
       "                      [ 1.2134e-02, -1.5433e-01,  6.9470e-02, -1.0564e-01,  6.6758e-02,\n",
       "                       -1.3266e-02, -5.9358e-02, -3.6723e-02,  2.5225e-02,  1.2634e-01],\n",
       "                      [ 1.4621e-01,  1.0567e-02, -7.3916e-02, -1.5028e-01,  1.4370e-01,\n",
       "                       -2.9287e-03, -1.8964e-01,  4.1497e-02, -1.2342e-01, -9.0682e-02],\n",
       "                      [ 1.0304e-01,  2.2318e-01, -7.8909e-02,  4.4542e-02, -1.2780e-01,\n",
       "                        1.0774e-01, -1.0090e-01, -9.7190e-04,  1.2857e-01, -2.0976e-01],\n",
       "                      [ 1.7704e-01, -1.3127e-01, -3.9944e-02, -1.0901e-01,  1.2033e-01,\n",
       "                        1.7726e-01,  9.4991e-02, -6.6559e-02, -1.3103e-01, -2.1678e-01],\n",
       "                      [-1.1451e-01,  1.3932e-01,  2.1815e-01,  7.4647e-02, -1.4192e-01,\n",
       "                       -1.5257e-02, -1.4996e-01, -1.1468e-01, -7.9043e-02,  2.0896e-04],\n",
       "                      [ 9.1778e-02, -3.7787e-02,  1.7893e-01,  1.7417e-01,  1.1723e-01,\n",
       "                       -1.1398e-01,  2.0218e-01, -1.6226e-01, -1.5324e-01,  1.6453e-01],\n",
       "                      [-4.1096e-02,  3.5127e-02, -2.6564e-02,  1.3060e-01,  1.1240e-01,\n",
       "                        1.2832e-01, -6.0388e-02, -1.6043e-01, -1.1856e-01, -9.0358e-02],\n",
       "                      [-1.1341e-01,  6.3853e-02, -1.5193e-02, -3.0635e-02,  2.0159e-01,\n",
       "                        1.7853e-02, -1.0652e-01, -1.2506e-02,  4.3364e-02, -2.0299e-01],\n",
       "                      [ 1.0600e-01,  4.7261e-02, -4.4307e-02,  2.2194e-01,  3.5859e-02,\n",
       "                       -1.3680e-01, -1.0458e-01, -6.3426e-03,  1.0163e-02, -6.6229e-02],\n",
       "                      [ 1.9117e-01, -2.1283e-01,  1.6592e-01, -2.8435e-02,  1.3522e-01,\n",
       "                       -1.0231e-01,  1.8444e-01, -1.9349e-01, -1.3053e-01, -5.5583e-02],\n",
       "                      [-1.2615e-01,  5.3625e-02, -9.0967e-02,  2.2072e-01, -1.2065e-01,\n",
       "                        1.9320e-01, -3.1628e-02, -1.6967e-01, -1.7600e-01, -5.9780e-02],\n",
       "                      [ 1.0697e-01, -9.3404e-02,  1.4724e-01, -1.0395e-01, -2.1225e-02,\n",
       "                        1.7122e-01, -1.3828e-01, -2.0293e-01, -1.9762e-01,  1.9010e-01],\n",
       "                      [-1.4744e-01, -2.8836e-03, -7.1953e-02,  1.3626e-01, -1.1067e-01,\n",
       "                        1.4163e-01, -1.7408e-01, -1.8649e-01,  3.1467e-02, -1.6798e-01],\n",
       "                      [ 5.6878e-02,  7.4429e-02,  1.9548e-01,  1.7873e-01, -1.3792e-01,\n",
       "                       -2.1083e-01, -1.1199e-01,  6.6533e-02, -2.0902e-01, -5.8954e-02],\n",
       "                      [ 1.9397e-01, -3.1206e-02, -1.5346e-01, -1.0855e-01,  1.6390e-01,\n",
       "                       -2.7010e-02,  2.4769e-02,  4.1623e-02, -1.2675e-01,  1.9009e-01],\n",
       "                      [-7.9597e-02, -1.6735e-01,  4.1853e-02, -6.7748e-02, -1.0438e-01,\n",
       "                        1.2998e-01,  2.2197e-01, -1.7207e-01, -5.4876e-02, -1.0757e-01],\n",
       "                      [ 1.8510e-01, -1.2313e-01, -3.8477e-02, -1.7393e-01,  6.3430e-02,\n",
       "                        1.8865e-01, -1.8124e-01,  1.7470e-01, -3.6790e-02, -1.0549e-01],\n",
       "                      [ 2.0395e-01,  1.1062e-01,  5.6223e-02, -4.1636e-02, -1.6395e-01,\n",
       "                        6.6134e-02, -1.3807e-01,  1.5377e-01, -1.2541e-01, -1.5526e-01],\n",
       "                      [ 1.3883e-01,  9.4161e-02, -1.0013e-01, -8.6189e-02,  1.3573e-01,\n",
       "                       -1.0642e-01, -1.7176e-01,  9.6083e-02, -1.5226e-01,  9.8589e-03],\n",
       "                      [-1.0528e-01, -2.1496e-01, -6.2462e-03,  3.9477e-02,  3.4029e-02,\n",
       "                        4.1945e-02, -1.5909e-01,  4.1580e-02,  6.0177e-02, -1.3006e-01],\n",
       "                      [-1.7881e-01, -1.1680e-01,  2.2067e-01, -6.5362e-02, -9.3540e-02,\n",
       "                       -1.4984e-01, -1.8760e-01, -1.8008e-01, -6.6254e-02, -1.8752e-01],\n",
       "                      [-1.1831e-01, -8.9229e-02, -1.4139e-01,  1.6684e-01,  1.8798e-01,\n",
       "                        1.5290e-01, -4.8725e-02, -1.8629e-01, -2.1657e-01,  7.6884e-02],\n",
       "                      [ 2.1040e-01, -1.5762e-03, -1.0624e-01, -4.9073e-02, -3.3427e-03,\n",
       "                        6.3858e-02,  1.3407e-02,  2.0594e-01,  1.2023e-01, -4.4857e-02],\n",
       "                      [-1.5955e-01, -1.9544e-01, -5.0592e-02,  1.8477e-01, -1.7465e-01,\n",
       "                       -5.6418e-02, -1.8406e-01,  5.7504e-02, -1.2544e-01,  2.1394e-01],\n",
       "                      [ 1.5081e-01,  5.0650e-02,  1.6607e-01,  1.7580e-01, -2.1897e-01,\n",
       "                       -3.7313e-02,  1.6347e-01, -1.4044e-01, -1.2519e-01,  7.0255e-02],\n",
       "                      [ 2.0707e-01,  1.7542e-01, -5.5348e-02,  2.2304e-01,  1.5803e-01,\n",
       "                       -1.9481e-01,  9.2446e-02,  3.3466e-02,  2.9147e-02,  4.8454e-02],\n",
       "                      [ 1.4145e-01, -1.2382e-01, -2.0818e-01, -1.2648e-01,  1.5880e-01,\n",
       "                        1.5799e-01, -7.0646e-02,  2.1929e-01,  1.7434e-01, -1.5238e-01],\n",
       "                      [-4.2155e-02, -1.4360e-01,  2.8481e-03, -2.4204e-02, -5.8382e-02,\n",
       "                       -7.6675e-02, -3.5035e-02,  1.8670e-01,  3.7571e-02,  7.5627e-02],\n",
       "                      [-1.4095e-01,  2.1932e-01, -5.9230e-03, -9.9040e-02, -7.5693e-02,\n",
       "                       -1.2447e-02, -1.5861e-01,  7.9445e-02, -1.6294e-01, -8.9336e-02],\n",
       "                      [ 8.6915e-02, -1.7087e-01, -1.7916e-01,  1.1384e-02, -5.9572e-02,\n",
       "                       -8.7986e-02,  5.0488e-02,  8.8470e-02,  5.9692e-02, -1.9863e-01],\n",
       "                      [-8.1174e-02,  8.2628e-02, -3.6157e-03, -8.2616e-02,  1.5849e-01,\n",
       "                        1.0596e-01,  1.1411e-01,  1.3164e-01, -1.1314e-01,  1.2717e-01],\n",
       "                      [ 1.0986e-01,  7.1760e-02, -5.3008e-03, -1.0684e-01, -1.7765e-01,\n",
       "                        2.0022e-01, -1.4250e-01,  7.8367e-02, -1.5654e-01,  1.4608e-01],\n",
       "                      [-1.2867e-01,  1.0489e-01,  2.2334e-01, -2.6891e-02, -1.3560e-01,\n",
       "                        2.0682e-01,  4.3877e-02,  9.3305e-03,  1.7240e-01,  6.1840e-02],\n",
       "                      [ 1.6696e-01, -3.9847e-02, -9.8828e-03, -2.5574e-02,  8.8013e-02,\n",
       "                       -1.5478e-01, -2.1017e-01,  1.7855e-01, -1.6999e-01, -1.4511e-01],\n",
       "                      [-2.0996e-01, -1.6928e-01,  1.7940e-01,  1.4853e-01,  7.6773e-02,\n",
       "                       -1.0441e-01,  9.2319e-02,  1.5430e-01, -7.1160e-02, -7.2202e-02],\n",
       "                      [ 2.1466e-01, -8.6939e-02, -5.0124e-02,  1.6355e-01,  2.0396e-01,\n",
       "                        1.2244e-01, -4.2179e-02, -2.1360e-02, -9.0146e-02,  1.1446e-01],\n",
       "                      [ 1.2847e-02,  8.7281e-02, -1.9671e-01, -1.2975e-01, -1.7498e-01,\n",
       "                       -5.4171e-02,  1.1075e-01,  3.0457e-02, -7.8293e-02, -1.5334e-03],\n",
       "                      [ 1.7837e-01,  2.1236e-01, -1.7742e-01, -1.5570e-01, -1.7759e-01,\n",
       "                        3.0330e-02, -1.6445e-01,  1.4060e-01, -1.0352e-01,  1.2549e-01],\n",
       "                      [-1.4254e-01, -1.0900e-01,  1.8083e-01, -2.0237e-01, -7.1328e-02,\n",
       "                        6.6213e-02, -9.7659e-02,  6.4450e-02, -9.0347e-02,  2.6802e-02],\n",
       "                      [ 8.3043e-02,  9.7312e-02, -2.3640e-03,  1.2919e-01,  1.5962e-01,\n",
       "                       -3.6806e-02,  1.0309e-02,  1.4893e-01, -2.0835e-01,  1.6310e-01],\n",
       "                      [-1.6651e-01,  3.2304e-02, -3.4297e-02, -1.4423e-01, -2.2094e-01,\n",
       "                       -2.1553e-01, -7.9143e-02,  1.6093e-01,  8.4524e-02, -2.0604e-01],\n",
       "                      [ 1.5717e-01,  9.6784e-03,  1.6383e-01, -8.7345e-02,  1.3059e-01,\n",
       "                        1.6175e-01, -1.8952e-01, -1.6484e-01, -8.2017e-02, -9.8629e-02],\n",
       "                      [ 1.4639e-01,  2.4181e-02,  1.2465e-02,  2.0425e-01, -1.0289e-01,\n",
       "                        6.3972e-03, -2.8671e-02,  1.2546e-01,  1.1905e-01,  5.2361e-02],\n",
       "                      [ 1.8160e-01,  6.6045e-02, -9.4013e-02, -1.6901e-01,  1.0910e-01,\n",
       "                       -1.4736e-01,  1.6441e-01, -6.4848e-02, -1.7955e-02, -7.3526e-02],\n",
       "                      [ 1.5046e-01,  1.4050e-01, -3.2980e-02, -1.0420e-01, -1.9472e-01,\n",
       "                       -9.2352e-03,  2.1356e-01, -8.7284e-02,  7.4817e-02,  1.2410e-01],\n",
       "                      [-1.7180e-01,  1.0064e-01, -1.6428e-01, -7.3457e-02, -1.7034e-01,\n",
       "                        1.7775e-01,  5.0703e-02,  1.7522e-01,  1.5025e-01, -1.4828e-01],\n",
       "                      [-1.5308e-01,  1.0193e-01, -1.7386e-01,  1.3959e-01,  1.4293e-01,\n",
       "                        1.8082e-01,  9.1085e-02, -6.9412e-03, -1.0881e-01, -7.0296e-02],\n",
       "                      [-8.5646e-02,  1.2804e-01, -3.8886e-02,  3.6930e-02,  6.7950e-02,\n",
       "                       -1.3241e-01, -1.9202e-01,  7.7491e-02, -5.9107e-02,  2.7712e-02],\n",
       "                      [-1.6331e-01,  2.7508e-02, -1.3339e-02,  1.7322e-01, -1.2401e-01,\n",
       "                       -1.8622e-01,  1.6419e-01, -4.6747e-03,  2.1457e-02, -1.1636e-01],\n",
       "                      [-1.9264e-01, -2.0388e-03,  1.0414e-01, -1.2372e-01,  4.6152e-03,\n",
       "                       -3.6111e-02,  1.6215e-02, -1.7718e-03,  1.3424e-01, -1.8735e-01],\n",
       "                      [-1.9494e-01,  7.0383e-02, -1.1304e-01, -1.4840e-01, -2.6715e-02,\n",
       "                       -1.8233e-02, -2.0697e-01,  1.9968e-01, -1.7875e-01, -1.7221e-01],\n",
       "                      [ 2.1181e-01,  1.0305e-01,  1.8134e-01, -5.5439e-02, -1.6465e-01,\n",
       "                        1.0893e-02, -1.4154e-01,  1.5624e-01,  2.8290e-03, -7.7059e-02],\n",
       "                      [ 3.7776e-02, -2.1638e-01, -1.9169e-01,  9.6668e-02,  3.8458e-03,\n",
       "                        9.9990e-02,  1.5021e-01,  8.7389e-02,  1.8023e-01,  7.7861e-02],\n",
       "                      [ 1.5228e-01,  1.7422e-01,  3.5503e-02,  9.7458e-02, -7.9849e-02,\n",
       "                        6.8396e-02,  8.1888e-02,  1.1286e-01, -1.8475e-01, -2.0218e-01],\n",
       "                      [-1.8617e-01, -1.5321e-01,  6.7031e-02, -1.6664e-01, -1.1491e-01,\n",
       "                       -1.1964e-01, -1.1577e-01, -2.1593e-01,  6.0696e-02, -4.4916e-02],\n",
       "                      [ 1.0242e-01, -1.0339e-01, -1.3511e-01, -5.3477e-02, -2.2750e-03,\n",
       "                        7.4091e-02, -3.8546e-02, -1.3473e-01, -1.0263e-01, -5.6995e-02],\n",
       "                      [ 8.3794e-02,  1.5827e-01, -4.8396e-02, -7.5520e-02,  2.0660e-01,\n",
       "                        1.4621e-01, -2.1889e-01,  7.1218e-02, -2.0485e-01,  9.6976e-02],\n",
       "                      [-1.4493e-01, -7.0272e-02,  1.2468e-01, -4.4337e-02, -1.9843e-01,\n",
       "                       -1.6686e-01, -2.0578e-01,  1.8200e-01,  1.3901e-01, -2.1846e-01],\n",
       "                      [ 1.7669e-01,  1.8814e-01, -1.9909e-01, -4.5999e-02,  1.7642e-01,\n",
       "                        6.3096e-02,  1.0387e-01, -1.0646e-01,  1.4041e-01,  5.1815e-02],\n",
       "                      [ 4.0290e-02, -1.4251e-01,  1.5817e-01,  2.1424e-01,  5.7408e-02,\n",
       "                       -8.8798e-02, -1.3896e-01,  1.0942e-01, -8.6352e-02,  7.7333e-02],\n",
       "                      [ 1.3346e-01, -1.8404e-01, -1.9273e-01,  1.2207e-01,  1.0145e-01,\n",
       "                       -2.2285e-01, -5.9488e-02, -1.1890e-01, -2.5097e-02,  5.7685e-02],\n",
       "                      [ 5.9690e-02, -1.6784e-01,  1.5115e-01,  1.8977e-01, -9.7732e-02,\n",
       "                       -1.9306e-01, -4.0153e-02,  1.0084e-01, -1.8217e-01,  5.5136e-02],\n",
       "                      [-1.6406e-01, -1.3861e-01,  7.9350e-02,  5.1487e-02,  9.3603e-02,\n",
       "                        1.9137e-01,  6.6263e-02, -3.7648e-02, -2.1379e-01, -2.0771e-01],\n",
       "                      [-2.2191e-01, -8.7485e-02, -2.9576e-02, -6.2762e-02, -2.1018e-01,\n",
       "                        6.5611e-03,  1.2649e-01,  1.3245e-01, -9.5067e-02, -1.2293e-01],\n",
       "                      [ 1.2645e-01,  1.1779e-01, -1.6037e-01,  1.7619e-02,  9.6424e-02,\n",
       "                       -1.0048e-01,  1.3741e-01,  1.2106e-01,  2.2884e-03, -1.9493e-01],\n",
       "                      [-1.4424e-02, -8.4532e-02,  1.2720e-01,  2.2136e-01,  1.8569e-01,\n",
       "                        3.7275e-03,  7.9209e-02,  1.9392e-01, -2.6736e-02,  2.1064e-02],\n",
       "                      [ 1.6878e-01, -7.2370e-02,  2.0391e-01, -2.5894e-02, -1.0056e-01,\n",
       "                        8.9827e-03,  7.2600e-02,  7.5700e-02,  1.4841e-01,  1.9691e-02],\n",
       "                      [-3.8646e-02, -6.4474e-02,  2.1001e-01, -9.0615e-02,  6.6508e-03,\n",
       "                        1.7982e-01,  5.0470e-03, -8.0012e-02, -2.2015e-01, -1.3599e-01],\n",
       "                      [-2.0342e-01, -1.3669e-01,  1.7773e-01, -9.8335e-02, -1.4214e-02,\n",
       "                        1.3579e-01, -1.0805e-01, -2.0513e-01,  1.8997e-01,  4.2129e-02],\n",
       "                      [-1.2457e-01,  1.9032e-05,  6.7089e-02,  1.2356e-01,  9.2684e-02,\n",
       "                        5.5915e-02, -1.0165e-01, -2.2343e-02, -2.2316e-01,  2.1038e-01],\n",
       "                      [-1.7644e-01,  6.1434e-02,  1.0706e-01, -1.0537e-01,  6.1431e-02,\n",
       "                        4.9665e-02, -1.2187e-01,  1.3182e-01,  1.8214e-01,  5.3674e-02],\n",
       "                      [ 2.2318e-01, -2.0744e-01,  1.3132e-02, -1.8174e-01, -5.6977e-02,\n",
       "                        1.5623e-01, -1.5227e-01,  1.7284e-01, -3.8254e-02, -5.7070e-02],\n",
       "                      [ 1.7987e-01,  8.6628e-02,  2.1715e-01,  1.0583e-01,  1.7472e-01,\n",
       "                       -1.2723e-01,  1.5968e-01,  4.8072e-02,  1.0394e-01,  1.6152e-01],\n",
       "                      [-1.0163e-01, -7.5591e-02,  2.1289e-01,  8.6909e-02, -1.3289e-01,\n",
       "                        2.0237e-01, -1.4666e-01,  5.7059e-02,  8.0052e-02, -1.1095e-01],\n",
       "                      [-1.2875e-01, -1.3222e-01,  1.8764e-01,  1.3522e-01, -1.6848e-01,\n",
       "                       -2.5279e-02, -2.0176e-01, -7.2160e-02,  1.9850e-01, -3.5428e-02],\n",
       "                      [ 1.1612e-02, -1.6670e-01, -2.0351e-01, -1.8086e-01, -2.0868e-01,\n",
       "                        1.9706e-01,  2.3630e-02, -8.0734e-02,  8.4702e-02, -1.5271e-01],\n",
       "                      [ 1.2024e-01, -1.7968e-01,  1.4983e-01,  6.2259e-02,  1.7508e-01,\n",
       "                        1.7296e-01, -1.9884e-01, -7.9123e-02, -7.1085e-02, -3.0455e-02],\n",
       "                      [-6.5671e-02, -2.0919e-01, -2.1791e-01,  1.7507e-01, -7.3537e-03,\n",
       "                       -4.0648e-02,  1.8500e-01, -1.7558e-01, -1.0119e-01,  9.8953e-03],\n",
       "                      [ 8.6551e-02, -5.7787e-02, -1.5940e-01, -9.3864e-02, -8.7906e-02,\n",
       "                       -3.5193e-02,  1.5085e-01, -3.6160e-02,  1.8554e-01, -6.4772e-02]])),\n",
       "             ('weight_hh_l0',\n",
       "              tensor([[ 0.0125,  0.0125,  0.2098,  ...,  0.0196,  0.0318, -0.0023],\n",
       "                      [-0.0683, -0.1724, -0.1872,  ..., -0.0661, -0.0059, -0.1310],\n",
       "                      [-0.1631,  0.0669, -0.1622,  ...,  0.1258,  0.1911,  0.0818],\n",
       "                      ...,\n",
       "                      [-0.0105, -0.2119,  0.1976,  ..., -0.0576, -0.0313, -0.1901],\n",
       "                      [ 0.0076,  0.0772, -0.1838,  ..., -0.2049, -0.1046,  0.1512],\n",
       "                      [-0.0038,  0.1343, -0.0565,  ...,  0.0271,  0.0463,  0.0675]])),\n",
       "             ('bias_ih_l0',\n",
       "              tensor([ 0.1000, -0.1973,  0.0234, -0.1700,  0.0817,  0.0606,  0.1748, -0.1785,\n",
       "                       0.1324,  0.1027,  0.0868, -0.0888,  0.0596, -0.1168,  0.1436,  0.1425,\n",
       "                      -0.0038, -0.0852,  0.1311,  0.1468, -0.0410, -0.1730, -0.0252, -0.1420,\n",
       "                       0.1351, -0.2038, -0.1942, -0.1814,  0.0014,  0.0166, -0.0311, -0.0709,\n",
       "                       0.0467,  0.1394, -0.0969,  0.0530,  0.2083,  0.0676,  0.0004,  0.1296,\n",
       "                       0.1185, -0.1625,  0.2186, -0.0453, -0.0573, -0.1125, -0.0582, -0.1143,\n",
       "                       0.2089,  0.1479, -0.1657, -0.0882, -0.2156,  0.2006, -0.1461, -0.1796,\n",
       "                       0.2102,  0.0426,  0.0760,  0.1605,  0.1664, -0.1475, -0.1673, -0.1464,\n",
       "                      -0.1467, -0.0010, -0.1697,  0.0237,  0.0467,  0.0931, -0.1778,  0.0237,\n",
       "                      -0.0831, -0.0284, -0.1549, -0.1218, -0.0331,  0.1873, -0.1327, -0.0560])),\n",
       "             ('bias_hh_l0',\n",
       "              tensor([-0.2089, -0.0885, -0.1066,  0.0029, -0.1418,  0.0658,  0.1980, -0.0420,\n",
       "                       0.1409, -0.1740, -0.1830,  0.0011,  0.0469,  0.0518, -0.1609, -0.0467,\n",
       "                       0.0333,  0.1034,  0.0393,  0.0757,  0.1441, -0.0267, -0.1852, -0.1476,\n",
       "                      -0.0861, -0.0395,  0.1849, -0.0656, -0.0034,  0.1995, -0.2150, -0.0843,\n",
       "                      -0.1807, -0.1582, -0.1296, -0.2056, -0.1196, -0.0207,  0.1265,  0.1514,\n",
       "                       0.1996, -0.1132, -0.1678, -0.1756, -0.1994, -0.0874, -0.2154, -0.0837,\n",
       "                      -0.2145,  0.0907,  0.0249,  0.0232,  0.0123, -0.0301, -0.0566, -0.0665,\n",
       "                      -0.1281,  0.1825,  0.0005, -0.2059,  0.1973,  0.2140,  0.0559, -0.0848,\n",
       "                      -0.0880, -0.1849, -0.0956, -0.1530, -0.0413, -0.1976, -0.0536, -0.0061,\n",
       "                      -0.0449, -0.0452,  0.1263, -0.2211,  0.0079,  0.0606, -0.0063, -0.0720]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM multicapa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 20])\n",
      "torch.Size([3, 4, 20])\n",
      "torch.Size([3, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "lstm = nn.LSTM(10, 20, num_layers=3, batch_first=True)  # input_size, hidden_size, num_layers\n",
    "input = torch.randn(4, 32, 10)  # batch_size, seq_len, input_size\n",
    "\n",
    "(output, (hn, cn)) = lstm(input)  # output: (batch, seq_len, hidden_size), hn: (num_layers, batch, hidden_size)\n",
    "\n",
    "print(output.shape)  # (batch, seq_len, hidden_size)\n",
    "print(hn.shape)  # (num_layers, batch, hidden_size)\n",
    "print(cn.shape)  # (num_layers, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM Bidireccional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 40])\n",
      "torch.Size([2, 4, 20])\n",
      "torch.Size([2, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "lstm = nn.LSTM(10, 20, batch_first=True, bidirectional=True)  # input_size, hidden_size, num_layers\n",
    "input = torch.randn(4, 32, 10)  # batch_size, seq_len, input_size\n",
    "\n",
    "(output, (hn, cn)) = lstm(input)  # output: (batch, seq_len, hidden_size), hn: (num_layers, batch, hidden_size)\n",
    "\n",
    "print(output.shape)  # (batch, seq_len, hidden_size)\n",
    "print(hn.shape)  # (num_layers, batch, hidden_size)\n",
    "print(cn.shape)  # (num_layers, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM Bidireccional multicapa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 40])\n",
      "torch.Size([6, 4, 20])\n",
      "torch.Size([6, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "lstm = nn.LSTM(10, 20, num_layers=3, batch_first=True, bidirectional=True)  # input_size, hidden_size, num_layers\n",
    "input = torch.randn(4, 32, 10)  # batch_size, seq_len, input_size\n",
    "\n",
    "(output, (hn, cn)) = lstm(input)  # output: (batch, seq_len, hidden_size), hn: (num_layers, batch, hidden_size)\n",
    "\n",
    "print(output.shape)  # (batch, seq_len, hidden_size)\n",
    "print(hn.shape)  # (num_layers, batch, hidden_size)\n",
    "print(cn.shape)  # (num_layers, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuración de la LSTM**: \n",
    "\n",
    "nn.LSTM(10, 20, num_layers=3, batch_first=True, bidirectional=True)\n",
    "\n",
    "    - `input_size=10`: Cada token de entrada tiene una dimensión de 10.\n",
    "    - `hidden_size=20`: Cada dirección de la LSTM (hacia adelante y hacia atrás) produce un vector oculto de 20 dimensiones por capa y por paso de tiempo.\n",
    "    - `num_layers=3`: La LSTM tiene 3 capas apiladas.\n",
    "    - `batch_first=True`: Indica que la dimensión del lote es la primera dimensión de la entrada y salida.\n",
    "    - `bidirectional=True`: Indica que la LSTM es bidireccional, procesando los datos tanto en la dirección hacia adelante como hacia atrás.\n",
    "\n",
    "### **Tensores de Salida**\n",
    "\n",
    "- **`output`**: `torch.Size([4, 32, 40])`\n",
    "    - La salida tiene dimensiones `(batch_size, seq_len, num_directions * hidden_size)`.\n",
    "    - `batch_size=4`: Tienes 4 ejemplos en un lote.\n",
    "    - `seq_len=32`: Cada ejemplo es una secuencia de 32 tokens.\n",
    "    - `num_directions * hidden_size=40`: Como es una LSTM bidireccional (`num_directions=2`), y cada dirección produce vectores de 20 dimensiones, la dimensión final es de 40. Esto es porque las salidas de ambas direcciones se concatenan para cada paso de tiempo.\n",
    "\n",
    "- **`hn` (estado oculto)**: `torch.Size([6, 4, 20])`\n",
    "    - La dimensión de `hn` es `(num_layers * num_directions, batch, hidden_size)`.\n",
    "    - `num_layers * num_directions=6`: Dado que tienes 3 capas y la LSTM es bidireccional, tienes 6 conjuntos de vectores ocultos al final del paso de tiempo final para cada dirección y cada capa.\n",
    "    - `batch=4`: Correspondiente al tamaño del lote.\n",
    "    - `hidden_size=20`: La dimensión de cada vector oculto.\n",
    "\n",
    "- **`cn` (estado de la celda)**: `torch.Size([6, 4, 20])`\n",
    "    - La dimensión de `cn` es la misma que la de `hn` porque representa el estado de la celda LSTM para cada capa y dirección al final de la secuencia, siguiendo la misma estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Capa lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6, 7, 8, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "lineal = nn.Linear(10, 5)\n",
    "\n",
    "batch = torch.randn(5,6,7,8,3, 4, 10)\n",
    "\n",
    "output = lineal(batch)\n",
    "\n",
    "print(output.shape)  # (3, 7, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GloVe embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove_vectors = GloVe(name='6B', dim=300)\n",
    "\n",
    "glove_embedding = nn.Embedding.from_pretrained(glove_vectors.vectors)\n",
    "\n",
    "# glove_embedding = torch.cat((glove_embedding.weight, torch.zeros(1, 300)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400000, 300])\n"
     ]
    }
   ],
   "source": [
    "# add one more word to the embedding at the end\n",
    "\n",
    "print(glove_embedding.weight.shape)  # (400001, 300)\n",
    "glove_embedding.weight[-1] = torch.zeros(300, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding.weight[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding(torch.LongTensor([399999]))  # torch.Size([1, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] [4, 5, 6] [7, 8, 9]\n",
      "<zip object at 0x1045b2880>\n",
      "(1, 4, 7)\n",
      "(2, 5, 8)\n",
      "(3, 6, 9)\n"
     ]
    }
   ],
   "source": [
    "paq = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "def fun(batch):\n",
    "    a = zip(*batch)  # Desempaqueta el batch. Prueba con zip(batch)\n",
    "    print(*batch)  # Prueba con print(batch)\n",
    "    print(a)\n",
    "    for i in a:\n",
    "        print(i)\n",
    "\n",
    "fun(paq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

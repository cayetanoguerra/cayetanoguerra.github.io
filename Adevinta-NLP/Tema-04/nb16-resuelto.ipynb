{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Redes neuronales recurrentes (RNN)**\n",
    "\n",
    "### **LSTM**\n",
    "\n",
    "El conjunto de datos \"AG_NEWS\" es un conjunto de datos de clasificación de texto ampliamente utilizado en el campo del procesamiento de lenguaje natural (NLP). Contiene noticias de diferentes categorías y se utiliza comúnmente para tareas de clasificación de texto. El conjunto de datos AG_NEWS consta de noticias de cuatro categorías principales, que son:\n",
    "\n",
    "1. **World**: Noticias sobre eventos y acontecimientos globales, como política internacional, relaciones internacionales y noticias mundiales en general.\n",
    "\n",
    "2. **Sports**: Noticias relacionadas con eventos deportivos, resultados de partidos, eventos deportivos nacionales e internacionales, etc.\n",
    "\n",
    "3. **Business**: Noticias relacionadas con el mundo de los negocios, finanzas, economía, empresas, informes de ganancias y otros temas económicos.\n",
    "\n",
    "4. **Sci/Tech**: Noticias relacionadas con ciencia y tecnología, incluyendo avances científicos, novedades tecnológicas, gadgets, investigaciones científicas y más.\n",
    "\n",
    "Cada instancia del conjunto de datos AG_NEWS generalmente consiste en un título y un cuerpo de una noticia, junto con una etiqueta que indica la categoría a la que pertenece. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "from torchtext.data import to_map_style_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "train_iter, test_iter = datasets.AG_NEWS(split=('train', 'test'))\n",
    "\n",
    "train_ds = to_map_style_dataset(train_iter)\n",
    "test_ds = to_map_style_dataset(test_iter)\n",
    "\n",
    "train = np.array(train_ds)\n",
    "test = np.array(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary and embedding\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "glove_vectors = GloVe(name='6B', dim=300)\n",
    "\n",
    "vocab = build_vocab_from_iterator(map(lambda x: tokenizer(x[1]), train_iter), specials=['<pad>','<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors[\"sfsfsdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.stoi[\"of\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.itos[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95812\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "210\n",
      "according\n",
      "prime\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(vocab[\"<pad>\"])\n",
    "print(vocab[\"<unk>\"])\n",
    "print(vocab[\".\"])\n",
    "print(vocab[\"the\"])\n",
    "print(vocab[\"according\"])\n",
    "print(vocab.lookup_token(210))\n",
    "print(vocab.lookup_token(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400000, 300])\n",
      "torch.Size([300])\n",
      "the\n",
      ",\n",
      ".\n",
      "of\n",
      "to\n",
      "and\n",
      "according\n",
      "200\n",
      "tensor([-0.2796,  0.1373,  0.0435,  0.3330, -0.1685,  0.0672, -0.1664,  0.1572,\n",
      "        -0.1213, -1.7386, -0.0249, -0.2657,  0.1754,  0.1733, -0.0024,  0.1593,\n",
      "        -0.1860,  0.2516, -0.3865, -0.3362,  0.1269,  0.0737,  0.2497,  0.4563,\n",
      "        -0.2016,  0.0179, -0.0856,  0.0823,  0.2025, -0.1380,  0.0408,  0.5478,\n",
      "        -0.0415,  0.1931, -0.8055, -0.2265,  0.2003, -0.0392, -0.1752, -0.1792,\n",
      "         0.4521, -0.4672,  0.0034,  0.5178,  0.0486, -0.2500, -0.2938,  0.2332,\n",
      "        -0.2310,  0.1783, -0.0716, -0.0211, -0.0837, -0.1419, -0.0739,  0.1935,\n",
      "        -0.3890,  0.0122, -0.4115, -0.2794, -0.4985,  0.1327,  0.5072,  0.0141,\n",
      "        -0.4911, -0.3892,  0.1637,  0.0691, -0.0703,  0.1932,  0.5417,  0.1424,\n",
      "        -0.1569, -0.2428, -0.4837,  0.1394,  0.3138, -0.1732,  0.0480,  0.4464,\n",
      "         0.2092,  0.1116, -0.2180,  0.4683,  0.0487,  0.0277, -0.2645,  0.1688,\n",
      "        -0.0581, -0.0362, -0.6662, -0.0793, -0.5161,  0.1852, -0.1684,  0.2140,\n",
      "        -0.2902,  0.4714,  0.2542, -0.0383,  0.1633, -0.0513,  0.3866, -0.2973,\n",
      "         0.1303, -0.0264,  0.0468,  0.1309, -0.0337,  0.4315, -0.2183, -0.2341,\n",
      "        -0.3371, -0.0199,  0.2116,  0.0920, -0.1776, -0.3857,  0.1919, -0.1319,\n",
      "        -0.2413, -0.1317, -0.1449,  0.0141, -0.1545,  0.4348,  0.3101,  0.0687,\n",
      "        -0.5515, -0.0882, -0.1567,  0.0310,  0.5377,  0.0210,  0.1534, -0.0465,\n",
      "        -0.1530, -0.3474,  0.0968,  0.1091, -0.0570,  0.0987,  0.1436, -0.4268,\n",
      "        -0.1049,  0.3455,  0.2084,  0.3223, -0.0331,  0.5180,  1.1373,  0.1513,\n",
      "         0.2032,  0.0224, -0.0290,  0.1035, -0.3850,  0.4546,  0.3155, -0.3071,\n",
      "         0.1419,  0.0189, -0.0402, -0.2000,  0.3040, -0.1831,  0.0545, -0.2547,\n",
      "        -0.4163,  0.3757,  0.1349,  0.2068, -0.5459,  0.2168,  0.3319,  0.0592,\n",
      "         0.1864,  0.0310, -0.4613,  0.1556,  0.1465,  0.5143, -0.0523,  0.2551,\n",
      "         0.2068, -0.1258, -0.5178, -0.0032, -0.0243,  0.2082,  0.0660,  0.3164,\n",
      "        -0.0197,  0.1740, -0.1900,  0.1183,  0.0551, -0.2881,  0.0378,  0.1526,\n",
      "        -0.2801, -0.3290,  0.3944,  0.0254, -0.0367,  0.4115, -0.4143,  0.1208,\n",
      "        -0.0803, -0.4722, -0.2957, -0.0747,  0.3132, -0.3265,  0.4405, -0.0416,\n",
      "         0.2676,  0.4845, -0.0435,  0.0648,  0.3276,  0.0287, -0.1253,  0.0542,\n",
      "        -0.1053,  0.2436, -0.0692, -0.0511, -0.0453, -0.0320, -0.0525, -0.1776,\n",
      "         0.0238, -0.3351, -0.0191, -0.1771, -0.0835,  0.0141,  0.0630,  0.1076,\n",
      "         0.1450, -0.2538,  0.9410,  0.0168, -0.0600,  0.1444,  0.0200, -0.2021,\n",
      "        -0.1417,  0.3919,  0.1465,  0.1063,  0.1384, -0.3747,  0.3600, -0.1227,\n",
      "         0.1587,  0.1038,  0.4256,  0.0647,  0.0638,  0.4721,  0.0379, -0.3613,\n",
      "         0.2092, -0.2457,  0.2590, -0.0601, -0.1379, -0.2334, -0.1924, -0.1887,\n",
      "         0.1577,  0.1787,  0.1168, -0.1928, -1.9656, -0.1001,  0.4872,  0.0045,\n",
      "         0.0107, -0.1081,  0.0545, -0.3562,  0.1424,  0.1844, -0.0328, -0.0666,\n",
      "        -0.1629,  0.3452, -0.0135, -0.2190,  0.0378,  0.1559,  0.4425,  0.4338,\n",
      "        -0.1923,  0.3901, -0.3246, -0.0183])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(glove_vectors.vectors.shape)  # 400000 palabras y 300 dimensiones\n",
    "print(glove_vectors['hello'].shape)  # 300 dimensiones\n",
    "print(glove_vectors.itos[0])\n",
    "print(glove_vectors.itos[1])\n",
    "print(glove_vectors.itos[2])\n",
    "print(glove_vectors.itos[3])\n",
    "print(glove_vectors.itos[4])\n",
    "print(glove_vectors.itos[5])\n",
    "print(glove_vectors.itos[200])\n",
    "print(glove_vectors.stoi['according'])\n",
    "print(glove_vectors['according'])\n",
    "print(glove_vectors['ññññ'])  # Si no existe la palabra, devuelve un vector de ceros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 95812 tokens\n",
      "Tokenización de la frase 'Here is an example sentence': ['here', 'is', 'an', 'example', 'sentence']\n",
      "Índices de las palabras 'here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious': [476, 22, 31, 5298, 1]\n",
      "Palabras correspondientes a los índices 475, 21, 30, 5297, 0: ['version', 'at', 'from', 'establish', '<pad>']\n",
      "Las diez primeras palabras del vocabulario: ['<pad>', '<unk>', '.', 'the', ',', 'to', 'a', 'of', 'in', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño del vocabulario:\", len(vocab), \"tokens\")\n",
    "print(\"Tokenización de la frase 'Here is an example sentence':\", tokenizer(\"Here is an example sentence\"))\n",
    "print(\"Índices de las palabras 'here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious':\", vocab(['here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious']))\n",
    "print(\"Palabras correspondientes a los índices 475, 21, 30, 5297, 0:\", vocab.lookup_tokens([475, 21, 30, 5297, 0]))\n",
    "print(\"Las diez primeras palabras del vocabulario:\", vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187, 14, 29, 880, 2422, 399999]\n"
     ]
    }
   ],
   "source": [
    "# Text and label pipelines usando el vocabulario y los vectores de GloVe\n",
    "def text_pipeline(x):\n",
    "    tokens_vocab = tokenizer(x)  # Devuelve una lista de tokens (palabras)\n",
    "    tokens_glove = [glove_vectors.stoi.get(w, 399999) for w in tokens_vocab]\n",
    "    return tokens_glove\n",
    "\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "print(text_pipeline(\"Here is an example sentence dfgdfñ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for sample in batch:\n",
    "        label, text = sample\n",
    "        text_list.append(torch.tensor(text_pipeline(text), dtype=torch.long))\n",
    "        label_list.append(label_pipeline(label))\n",
    "    return torch.tensor(label_list, dtype=torch.long), torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=399999)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que estamos creando correctamente los lotes, vamos a imprimir las primeras cuatro instancias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    95,     50,    163,    526,  10053,    524,      2,    409,     72,\n",
      "              6,    375,     23,   1184,     24,   1184,     11,    526,      3,\n",
      "             50,     95,   1288,   5086,      6,    375,     10,      0,    126,\n",
      "           1362,    229,      1,    933,    896,      1,      7,   1100,      3,\n",
      "         138728,    956,   9283,      0,    442, 118464,    211,      2, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999],\n",
      "        [   361,   1555,      4,   1783,   5028,   8327,   1363,     23,   1582,\n",
      "             24,     65,      7,    623,     78,   1908,     31,   1412,    559,\n",
      "              4,   1545,     55,   5028,   8327,     75,   1828,     57,   1534,\n",
      "           6960,   1084,    182,     62,      1,      7,    493,    255,     16,\n",
      "            177,      2,     83,   1311,      1,      0,   4200,     54,     30,\n",
      "              0,     58,    108,    288,    782,     10,    361,     57,   1534,\n",
      "           7550,    863,    371,      1,     42,     31,   1548,      7,  15413,\n",
      "              3,   1783,      5,    985,   7693,      2,      2,      2, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999],\n",
      "        [    36,     64,   1052,   6288,      4,      0,    298,    300,    967,\n",
      "           6589,      1,  10157,    608,      1,      5,      0,   6403,     10,\n",
      "          16713,     22,    161,    376,      7,  36535,      3,    327,      1,\n",
      "          13065,      1,    623,      1,   3819,      1,  17867,      1,      5,\n",
      "            423,    107,  12715,     59,   2660,     44,    921,   2995,      6,\n",
      "           2299,   1985,      6,   5253,      5,   1699,      6,      0,   2396,\n",
      "              1,    158,  11166,    399,   6575,   1704, 399999,      5,  54748,\n",
      "         399999,  66515,      5, 158892,  54479,     21,     57,   4826,    908,\n",
      "             57,     11,      0,  24760,      5,  32036,    570,     10,      0,\n",
      "         309118,  20658,      2, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999],\n",
      "        [  2917,      6,   4152,     19,   2426,   2493,   2042,    759,     23,\n",
      "          10851,     24,  10851,     11,   2004,   1102,    142,  13944,      0,\n",
      "         399999,      0,  11210,    351,      5,   1596,     15,    897,     13,\n",
      "            240,      1, 399999,    122,      1,     19,   5637,  29952,  15204,\n",
      "             60,      4,      0, 399999,   7605,   6133,     12,    256,     22,\n",
      "            338,    759,     69,      2, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999]])\n",
      "\n",
      "\n",
      "tensor([2, 3, 2, 0])\n",
      "\n",
      "\n",
      "torch.Size([64, 91])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[1][:4])\n",
    "    print(\"\\n\")\n",
    "    print(batch[0][:4])\n",
    "    print(\"\\n\")\n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super(LSTMTextClassificationModel, self).__init__()\n",
    "        # self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.glove_embedding = nn.Embedding.from_pretrained(glove_vectors.vectors)\n",
    "\n",
    "        self.glove_embedding.weight.requires_grad = False\n",
    "        self.glove_embedding.padding_idx = 399999  # Índice de la palabra <unk> en los vectores de GloVe\n",
    "        self.glove_embedding.weight.data[399999] = torch.zeros(embed_dim)  # Vector de ceros para la palabra <unk>\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, num_layers=1, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.glove_embedding(text)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Tomar la última salida de la secuencia LSTM\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 76])\n",
      "tensor([[  4361,    907,    211,   1604,      6,      7,   1604,  20359,  10977,\n",
      "              3,      0,   9500,      2,  10108,   4707,      1,    544,      3,\n",
      "            925,   1468,   1752,   4361,   5698,      6,     44,     58,    122,\n",
      "              3,    198,    857,      2, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999],\n",
      "        [   474,   5760,     19,  56714,  13439,   1590,   1891,     23,  10851,\n",
      "             24,  10851,     11,    474,   4987,     17,   5973,     19,  11663,\n",
      "         250510,  56714,   9756,     44,     58,   1638,     22,      0,   3264,\n",
      "           1478,     13,    171,      5,     44,     58,    661,   1426,   1891,\n",
      "              2, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999],\n",
      "        [ 34243,    491,    401,     81,    682,      6,  27232,     46,  76317,\n",
      "             46,  31270,      1,     20,     14,      7,    219,    122,     61,\n",
      "              0,    328,   1229,   1666,    204,      4,    914,    645,     59,\n",
      "            392, 399999,    491,      2,      0,  39681,    169,   2703,      1,\n",
      "              5,      0,    491,   1666,  94553,      2,     50,  17848,      5,\n",
      "           7180,    242,      6,      2,   2816,      5,    151,     77,   4384,\n",
      "             32,   6360,      2, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999],\n",
      "        [ 10980,   6133,     71,    179,      4,    764,     21,  96253,  11835,\n",
      "           3264,     23,  10851,     24,     11,  17393, 288758,   8802,     71,\n",
      "            179,      4,   1055,    764,    277,      1,   2839,  20891,  13667,\n",
      "           2882,      1,   2882,      6,      7,   4047,      3,      0,     85,\n",
      "             57,   1534,    220,     55,    479,      2, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999, 399999,\n",
      "         399999, 399999, 399999, 399999]])\n",
      "tensor([[ 0.0115, -0.0598, -0.0142, -0.0152],\n",
      "        [ 0.0115, -0.0598, -0.0142, -0.0152],\n",
      "        [ 0.0115, -0.0598, -0.0142, -0.0152],\n",
      "        [ 0.0115, -0.0598, -0.0142, -0.0152]], grad_fn=<SliceBackward0>)\n",
      "tensor([2, 0, 2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTMTextClassificationModel(len(glove_vectors), 300, 64, 4).to(device)\n",
    "model.train()\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch[1].shape)\n",
    "    predicted_label = model(batch[1])\n",
    "    label = batch[0]\n",
    "    break\n",
    "\n",
    "print(batch[1][:4])\n",
    "print(predicted_label[:4])\n",
    "print(label[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count, max_acc = 0, 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            #print(total_acc / total_count)\n",
    "\n",
    "            print('| {:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(idx, \n",
    "                                              total_acc / total_count))\n",
    "\n",
    "            if max_acc < total_acc / total_count:\n",
    "                max_acc = total_acc / total_count\n",
    "                \n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    return max_acc\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   500 batches | accuracy    0.254\n",
      "|  1000 batches | accuracy    0.253\n",
      "|  1500 batches | accuracy    0.269\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 71.12s | valid accuracy    0.687 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.774\n",
      "|  1000 batches | accuracy    0.855\n",
      "|  1500 batches | accuracy    0.880\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 77.96s | valid accuracy    0.888 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.882\n",
      "|  1000 batches | accuracy    0.891\n",
      "|  1500 batches | accuracy    0.909\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 70.79s | valid accuracy    0.896 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.899\n",
      "|  1000 batches | accuracy    0.908\n",
      "|  1500 batches | accuracy    0.925\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 72.19s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.906\n",
      "|  1000 batches | accuracy    0.911\n",
      "|  1500 batches | accuracy    0.930\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 69.12s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 5  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    accu_train = train(train_dataloader)\n",
    "    accu_val = evaluate(test_dataloader)\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/langchain.png\" width=\"400\">\n",
    "\n",
    "**LangChain** es una biblioteca de Python diseñada para simplificar la creación de aplicaciones de lenguaje utilizando modelos de inteligencia artificial, como los basados en la arquitectura GPT de OpenAI. Su objetivo es proporcionar un marco de trabajo que permita a los desarrolladores integrar fácilmente capacidades avanzadas de procesamiento de lenguaje natural (PLN) en sus proyectos, especialmente en contextos donde se necesitan interacciones en lenguaje natural y se quiere explotar la potencia de los modelos de lenguaje grandes y complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clases principales**\n",
    "\n",
    "Las clases principales de Langchain se pueden agrupar en las siguientes categorías:\n",
    "\n",
    "1. **Prompts:**\n",
    "   - `PromptTemplate`: Permite definir plantillas de prompts con variables.\n",
    "   - `ChatPromptTemplate`: Permite definir plantillas de prompts para modelos de chat, como ChatGPT.\n",
    "   - `FewShotPromptTemplate`: Permite definir plantillas de prompts para aprendizaje con pocos ejemplos (few-shot learning).\n",
    "\n",
    "2. **Modelos de lenguaje (LLMs):**\n",
    "   - `LLMChain`: Permite encadenar múltiples prompts y modelos de lenguaje.\n",
    "   - `ChatOpenAI`: Interfaz para interactuar con el modelo de chat de OpenAI (ChatGPT).\n",
    "   - `OpenAI`: Interfaz para interactuar con los modelos de lenguaje de OpenAI (GPT-3, Codex, etc.).\n",
    "\n",
    "3. **Documentos y cargadores:**\n",
    "   - `Document`: Representa un documento con texto y metadatos.\n",
    "   - `TextLoader`: Carga documentos de texto desde archivos.\n",
    "   - `PDFLoader`: Carga documentos PDF y extrae su texto.\n",
    "   - `WebBaseLoader`: Carga documentos desde páginas web.\n",
    "\n",
    "4. **Índices y almacenamiento:**\n",
    "   - `VectorStore`: Almacena y recupera documentos utilizando embeddings vectoriales.\n",
    "   - `FAISS`: Implementación de VectorStore basada en la biblioteca FAISS de Facebook.\n",
    "   - `Chroma`: Implementación de VectorStore basada en la base de datos Chroma.\n",
    "\n",
    "5. **Agentes y herramientas:**\n",
    "   - `Tool`: Representa una herramienta externa que puede ser utilizada por un agente.\n",
    "   - `Agent`: Define un agente que puede realizar tareas utilizando herramientas.\n",
    "   - `AgentExecutor`: Ejecuta un agente y coordina la interacción con las herramientas.\n",
    "\n",
    "6. **Cadenas y pipelines:**\n",
    "   - `Chain`: Clase base para definir cadenas de componentes.\n",
    "   - `SequentialChain`: Permite encadenar múltiples componentes en una secuencia.\n",
    "   - `MapReduceChain`: Permite aplicar una cadena a múltiples entradas y combinar los resultados.\n",
    "   - `TransformChain`: Permite aplicar una transformación a la salida de una cadena.\n",
    "\n",
    "7. **Utilidades:**\n",
    "   - `Memory`: Clase base para implementar memoria en las cadenas y agentes.\n",
    "   - `ConversationBufferMemory`: Implementación de memoria que almacena el historial de conversación.\n",
    "   - `ConversationSummaryMemory`: Implementación de memoria que resume el historial de conversación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejemplos de uso**\n",
    "\n",
    "Veamos algunos ejemplos de cómo se pueden utilizar las clases principales de Langchain para crear nuestras propias aplicaciones.\n",
    "\n",
    "`langchain_openai`: Esta clase se utiliza para interactuar con los modelos de lenguaje de OpenAI a través de la API de OpenAI. Proporciona una interfaz para enviar solicitudes a los modelos de OpenAI, como GPT-3, y recibir las respuestas generadas. Con esta clase, puedes aprovechar la potencia de los modelos de lenguaje de OpenAI en tus aplicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='La capital de Francia es París.' response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 17, 'total_tokens': 26}, 'model_name': 'gpt-4-turbo', 'system_fingerprint': 'fp_ea6eb70039', 'finish_reason': 'stop', 'logprobs': None} id='run-e11797e9-7fdc-4765-bca4-a970897f3cc9-0'\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain-openai\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
    "\n",
    "print(llm.invoke(\"¿Cuál es la capital de Francia?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora queremos utilizar la API para conversaciones con el usuario, podemos hacerlo mediante las plantillas de *prompts*, incluidas en el módulo `langchain_core.prompts`.\n",
    "\n",
    "`langchain_core`: Esta clase es el núcleo de Langchain y contiene las funcionalidades principales de la biblioteca. Proporciona las abstracciones y componentes básicos para construir cadenas de procesamiento de lenguaje natural. Incluye clases para representar documentos, consultas, agentes y otros elementos fundamentales en el flujo de trabajo de Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='La capital de Francia es París.' response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 39, 'total_tokens': 48}, 'model_name': 'gpt-4-turbo', 'system_fingerprint': 'fp_294de9593d', 'finish_reason': 'stop', 'logprobs': None} id='run-3a0a4652-a640-4981-afb6-a80f21c6b1c9-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un agente que contesta amablemente las preguntas de los usuarios.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "print(chain.invoke(\"¿Cuál es la capital de Francia?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, obtenermos la respuesta que nos devuelve el modelo, que es una estructura de datos que contiene el texto generado por el modelo, así como información adicional como los tokens generados, nombre del modelo usado, etc. Para usar solo el texto utilizaremos el módulo `langchain_core.output_parsers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capital de Francia es París.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "print(chain.invoke(\"¿Cuál es la capital de Francia?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo podemos crear un proceso que combine un contexto dado por un archivo de texto y una pregunta dada por el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Beatles estuvo integrado por John Lennon, Paul McCartney, George Harrison y Ringo Starr.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Cargar el fichero de texto\n",
    "loader = TextLoader('data_small.txt')\n",
    "document = loader.load()\n",
    "\n",
    "# Crear el prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Responde a las preguntas basándote solo en el siguiente contexto:\n",
    "                                          \n",
    "{context}\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"context\": document,\n",
    "        \"input\": \"¿Quiénes formaron el grupo Los Beatles?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasaría si nuestro fichero fuera muy grade y no cupiera en el contexto de nuestro modelo del lenguaje? Para esto casos dividiremos el documento en fragmentos de un cierto tamaño y los almacenaremos como documentos separados en una base de datos vectorial. A partir de la pregunta del usuario, buscaremos los fragmentos más relevantes y los pasaremos al modelo para obtener la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1364, which is longer than the specified 1000\n",
      "Created a chunk of size 1608, which is longer than the specified 1000\n",
      "Created a chunk of size 1269, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1067, which is longer than the specified 1000\n",
      "Created a chunk of size 1091, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
    "\n",
    "# Cargar el fichero de texto\n",
    "loader = TextLoader('data.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Dividir el texto en trozos\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Crear una instancia de OpenAIEmbeddings para generar los embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Almacenar los trozos de texto en Chroma\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto: ______________________________________________________\n",
      "1 Décadas después surgieron dos versiones alternativas sobre el origen del nombre The Beatles. La primera proviene de George Harrison que sostuvo que fue tomada de la película The Wild One (El Salvaje), protagonizada por Marlon Brando, aunque la versión es discutible porque dicha película estuvo prohibida en Gran Bretaña hasta 1968. La segunda versión pertenece al poeta Royston Ellis, amigo de la banda y perteneciente al movimiento beat. Según Ellis, una noche de junio de 1960, se encontraba pasando el rato con Lennon y Sutcliffe, cuando surgió la cuestión del nuevo nombre de la banda. John entonces le respondió que se llamaría \"The Beetles\", deletreando la palabra con dos \"E\", significando Los Escarabajos\". Ellis, que pertenecía al movimiento beat y se identificaba con los beatniks, propuso entonces el célebre cambio de la letra \"E\", por la letra \"A\". Ellis relató también que ese día él había cocinado un pollo que se prendió fuego, hecho que habría dado origen al relato de John Lennon, sobre el \"hombre que apareció sobre un pastel flameante\" para darles el nombre de la banda.\n",
      "2 Décadas después surgieron dos versiones alternativas sobre el origen del nombre The Beatles. La primera proviene de George Harrison que sostuvo que fue tomada de la película The Wild One (El Salvaje), protagonizada por Marlon Brando, aunque la versión es discutible porque dicha película estuvo prohibida en Gran Bretaña hasta 1968. La segunda versión pertenece al poeta Royston Ellis, amigo de la banda y perteneciente al movimiento beat. Según Ellis, una noche de junio de 1960, se encontraba pasando el rato con Lennon y Sutcliffe, cuando surgió la cuestión del nuevo nombre de la banda. John entonces le respondió que se llamaría \"The Beetles\", deletreando la palabra con dos \"E\", significando Los Escarabajos\". Ellis, que pertenecía al movimiento beat y se identificaba con los beatniks, propuso entonces el célebre cambio de la letra \"E\", por la letra \"A\". Ellis relató también que ese día él había cocinado un pollo que se prendió fuego, hecho que habría dado origen al relato de John Lennon, sobre el \"hombre que apareció sobre un pastel flameante\" para darles el nombre de la banda.\n",
      "3 Décadas después surgieron dos versiones alternativas sobre el origen del nombre The Beatles. La primera proviene de George Harrison que sostuvo que fue tomada de la película The Wild One (El Salvaje), protagonizada por Marlon Brando, aunque la versión es discutible porque dicha película estuvo prohibida en Gran Bretaña hasta 1968. La segunda versión pertenece al poeta Royston Ellis, amigo de la banda y perteneciente al movimiento beat. Según Ellis, una noche de junio de 1960, se encontraba pasando el rato con Lennon y Sutcliffe, cuando surgió la cuestión del nuevo nombre de la banda. John entonces le respondió que se llamaría \"The Beetles\", deletreando la palabra con dos \"E\", significando Los Escarabajos\". Ellis, que pertenecía al movimiento beat y se identificaba con los beatniks, propuso entonces el célebre cambio de la letra \"E\", por la letra \"A\". Ellis relató también que ese día él había cocinado un pollo que se prendió fuego, hecho que habría dado origen al relato de John Lennon, sobre el \"hombre que apareció sobre un pastel flameante\" para darles el nombre de la banda.\n",
      "4 Décadas después surgieron dos versiones alternativas sobre el origen del nombre The Beatles. La primera proviene de George Harrison que sostuvo que fue tomada de la película The Wild One (El Salvaje), protagonizada por Marlon Brando, aunque la versión es discutible porque dicha película estuvo prohibida en Gran Bretaña hasta 1968. La segunda versión pertenece al poeta Royston Ellis, amigo de la banda y perteneciente al movimiento beat. Según Ellis, una noche de junio de 1960, se encontraba pasando el rato con Lennon y Sutcliffe, cuando surgió la cuestión del nuevo nombre de la banda. John entonces le respondió que se llamaría \"The Beetles\", deletreando la palabra con dos \"E\", significando Los Escarabajos\". Ellis, que pertenecía al movimiento beat y se identificaba con los beatniks, propuso entonces el célebre cambio de la letra \"E\", por la letra \"A\". Ellis relató también que ese día él había cocinado un pollo que se prendió fuego, hecho que habría dado origen al relato de John Lennon, sobre el \"hombre que apareció sobre un pastel flameante\" para darles el nombre de la banda.\n",
      "_________________________________________________________________\n",
      "Existen dos teorías principales sobre el origen del nombre de Los Beatles:\n",
      "\n",
      "1. **Teoría de George Harrison**: Según Harrison, el nombre fue inspirado por la película \"The Wild One\" (El Salvaje), protagonizada por Marlon Brando. Sin embargo, esta versión es discutible, ya que la película estuvo prohibida en Gran Bretaña hasta 1968.\n",
      "\n",
      "2. **Teoría de Royston Ellis**: Ellis, un poeta amigo de la banda y miembro del movimiento beat, relata que durante una noche en junio de 1960, mientras estaba con John Lennon y Stuart Sutcliffe, se discutió el nuevo nombre de la banda. Lennon inicialmente propuso \"The Beetles\", con dos \"E\", significando \"Los Escarabajos\". Ellis, influenciado por su identificación con los beatniks, sugirió cambiar una \"E\" por una \"A\", dando origen al nombre \"The Beatles\". Además, Ellis menciona un incidente en el que cocinó un pollo que accidentalmente se prendió fuego, lo cual habría inspirado a Lennon a crear la historia del \"hombre que apareció sobre un pastel flameante\", que supuestamente les dio el nombre de la banda.\n"
     ]
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"¿Qué teorías hay sobre el origen del nombre de Los Beatles?\"})\n",
    "\n",
    "print(\"Contexto: ______________________________________________________\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(i+1, doc.page_content)\n",
    "print(\"_________________________________________________________________\")\n",
    "    \n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recuperación de información de otras fuentes y con otros LLMs**\n",
    "\n",
    "Vamos a utilizar un modelo de Antrhopic para recuperar información de un documento en formato PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "ANT_API_KEY = os.getenv(\"ANTRHOPIC_API_KEY\")\n",
    "\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229', api_key=ANT_API_KEY)\n",
    "\n",
    "# Cargamos el documento PDF\n",
    "loader = PyPDFLoader(\"https://sie.ulpgc.es/sites/default/files/Gu%C3%ADa%20ULPGC%202023%20(1).pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Separamos el documento en trozos\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Almacenamos los trozos en Chroma\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# Crear un objeto RetrievalQA para responder preguntas\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '¿Cuántos créditos tienen los masters de la ULPGC?', 'result': 'Según la información proporcionada, los planes de estudios de los másteres universitarios de la ULPGC tendrán entre 60 y 120 créditos ECTS, equivalentes a uno o dos cursos académicos respectivamente.'}\n"
     ]
    }
   ],
   "source": [
    "# Realizamos una pregunta y obtenemos la respuesta\n",
    "query = \"¿Cuántos estudiantes tiene la ULPGC?\"\n",
    "query = \"¿Cuántos grupos de investigación hay en la ULPGC?\"\n",
    "query = \"¿Cuántos créditos tienen los masters de la ULPGC?\"\n",
    "result = qa.invoke(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo descargar un modelo de Hugging Face y utilizarlo para responder a una pregunta dada por el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/cayetano/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La capital de Francia es París. París es la ciudad más grande y más poblada de Francia, y es conocida por ser una de las ciudades más importantes del mundo en términos de arte, cultura y economía. Es un destino popular para viajeros de todo el mundo debido a sus famosas obras de arte, monumentos históricos, museos y otros lugares de interés.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\", huggingfacehub_api_token = HF_API_KEY)\n",
    "\n",
    "print(llm(\"<s>[INST] ¿Cuál es la capital de Francia? [/INST]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transcripciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=AHw2yOcK3-k\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué tal? ¿Cómo están? Bienvenidos a una nueva mañana de radio. Estamos el 6 de mayo de 2024, a seis días de San Pancracio. Seis días de que San Pancracio reparta suerte en las elecciones catalanas. Salvador Illa fue el primero en invocar el martirologio cristiano en este programa. Fue él quien nos recordó que el 12 de mayo es San Pancracio. Cuando digo que soy creyente, ayer me hicieron llegar una imagen de San Pancracio, porque el 10 de mayo vive San Pancracio. San Pancracio es un buen santo. Es un buen santo, sí, señor. El patrón de la suerte, aunque el propio Pancracio no tuviera mucha suerte, porque fue decapitado con 15 años, criatura. Pero la fortuna es verdad, sonría Salvador Illa, que cuenta en una entrevista que reza un padre nuestro cada día por la mañana. Y otra oración de creación propia que no dice cuál es. La fortuna sonríe a Salvador Illa porque llega a la última semana de campaña electoral en Cataluña liderando muy cómodamente las encuestas y habiendo mejorado la estimación que tenía cuando las elecciones fueron convocadas. Con los últimos sondeos publicados en la mano, la victoria la tiene sobradamente amarrada. No hay un solo sondeo que le dé la victoria a otro que no sea Salvador Illa. Luego, ya que gobierne, pues va a depender, con los números que hoy tenemos, va a depender de Esquerra Republicana de Cataluña. Si está por la labor o no de investir a Salvador Illa, Esquerra y los comunes, los menguantes comunes de Yolanda Díaz. Sumar las encuestas le están dando entre cinco o seis diputados. Y a ver quién se va a convertir ahora en un partido residual en Cataluña. Que igual es sumar que ganó las elecciones generales. Bueno, quedó segundo, perdón, después del PSC, quedó segundo, creo recordar. Pero que ahora podría quedar uno, dos, tres, cuatro, sexto. Por detrás del PP y por detrás de Vox. Bueno, el domingo estaremos aquí para contárselo, por cierto, en un programa especial, como siempre, de los informativos de esta casa. El PP sigue martilleando con su convicción, o presunta convicción, de que Sánchez hará presidente a Puigdemont, de una manera o de otra. Porque, dicen el PP, sus carreras políticas discurren paralelas y, por tanto, pues seguro que si tiene que elegir Sánchez, pues será el presidente Puigdemont y no Salvador Illa. Porque, como decía el PP, hace unos meses que el sanchismo declinaba y ahí está Sánchez. Encantado de haberse conocido y de haber tenido al país en vilo, su partido incluido, con una carta durante cinco días. Claro, ¿quién le iba a decir a Salvador Illa hace solo cuatro años, cuando era el segundo de Miquel Iceta en el PSC? Del que decían de Illa que tenía un perfil más gris que otra cosa. Cuando le tocó hace cuatro años, en la pedrea del gobierno de coalición, una cartera por la que nadie peleaba, que era Sanidad. Fruto de la atomización del Ministerio de Asuntos Sociales. Hasta entonces, una mujer, la señora Garcedo, había dirigido Asuntos Sociales, Sanidad y Consumo. Y desde el nacimiento del gobierno de coalición se repartió entre tres hombres. Pablo Iglesias, Alberto Garzón y un tal Salvador Illa, que era la cuota socialista catalana. El ministro de turno, que siempre corresponde al PSC. Por lo menos uno cuando gobierna el PSOE. Bueno, la pandemia cambió todo. El tal Salvador Illa se convirtió en una de las juradas más conocidas de España. Cataluña, naturalmente, incluida. Y el domingo, si nada cambia, será el aspirante con más papeletas para heredar a Per Aragonés al frente de la presidencia de la Generalitat de Cataluña. Digo, con los números que hoy tenemos encima de la mesa. Queda una semana. Habría de producirse un terremoto para que las expectativas se dieran la vuelta. Un terremoto como el que, acuérdese, temían algunos socialistas hace unos meses. Cuando se paliciaban que pudiera el señor Puigdemont dar un golpe de efecto. Que cuando quedasen, que te digo yo, dos días, tres días para las urnas, o sea ya con la campaña electoral en su fase finalísima, se presentara Puigdemont en la frontera y cruzara la frontera con sus apóstoles para forzar su detención y su encarcelamiento. Golpe de efecto supremo, ¿no? Ríete tú de la pájara falsa de Pedro Sánchez. Hombre, con Puigdemont cualquier cosa puede suceder, pero a la frontera por ahora no se acerca. No parece que vaya a haber golpe de efecto, pero está por ver. Y al líder de la UGT en Cataluña y candidato del PSC que el viernes hizo huasa con la fuga de Puigdemont de hace siete años, lo han retirado ya sus compañeros de partido del escaparate para que no reincida en esto que el viernes dijo sobre el prófugo. El otro día lo escuchaba y dijo, es que el presidente Pedro Sánchez tiene que venir llorado y esto que ha hecho es una indecencia. Llorado se fue en el maletero y no sé si cagado o meado, pero se fue hasta Bruselas. Bueno, el PSC se ha disculpado con quien haya podido sentirse ofendido por estas palabras, que se entiende que es Puigdemont, claro. Que mire, más que ofendido está encantado con esto, porque está encantado de que le den la oportunidad de repetir que él nunca se metió en el maletero de aquel coche. Que eso es un bulo creado por el Estado español, por las cloacas del Estado. Es un bulo que ahora está aireando otra vez el partido socialista mientras Sánchez se revuelve contra los bulos que le afectan a él. Los que le afectan a los demás, mira cómo los propaga. Y en rigor lo más duro que dijo este socialista de UGT, Matías Carnero, el viernes, lo más duro desde el punto de vista de un socialista, no fue lo del cagado y meado de Puigdemont, sino que Puigdemont es la derecha y punto. Iba sentado en un coche, por cierto un Volvo, el que quiere padre y país, con un Volvo, está muy bien. ¿Por qué no se hizo la foto en el maletero? Claro, es que nos lo ponen a huevo para poder darles caña, porque ahora se ponen de defensores de la clase trabajadores cuando es la derecha catalana, es la derecha catalana, y ya está. Y ya está. Es la derecha catalana a la que ya lo siento Matías Carnero, pero su partido en Madrid no le da caña, más bien al contrario. Porque a la derecha catalana le debe el presidente Sánchez ser presidente, su última investidura. Es que Sánchez es presidente por la gracia de Garras Puigdemont, es decir, por la gracia de la derecha catalana y ya está. Previa promesa de una amnistía, que la semana que viene terminará de aprobar, primero la rechazará el Senado y luego irá al Congreso para que allí sea definitivamente aprobada. Hoy, por cierto, se reúne la comisión que creó el Senado para estudiar la ley de amnistía, el Congreso aprobó la ley y se la envió al Senado. El Senado reúne hoy a esta comisión conjunta, comisión constitucional, comisión de justicia, y allí va a presentar hoy el último informe que han hecho los letrados, en concreto el letrado asignado a esta comisión, cuya conclusión, la de este informe son 60 folios, ya podemos avanzar que la conclusión, según este letrado, es que la ley de amnistía sufre de graves indicios de ser contraria a la Constitución y contraria al derecho de la Unión Europea, y que en lo que se refiere a los efectos prácticos, caso de que un juez presentara una consulta ante el Tribunal Europeo porque entendiera que esta ley contraviene los principios de la legislación europea, esto que se llama la Cuestión Prejudicial, entonces, según el criterio de este letrado, no solo dejaría de aplicarse la norma, la ley, hasta que el Tribunal Europeo resolviera la Cuestión Prejudicial, sino que permanecerían también las medidas cautelares. A pesar, acuérdense, todo el empeño que puso Junts per Cataluña en la última negociación que tuvo con el Gobierno para asegurarse de que publicada esta ley en el Boletín Oficial del Estado, todas las medidas cautelares decaerían. Bueno, el Senado también es hoy noticia porque se persona a las 11 de la mañana un apestado. Soy consciente, tengo ya alguna edad, y sé lo que es un apestado político. El señor Ábalos, el mismo dijo el día que se negó a entregar el escaño, que sería un apestado si renunciara a ser diputado porque todo el mundo interpretaría que estaba asumiendo su culpabilidad. En realidad, asumiéndola o no asumiéndola, ha sido desde entonces un apestado, por lo menos para el partido del que fue secretario de organización. Y para el gobierno del que fue ministro de fomento e integrante del núcleo duro, el núcleo durísimo de Pedro Sánchez. Compares esa mañana a Ábalos en la comisión de investigación del Senado, por la que ya pasó Coldo, sin abrir apenas el pico. Por la que ya pasó Salvador Illa, sin que sus interrogadores consiguieran despeinarle. Y por la que Sánchez da por hecho que acabará pasando su esposa, Begoña Gómez. Interesante, por cierto, lo de Sánchez este domingo en El País. Entrevistado por la directora del periódico. Interesante porque confirma el presidente que su esposa firmó dos cartas de apoyo a la consultora del señor Barrabés. Tal como publicó el confidencial, solo que él dice que no son cartas de recomendación, sino declaraciones de interés. Hombre, en el encabezado de las dos cartas se dice declaración de interés y apoyo. Y a la vez que el presidente confirma que esta información es correcta, añade luego que las cartas de su esposa no tuvieron influencia en la decisión final y por eso lo llama a todo bulo. No fue una carta de recomendación, fue una declaración de interés. Una declaración de interés, por cierto, junto con otras 32 declaraciones de intereses de empresas, de instituciones. Esto lo aclaró a estos eudomedios digitales la empresa pública Red.es. Efectivamente, ustedes hicieron un análisis exhaustivo de cómo efectivamente se había propagado un bulo basado en desinformación. Y a partir de ahí, pues nos encontramos con la judicialización. Observemos que el presidente atribuye al diario El País a haber examinado la propagación del bulo. Pero ocurre que El País nunca llamó bulo a las cartas de recomendación, que por cierto describió así como recomendaciones recopiladas por los aspirantes a esos concursos. Y cuando la directora del periódico le plantea al presidente si él habría animado a su esposa a firmar esas cartas de haberle consultado ella, el presidente no responde a la pregunta. Pero vamos a ver Pepa, que no incidió, que es una declaración de interés como han firmado otras muchas instituciones, el Ayuntamiento de Madrid, por ejemplo, o también empresas. No incidió en la adjudicación final, en el resultado del concurso, pero sí las firmó entonces. O sea que no hubo bulo, queda claro que no hubo bulo. Si repite Pedro Sánchez en la entrevista, está en el vídeo, aunque no está en la transcripción impresa, si repite que Feijó ha dicho que Begoña Gómez tiene que quedarse en su casa y no trabajar. Yo he escuchado al señor Feijó decir efectivamente que lo mejor que podría hacer mi mujer para tener que evitar todo este tipo de situaciones es no trabajar. Cuando la entrevistadora le objeta que Feijó no ha dicho eso, entonces cambia el presidente y le atribuye lo que sí dijo, que es que ha habido esposas de presidentes que se han quedado en su casa. Y qué más, bueno, sostiene el presidente Sánchez que después de la carta del miércoles 24 de abril decidió permanecer en el cargo la noche del sábado y que si no lo comunicó hasta la mañana del lunes, ni siquiera su equipo, fue porque tenía que escribir. No la prolongo, simplemente necesito el tiempo para poder escribir. Y en efecto el lunes lo que hice fue estar en Zarzuela, informar al jefe del estado, al rey Felipe VI sobre mi decisión y posteriormente comparecer ante ustedes, ante los medios de comunicación para informar sobre mi decisión. Bueno, el presidente no compareció ante ningún medio de comunicación. Reincide en esto que él sabe que es falso porque él leyó una declaración del presidente Sánchez que dijo que el presidente Sánchez tenía que estar en su casa. No compareció ante una cámara del palacio de la Moncloa, punto. Eso no es comparecer ante los medios de comunicación. Es comparecer ante una cámara cuya señal luego se distribuye a los medios de comunicación. ¿Pudieron preguntarle algo los periodistas que según el presidente estaban allí asistiendo a su comparecencia en los jardines del palacio de la Moncloa? Pues no. Bueno, a día de hoy todavía no han podido hacerlo. Salvo los cuatro periodistas que han entrevistado estos últimos días al presidente. El resto no ha podido preguntarle nada. Se interesó Pepa Bueno por la situación emocional del presidente Sánchez en sus cinco días de reflexión. Si es verdad que no comía, que dormía mal. Y fue Sánchez quien quiso afirmar, a cuento de debates que según él se han abierto a raíz de su carta, fue Sánchez quien quiso afirmar que él no padece lo que llama la enfermedad de salud mental. Yo respeto mucho a las personas que sufren esa enfermedad de salud mental. Tengo que decirle que no es mi caso. Mi caso es un tema de salud democrática. La salud democrática que pasa por acabar con los insultos en política, con la deshumanización, con el bulo, con el juego sucio. Seguramente por eso el ministro Óscar Puente ha atribuido al presidente de la Argentina a ingerir sustancias. Para acabar con el juego sucio y abrir camino al escrupuloso respeto a quien comulga con una ideología diferente a la tuya.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file= open(\"./Alsina.mp3\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transcription to a file\n",
    "with open(\"transcription.txt\", \"w\") as file:\n",
    "    file.write(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resúmenes**\n",
    "\n",
    "¿Cómo se puede resumir un texto que es mayor que el contexto del LLM? Si el texto a resumir es mayor que el contexto máximo permitido por el modelo de lenguaje, puedes utilizar una técnica llamada \"resumen recursivo\" o \"resumen jerárquico\". Esta técnica consiste en dividir el texto en fragmentos más pequeños, resumir cada fragmento por separado y luego combinar los resúmenes en un resumen final. Hay que ajustar los valores de `chunk_size` y `chunk_overlap` según las necesidades y limitaciones del modelo de lenguaje que estés utilizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate que utilizamos la función `RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)` para dividir el texto en fragmentos de 1000 caracteres con un solapamiento de 50 caracteres. Luego, utilizamos el modelo de lenguaje para generar un resumen de cada fragmento y finalmente combinamos los resúmenes en un resumen final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcription.txt\") as f:\n",
    "    texto = f.read()\n",
    "    \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "textos = text_splitter.split_text(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=t) for t in textos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI( \n",
    "    temperature=0\n",
    ")\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\") # max_tokens=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_documents': [Document(page_content='¿Qué tal? ¿Cómo están? Bienvenidos a una nueva mañana de radio. Estamos el 6 de mayo de 2024, a seis días de San Pancracio. Seis días de que San Pancracio reparta suerte en las elecciones catalanas. Salvador Illa fue el primero en invocar el martirologio cristiano en este programa. Fue él quien nos recordó que el 12 de mayo es San Pancracio. Cuando digo que soy creyente, ayer me hicieron llegar una imagen de San Pancracio, porque el 10 de mayo vive San Pancracio. San Pancracio es un buen santo. Es un buen santo, sí, señor. El patrón de la suerte, aunque el propio Pancracio no tuviera mucha suerte, porque fue decapitado con 15 años, criatura. Pero la fortuna es verdad, sonría Salvador Illa, que cuenta en una entrevista que reza un padre nuestro cada día por la mañana. Y otra oración de creación propia que no dice cuál es. La fortuna sonríe a Salvador Illa porque llega a la última semana de campaña electoral en Cataluña liderando muy cómodamente las encuestas y habiendo mejorado la'), Document(page_content='cómodamente las encuestas y habiendo mejorado la estimación que tenía cuando las elecciones fueron convocadas. Con los últimos sondeos publicados en la mano, la victoria la tiene sobradamente amarrada. No hay un solo sondeo que le dé la victoria a otro que no sea Salvador Illa. Luego, ya que gobierne, pues va a depender, con los números que hoy tenemos, va a depender de Esquerra Republicana de Cataluña. Si está por la labor o no de investir a Salvador Illa, Esquerra y los comunes, los menguantes comunes de Yolanda Díaz. Sumar las encuestas le están dando entre cinco o seis diputados. Y a ver quién se va a convertir ahora en un partido residual en Cataluña. Que igual es sumar que ganó las elecciones generales. Bueno, quedó segundo, perdón, después del PSC, quedó segundo, creo recordar. Pero que ahora podría quedar uno, dos, tres, cuatro, sexto. Por detrás del PP y por detrás de Vox. Bueno, el domingo estaremos aquí para contárselo, por cierto, en un programa especial, como siempre, de'), Document(page_content='cierto, en un programa especial, como siempre, de los informativos de esta casa. El PP sigue martilleando con su convicción, o presunta convicción, de que Sánchez hará presidente a Puigdemont, de una manera o de otra. Porque, dicen el PP, sus carreras políticas discurren paralelas y, por tanto, pues seguro que si tiene que elegir Sánchez, pues será el presidente Puigdemont y no Salvador Illa. Porque, como decía el PP, hace unos meses que el sanchismo declinaba y ahí está Sánchez. Encantado de haberse conocido y de haber tenido al país en vilo, su partido incluido, con una carta durante cinco días. Claro, ¿quién le iba a decir a Salvador Illa hace solo cuatro años, cuando era el segundo de Miquel Iceta en el PSC? Del que decían de Illa que tenía un perfil más gris que otra cosa. Cuando le tocó hace cuatro años, en la pedrea del gobierno de coalición, una cartera por la que nadie peleaba, que era Sanidad. Fruto de la atomización del Ministerio de Asuntos Sociales. Hasta entonces, una'), Document(page_content='de Asuntos Sociales. Hasta entonces, una mujer, la señora Garcedo, había dirigido Asuntos Sociales, Sanidad y Consumo. Y desde el nacimiento del gobierno de coalición se repartió entre tres hombres. Pablo Iglesias, Alberto Garzón y un tal Salvador Illa, que era la cuota socialista catalana. El ministro de turno, que siempre corresponde al PSC. Por lo menos uno cuando gobierna el PSOE. Bueno, la pandemia cambió todo. El tal Salvador Illa se convirtió en una de las juradas más conocidas de España. Cataluña, naturalmente, incluida. Y el domingo, si nada cambia, será el aspirante con más papeletas para heredar a Per Aragonés al frente de la presidencia de la Generalitat de Cataluña. Digo, con los números que hoy tenemos encima de la mesa. Queda una semana. Habría de producirse un terremoto para que las expectativas se dieran la vuelta. Un terremoto como el que, acuérdese, temían algunos socialistas hace unos meses. Cuando se paliciaban que pudiera el señor Puigdemont dar un golpe de'), Document(page_content='que pudiera el señor Puigdemont dar un golpe de efecto. Que cuando quedasen, que te digo yo, dos días, tres días para las urnas, o sea ya con la campaña electoral en su fase finalísima, se presentara Puigdemont en la frontera y cruzara la frontera con sus apóstoles para forzar su detención y su encarcelamiento. Golpe de efecto supremo, ¿no? Ríete tú de la pájara falsa de Pedro Sánchez. Hombre, con Puigdemont cualquier cosa puede suceder, pero a la frontera por ahora no se acerca. No parece que vaya a haber golpe de efecto, pero está por ver. Y al líder de la UGT en Cataluña y candidato del PSC que el viernes hizo huasa con la fuga de Puigdemont de hace siete años, lo han retirado ya sus compañeros de partido del escaparate para que no reincida en esto que el viernes dijo sobre el prófugo. El otro día lo escuchaba y dijo, es que el presidente Pedro Sánchez tiene que venir llorado y esto que ha hecho es una indecencia. Llorado se fue en el maletero y no sé si cagado o meado, pero se fue'), Document(page_content='maletero y no sé si cagado o meado, pero se fue hasta Bruselas. Bueno, el PSC se ha disculpado con quien haya podido sentirse ofendido por estas palabras, que se entiende que es Puigdemont, claro. Que mire, más que ofendido está encantado con esto, porque está encantado de que le den la oportunidad de repetir que él nunca se metió en el maletero de aquel coche. Que eso es un bulo creado por el Estado español, por las cloacas del Estado. Es un bulo que ahora está aireando otra vez el partido socialista mientras Sánchez se revuelve contra los bulos que le afectan a él. Los que le afectan a los demás, mira cómo los propaga. Y en rigor lo más duro que dijo este socialista de UGT, Matías Carnero, el viernes, lo más duro desde el punto de vista de un socialista, no fue lo del cagado y meado de Puigdemont, sino que Puigdemont es la derecha y punto. Iba sentado en un coche, por cierto un Volvo, el que quiere padre y país, con un Volvo, está muy bien. ¿Por qué no se hizo la foto en el'), Document(page_content='está muy bien. ¿Por qué no se hizo la foto en el maletero? Claro, es que nos lo ponen a huevo para poder darles caña, porque ahora se ponen de defensores de la clase trabajadores cuando es la derecha catalana, es la derecha catalana, y ya está. Y ya está. Es la derecha catalana a la que ya lo siento Matías Carnero, pero su partido en Madrid no le da caña, más bien al contrario. Porque a la derecha catalana le debe el presidente Sánchez ser presidente, su última investidura. Es que Sánchez es presidente por la gracia de Garras Puigdemont, es decir, por la gracia de la derecha catalana y ya está. Previa promesa de una amnistía, que la semana que viene terminará de aprobar, primero la rechazará el Senado y luego irá al Congreso para que allí sea definitivamente aprobada. Hoy, por cierto, se reúne la comisión que creó el Senado para estudiar la ley de amnistía, el Congreso aprobó la ley y se la envió al Senado. El Senado reúne hoy a esta comisión conjunta, comisión constitucional,'), Document(page_content='esta comisión conjunta, comisión constitucional, comisión de justicia, y allí va a presentar hoy el último informe que han hecho los letrados, en concreto el letrado asignado a esta comisión, cuya conclusión, la de este informe son 60 folios, ya podemos avanzar que la conclusión, según este letrado, es que la ley de amnistía sufre de graves indicios de ser contraria a la Constitución y contraria al derecho de la Unión Europea, y que en lo que se refiere a los efectos prácticos, caso de que un juez presentara una consulta ante el Tribunal Europeo porque entendiera que esta ley contraviene los principios de la legislación europea, esto que se llama la Cuestión Prejudicial, entonces, según el criterio de este letrado, no solo dejaría de aplicarse la norma, la ley, hasta que el Tribunal Europeo resolviera la Cuestión Prejudicial, sino que permanecerían también las medidas cautelares. A pesar, acuérdense, todo el empeño que puso Junts per Cataluña en la última negociación que tuvo con el'), Document(page_content='Cataluña en la última negociación que tuvo con el Gobierno para asegurarse de que publicada esta ley en el Boletín Oficial del Estado, todas las medidas cautelares decaerían. Bueno, el Senado también es hoy noticia porque se persona a las 11 de la mañana un apestado. Soy consciente, tengo ya alguna edad, y sé lo que es un apestado político. El señor Ábalos, el mismo dijo el día que se negó a entregar el escaño, que sería un apestado si renunciara a ser diputado porque todo el mundo interpretaría que estaba asumiendo su culpabilidad. En realidad, asumiéndola o no asumiéndola, ha sido desde entonces un apestado, por lo menos para el partido del que fue secretario de organización. Y para el gobierno del que fue ministro de fomento e integrante del núcleo duro, el núcleo durísimo de Pedro Sánchez. Compares esa mañana a Ábalos en la comisión de investigación del Senado, por la que ya pasó Coldo, sin abrir apenas el pico. Por la que ya pasó Salvador Illa, sin que sus interrogadores'), Document(page_content='ya pasó Salvador Illa, sin que sus interrogadores consiguieran despeinarle. Y por la que Sánchez da por hecho que acabará pasando su esposa, Begoña Gómez. Interesante, por cierto, lo de Sánchez este domingo en El País. Entrevistado por la directora del periódico. Interesante porque confirma el presidente que su esposa firmó dos cartas de apoyo a la consultora del señor Barrabés. Tal como publicó el confidencial, solo que él dice que no son cartas de recomendación, sino declaraciones de interés. Hombre, en el encabezado de las dos cartas se dice declaración de interés y apoyo. Y a la vez que el presidente confirma que esta información es correcta, añade luego que las cartas de su esposa no tuvieron influencia en la decisión final y por eso lo llama a todo bulo. No fue una carta de recomendación, fue una declaración de interés. Una declaración de interés, por cierto, junto con otras 32 declaraciones de intereses de empresas, de instituciones. Esto lo aclaró a estos eudomedios digitales'), Document(page_content='Esto lo aclaró a estos eudomedios digitales la empresa pública Red.es. Efectivamente, ustedes hicieron un análisis exhaustivo de cómo efectivamente se había propagado un bulo basado en desinformación. Y a partir de ahí, pues nos encontramos con la judicialización. Observemos que el presidente atribuye al diario El País a haber examinado la propagación del bulo. Pero ocurre que El País nunca llamó bulo a las cartas de recomendación, que por cierto describió así como recomendaciones recopiladas por los aspirantes a esos concursos. Y cuando la directora del periódico le plantea al presidente si él habría animado a su esposa a firmar esas cartas de haberle consultado ella, el presidente no responde a la pregunta. Pero vamos a ver Pepa, que no incidió, que es una declaración de interés como han firmado otras muchas instituciones, el Ayuntamiento de Madrid, por ejemplo, o también empresas. No incidió en la adjudicación final, en el resultado del concurso, pero sí las firmó entonces. O sea'), Document(page_content='del concurso, pero sí las firmó entonces. O sea que no hubo bulo, queda claro que no hubo bulo. Si repite Pedro Sánchez en la entrevista, está en el vídeo, aunque no está en la transcripción impresa, si repite que Feijó ha dicho que Begoña Gómez tiene que quedarse en su casa y no trabajar. Yo he escuchado al señor Feijó decir efectivamente que lo mejor que podría hacer mi mujer para tener que evitar todo este tipo de situaciones es no trabajar. Cuando la entrevistadora le objeta que Feijó no ha dicho eso, entonces cambia el presidente y le atribuye lo que sí dijo, que es que ha habido esposas de presidentes que se han quedado en su casa. Y qué más, bueno, sostiene el presidente Sánchez que después de la carta del miércoles 24 de abril decidió permanecer en el cargo la noche del sábado y que si no lo comunicó hasta la mañana del lunes, ni siquiera su equipo, fue porque tenía que escribir. No la prolongo, simplemente necesito el tiempo para poder escribir. Y en efecto el lunes lo que'), Document(page_content='para poder escribir. Y en efecto el lunes lo que hice fue estar en Zarzuela, informar al jefe del estado, al rey Felipe VI sobre mi decisión y posteriormente comparecer ante ustedes, ante los medios de comunicación para informar sobre mi decisión. Bueno, el presidente no compareció ante ningún medio de comunicación. Reincide en esto que él sabe que es falso porque él leyó una declaración del presidente Sánchez que dijo que el presidente Sánchez tenía que estar en su casa. No compareció ante una cámara del palacio de la Moncloa, punto. Eso no es comparecer ante los medios de comunicación. Es comparecer ante una cámara cuya señal luego se distribuye a los medios de comunicación. ¿Pudieron preguntarle algo los periodistas que según el presidente estaban allí asistiendo a su comparecencia en los jardines del palacio de la Moncloa? Pues no. Bueno, a día de hoy todavía no han podido hacerlo. Salvo los cuatro periodistas que han entrevistado estos últimos días al presidente. El resto no ha'), Document(page_content='estos últimos días al presidente. El resto no ha podido preguntarle nada. Se interesó Pepa Bueno por la situación emocional del presidente Sánchez en sus cinco días de reflexión. Si es verdad que no comía, que dormía mal. Y fue Sánchez quien quiso afirmar, a cuento de debates que según él se han abierto a raíz de su carta, fue Sánchez quien quiso afirmar que él no padece lo que llama la enfermedad de salud mental. Yo respeto mucho a las personas que sufren esa enfermedad de salud mental. Tengo que decirle que no es mi caso. Mi caso es un tema de salud democrática. La salud democrática que pasa por acabar con los insultos en política, con la deshumanización, con el bulo, con el juego sucio. Seguramente por eso el ministro Óscar Puente ha atribuido al presidente de la Argentina a ingerir sustancias. Para acabar con el juego sucio y abrir camino al escrupuloso respeto a quien comulga con una ideología diferente a la tuya.')], 'output_text': \" The article discusses the upcoming Catalan elections and the role of Saint Pancras in bringing luck. It also mentions the leading candidate, Salvador Illa, and his rise in the polls. The possibility of a political earthquake and former Catalan leader Carles Puigdemont making a dramatic move before the elections is also discussed. The article also covers the controversy surrounding Illa's party and their derogatory comments about Puigdemont. It also mentions the upcoming approval of an amnesty law and the controversy surrounding a statement made by Spanish President Pedro Sánchez. The article concludes with a discussion about the president's emotional state and the need for respectful treatment in politics.\"}\n"
     ]
    }
   ],
   "source": [
    "resumen = chain.invoke(docs)\n",
    "print(resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nos ha hecho el resumen en inglés. Si queremos que nos lo haga en español, podemos utilizar una plantilla que especifique el idioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hoy es 6 de mayo de 2024 y faltan seis días para las elecciones catalanas. Se dice que San Pancracio traerá suerte y Salvador Illa, quien es creyente, menciona que el 12 de mayo es su día. A pesar de haber sido decapitado a los 15 años, es considerado el patrón de la suerte. Illa reza cada mañana y tiene una oración para la fortuna. Actualmente, lidera las encuestas en la última semana de campaña en Cataluña.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Resuma el siguiente texto en español:\n",
    "\n",
    "{text}\n",
    "\n",
    "Resumen:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=prompt, combine_prompt=prompt)\n",
    "\n",
    "text = docs[0]\n",
    "summary = chain.invoke({\"input_documents\": [text]}, return_only_outputs=True)\n",
    "print(summary[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros `map_prompt` y `combine_prompt` en la función `load_summarize_chain` se utilizan para especificar los prompts que se utilizarán en las etapas de mapeo y combinación de la cadena de resumen, respectivamente.\n",
    "\n",
    "`map_prompt`: Este parámetro se utiliza para especificar el prompt que se aplicará a cada documento individual durante la etapa de mapeo. En la etapa de mapeo, el modelo de lenguaje procesa cada documento por separado y genera un resumen intermedio para cada uno de ellos. Al establecer map_prompt=PROMPT, estás indicando que se utilice el prompt definido en la variable PROMPT para generar los resúmenes intermedios de cada documento.\n",
    "\n",
    "`combine_prompt`: Este parámetro se utiliza para especificar el prompt que se aplicará durante la etapa de combinación. En la etapa de combinación, los resúmenes intermedios generados en la etapa de mapeo se combinan en un resumen final. Al establecer combine_prompt=PROMPT, estás indicando que se utilice el mismo prompt definido en la variable PROMPT para generar el resumen final a partir de los resúmenes intermedios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()\n",
    "search.run(\"manchester united vs luton town match summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Búsqueda de información en Google**\n",
    "https://python.langchain.com/v0.1/docs/integrations/platforms/google\n",
    "\n",
    "Veamos cómo podemos buscar información en Google y obtener respuestas a nuestras preguntas utilizando la API de Google Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "def top5_results(query):\n",
    "    return search.results(query, num_results=5)\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=top5_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tool.run(\"¿Qué es una calculadora?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculadora - Wikipedia, la enciclopedia libre\n",
      "https://es.wikipedia.org/wiki/Calculadora\n",
      "Una calculadora es un dispositivo que se utiliza para realizar cálculos aritméticos. Aunque las calculadoras modernas se incorporan a menudo en un ordenador ...\n",
      "\n",
      "¿Qué es una calculadora y para qué sirve? - Definición\n",
      "https://www.geeknetic.es/Calculadora/que-es-y-para-que-sirve\n",
      "Dec 6, 2020 ... La calculadora científica sirve para realizar, prácticamente, cualquier tipo de cálculos, con ella podemos calcular superficies, sistemas de ...\n",
      "\n",
      "Calculadora - Aplicaciones en Google Play\n",
      "https://play.google.com/store/apps/details?id=com.google.android.calculator&hl=es\n",
      "Con la Calculadora puedes hacer operaciones matemáticas sencillas y avanzadas en una aplicación con un diseño atractivo. • Haz operaciones básicas como ...\n",
      "\n",
      "Calculadora de BMI\n",
      "https://www.nhlbi.nih.gov/health/educational/lose_wt/BMI/bmi-m_sp.htm\n",
      "El Indice de Masa Corporal (IMC) mide el contenido de grasa corporal en relación a la estatura y el peso que presentan tanto los hombres como las mujeres. Anote ...\n",
      "\n",
      "¿Qué es una calculadora secreta y cómo puedes detectarla ...\n",
      "https://www.qustodio.com/es/blog/como-detectar-una-aplicacion-de-calculadora-secreta/\n",
      "Feb 16, 2022 ... Cómo detectar una aplicación de calculadora secreta · 1. Comprueba el tamaño que ocupa la aplicación en la memoria · 2. Comprueba cuántas ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in res:\n",
    "    print(r[\"title\"])\n",
    "    print(r[\"link\"])\n",
    "    print(r[\"snippet\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'es.wikipedia.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.geeknetic.es'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'play.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.nhlbi.nih.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.qustodio.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importa beautifulsoup4 y requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Extrae el contenido de la página\n",
    "def extract_content(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Preparamos todo el contenido de las páginas como una lista de documentos\n",
    "def prepare_documents(results):\n",
    "    documents = []\n",
    "    for result in results:\n",
    "        url = result[\"link\"]\n",
    "        content = extract_content(url)\n",
    "        documents.append(content)\n",
    "    return documents\n",
    "\n",
    "# Extraemos el contenido de las páginas\n",
    "documents = prepare_documents(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "Prepara los documentos extraídos de la web para pasarlos como fuente de información al modelo de lenguaje y obterner respuestas a preguntas dadas por el usuario.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Agentes**\n",
    "\n",
    "En LangChain, los agentes son un componente clave que permite crear sistemas capaces de realizar tareas complejas combinando múltiples herramientas y habilidades.\n",
    "\n",
    "Algunas características importantes de los agentes en LangChain:\n",
    "\n",
    "- Pueden acceder a una variedad de herramientas, como búsquedas web, calculadoras, bases de conocimiento, etc. Esto les permite obtener información y realizar acciones.\n",
    "\n",
    "- Tienen una cadena de pensamiento que determina qué herramientas usar y en qué orden para completar una tarea dada. Esta cadena se construye usando prompts y modelos de lenguaje.\n",
    "\n",
    "- Pueden iterar, es decir, usar la salida de una herramienta como entrada para la siguiente hasta lograr el objetivo.\n",
    "\n",
    "- Permiten abstraer la complejidad, ya que el usuario sólo necesita proporcionar el objetivo de alto nivel y el agente determina los pasos necesarios.\n",
    "\n",
    "- Son muy flexibles y extensibles. Se pueden agregar fácilmente nuevas herramientas y adaptar los agentes a diferentes dominios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should greet Juan.\n",
      "Action: Saludo\n",
      "Action Input: Juan\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m¡Hola Juan!\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should wait for Juan to respond.\n",
      "Action: Saludo\n",
      "Action Input: Juan\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m¡Hola Juan!\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should listen to Juan's response.\n",
      "Action: Saludo\n",
      "Action Input: Juan\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m¡Hola Juan!\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should respond to Juan's greeting.\n",
      "Action: Saludo\n",
      "Action Input: Juan\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m¡Hola Juan!\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: ¡Hola Juan!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "¡Hola Juan!\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "# Definir una herramienta simple\n",
    "class SaludoTool(BaseTool):\n",
    "    name = \"Saludo\"\n",
    "    description = \"Herramienta para saludar a alguien\"\n",
    "\n",
    "    def _run(self, nombre: str) -> str:\n",
    "        return f\"¡Hola {nombre}!\"\n",
    "\n",
    "    def _arun(self, nombre: str) -> str:\n",
    "        raise NotImplementedError(\"SaludoTool no soporta ejecución asíncrona\")\n",
    "\n",
    "# Configurar el modelo de lenguaje (en este caso, OpenAI)\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Inicializar el agente con la herramienta y el modelo de lenguaje\n",
    "tools = [SaludoTool()]\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "# Ejecutar el agente con una entrada\n",
    "entrada = \"Saluda a Juan.\"\n",
    "# entrada = \"Saluda a Juan. También saludo a María.\"\n",
    "# entrada = \"Saluda a Juan. También saludo a María. A Pedro no hace falta que lo saludes.\"\n",
    "resultado = agent.run(entrada)\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Saludo: Herramienta para saludar a alguien\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Saludo]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m We need to check the current time and compare it to the store's opening and closing hours.\n",
      "Action: Hora actual\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m14:14\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m We can see that it is currently 14:14, which falls within the store's opening and closing hours.\n",
      "Action: Hora actual\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m14:14\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m We can also see that the store is open for 10 hours, from 10:00 to 20:00.\n",
      "Action: Hora actual\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m14:14\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Therefore, the store is currently open.\n",
      "Final Answer: Yes, the store is currently open.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, the store is currently open.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import Tool\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "\n",
    "# Configurar el modelo de lenguaje (en este caso, OpenAI)\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Definir las herramientas que el agente puede utilizar\n",
    "\n",
    "#search = DuckDuckGoSearchRun()\n",
    "wikipedia_retriever = WikipediaRetriever()\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Searches Wikipedia for a query.\"\"\"\n",
    "    return wikipedia_retriever.invoke(query)\n",
    "\n",
    "@tool\n",
    "def current_time(input: str) -> str:\n",
    "    \"\"\"Returns the current time.\"\"\"\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    # return \"11:30\"\n",
    "    return now.strftime(\"%H:%M\")\n",
    "\n",
    "@tool\n",
    "def convert_number_to_binary(number: str) -> str:\n",
    "    \"\"\"Converts a number to binay.\"\"\"\n",
    "    return bin(int(number))[2:]\n",
    "\n",
    "\n",
    "# Inicializar el agente con la herramienta y el modelo de lenguaje\n",
    "tools = [\n",
    "    # Tool(\n",
    "    #     name=\"Búsqueda en Internet\",\n",
    "    #     func=search.run,\n",
    "    #     description=\"Busca información en Internet utilizando DuckDuckGo\"\n",
    "    # ),\n",
    "    # Tool(\n",
    "    #     name=\"Longitud de una palabra\",\n",
    "    #     func=get_word_length,\n",
    "    #     description=\"Devuelve la longitud de una palabra\"\n",
    "    # ),\n",
    "    Tool(\n",
    "        name=\"Búsqueda en Wikipedia\",\n",
    "        func=search_wikipedia,\n",
    "        description=\"Busca información en Wikipedia\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Hora actual\",\n",
    "        func=current_time,\n",
    "        description=\"Devuelve la hora actual\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Conversión de números a binario\",\n",
    "        func=convert_number_to_binary,\n",
    "        description=\"Convierte un número a su representación binaria\"\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "# Ejecutar el agente con una entrada\n",
    "# entrada = \"¿Cuántos caracteres tiene la palabra verborrea?\"\n",
    "# entrada = \"¿Quién fue el primer presidente de Estados Unidos?\"\n",
    "# entrada = \"¿Cuántos caracteres tiene el nombre completo de la ULPGC?\"\n",
    "# entrada = \"¿Quién fue Benito Pérez Galdós?\"\n",
    "entrada = \"La tienda está abierta desde las 10:00 hasta las 20:00 horas. ¿Está abierta ahora?\"\n",
    "# entrada = \"Convierte a binario el año de nacimiento de Benito Pérez Galdós\"\n",
    "\n",
    "resultado = agent.run(entrada)\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Búsqueda en Wikipedia: Busca información en Wikipedia\n",
      "Hora actual: Devuelve la hora actual\n",
      "Conversión de números a binario: Convierte un número a su representación binaria\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Búsqueda en Wikipedia, Hora actual, Conversión de números a binario]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "etriever = WikipediaRetriever()\n",
    "docs = retriever.invoke(\"Benito Pérez Galdós\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    "  stream_options={\"return_intermediate_steps\": True}\n",
    ")\n",
    "\n",
    "for message in completion.choices[0].messages:\n",
    "    print(message.role, message.content)\n",
    "\n",
    "\n",
    "\n",
    "# print(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/hf-logo-with-title.svg\" width=\"500\">\n",
    "\n",
    "https://huggingface.co/docs/hub/index\n",
    "\n",
    "Hugging Face es una empresa enfocada en inteligencia artificial y aprendizaje automático, conocida principalmente por su biblioteca \"Transformers\" que proporciona modelos preentrenados para procesamiento de lenguaje natural y otras tareas. Ofrecen una plataforma colaborativa, el Hugging Face Hub, donde los usuarios pueden compartir y trabajar en modelos de IA, conjuntos de datos y aplicaciones. La compañía promueve la democratización y accesibilidad de la IA a través de soluciones de código abierto y ciencia abierta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8676056265830994}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"Yesterday my mother in law died, finally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, este **pipeline** selecciona un modelo preentrenado específico que ha sido ajustado para el análisis de sentimientos en inglés. El modelo se descarga y almacena en caché cuando creas el objeto clasificador. Si vuelves a ejecutar el comando, se utilizará el modelo almacenado en caché en lugar de descargar el modelo de nuevo. Vamos a cargar un modelo preentrenado para el análisis de sentimientos en multi-idoma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '2 stars', 'score': 0.4662599563598633}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_multi = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "classifier_multi(\"No me gusta esa playa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pasos principales cuando pasas algún texto a un pipeline son:\n",
    "\n",
    "1. El texto se preprocesa en un formato que el modelo puede entender.\n",
    "2. Las entradas preprocesadas se pasan al modelo.\n",
    "3. Las predicciones del modelo se post-procesan, para que puedas entenderlas.\n",
    "\n",
    "Algunas de los pipeline disponibles son:\n",
    "\n",
    "- Extracción de la representación vectorial de un texto\n",
    "- Rellenar-enmascaramientos\n",
    "- Reconocimiento de entidades nombradas (NER)\n",
    "- Preguntas y respuestas\n",
    "- Análisis de sentimientos\n",
    "- Resúmenes\n",
    "- Generación de texto\n",
    "- Traducción\n",
    "- Clasificación \"zero-shot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clasificación *zero-shot***\n",
    "\n",
    "Esta tarea puede ser muy desafiante ya que necesitamos clasificar textos que no han sido etiquetados. Este es un escenario común en proyectos del mundo real porque anotar texto suele ser un proceso que consume mucho tiempo y requiere experiencia en el dominio. Para este caso de uso, el *pipeline* de clasificación *zero-shot* es muy útil: te permite especificar qué etiquetas usar para la clasificación, por lo que no tienes que depender de las etiquetas del modelo preentrenado. Ya has visto cómo el modelo puede clasificar una oración como positiva o negativa, pero también puede clasificar el texto usando cualquier otro conjunto de etiquetas que consideres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library in Natural Language Processing',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8677826523780823, 0.09560420364141464, 0.03661316633224487]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library in Natural Language Processing\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reconocimiento de entidades nombradas**\n",
    "\n",
    "El reconocimiento de entidades nombradas (NER) es una tarea donde el modelo tiene que encontrar qué partes del texto de entrada corresponden a entidades como personas, ubicaciones u organizaciones. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9991522,\n",
       "  'word': 'Juan Pérez',\n",
       "  'start': 11,\n",
       "  'end': 21},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.80377835,\n",
       "  'word': 'University of Las Palmas',\n",
       "  'start': 40,\n",
       "  'end': 64},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.7861865,\n",
       "  'word': 'of Gran Canaria',\n",
       "  'start': 65,\n",
       "  'end': 80},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9997646,\n",
       "  'word': 'Spain',\n",
       "  'start': 84,\n",
       "  'end': 89}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Juan Pérez and I work at the University of Las Palmas of Gran Canaria in Spain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preguntas y respuestas**\n",
    "\n",
    "El *pipeline* de preguntas y respuestas responde preguntas utilizando información de un contexto dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9867210984230042, 'start': 99, 'end': 109, 'answer': 'colleagues'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"What are María Pérez and Juan Pérez?\",\n",
    "    context=\"Juan Pérez and María Pérez work at the University of Las Palmas of Gran Canaria in Spain. They are colleagues. María Pérez is 32 years old and Juan Pérez is 45.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resúmenes**\n",
    "\n",
    "El resumen es la tarea de reducir un texto a un texto más corto mientras se mantienen todos (o la mayoría) de los aspectos importantes mencionados en el texto. Aquí hay un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae178c74648d45c2934b3ab97097a6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b6ef20af0c49f283cc557de744061d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1aadbf621f4a5792f67b571590ac0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c82086ab7f64e87a0264a070879bfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ad18e5ae1946dabf5aca0b014733da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Speech to text**\n",
    "\n",
    "Conversión del habla en texto escrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' Bueno, esto es solamente una prueba a ver qué tal funciona el traductor de... bueno, no es un traductor realmente, es recoger el audio del ordenador y traducirlo a texto. Vamos a ver qué tal.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "t2s = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n",
    "t2s(\"data/voz.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Todo en uno: Transformer**\n",
    "\n",
    "Un Transformer es un modelo de aprendizaje profundo que se ha vuelto extremadamente popular en el campo del procesamiento del lenguaje natural (NLP). Fue introducido en el artículo \"Attention is All You Need\" por Vaswani et al. en 2017. Los Transformers eliminan la necesidad de métodos de aprendizaje supervisado previos, como las redes neuronales recurrentes (RNN) o las redes neuronales convolutivas (CNN), al introducir una nueva arquitectura llamada codificador-decodificador. Los Transformers han demostrado ser muy eficaces en una variedad de tareas de NLP, incluido el modelado de lenguaje, la traducción automática y el resumen de texto.\n",
    "\n",
    "El modelo T5 (que veremos en el ejemplo), o Text-to-Text Transfer Transformer, fue presentado en un artículo titulado \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" publicado por investigadores de Google en octubre de 2019. Este trabajo introdujo el modelo T5 junto con su enfoque innovador para manejar todas las tareas de procesamiento de lenguaje natural (NLP) como problemas de generación de texto, utilizando un formato de texto a texto.\n",
    "\n",
    "\n",
    "`T5ForConditionalGeneration` es una clase en la biblioteca transformers de Hugging Face que proporciona una implementación del modelo T5 diseñada específicamente para tareas de generación de texto, como traducción de idiomas, resumen de texto, y generación de texto basada en prompts. La idea detrás de T5 es tratar todos los problemas de procesamiento de lenguaje natural como problemas de generación de texto. Por ejemplo, en lugar de tener una arquitectura específica para traducción y otra diferente para resumen, T5 puede ser entrenado para realizar ambas tareas (y muchas otras) simplemente cambiando el texto de entrada que se le proporciona. Esto se logra prefijando el texto de entrada con un indicador de la tarea a realizar, como \"traducir del inglés al alemán\" o \"resumir\", seguido por el texto a procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Traducción**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglés a Francés: Les Beatles sont un groupe rock anglais formé à Liverpool en 1960, composé de John Lennon, Paul McCartney, George Harrison et Ringo Starr.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglés a Francés: Ils sont considérés comme le groupe le plus influent de tous les temps et ont joué un rôle essentiel dans le développement de la contreculture des années 1960 et la reconnaissance de la musique populaire comme forme d'art.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def translate(text, model_name=\"t5-base\", task=\"translate English to French\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Ejemplo de traducción del inglés al francés\n",
    "text_en_to_es = \"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr.\"\n",
    "print(\"Inglés a Francés:\", translate(text_en_to_es, task=\"translate English to French\"))\n",
    "text_en_to_es = \"They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\"\n",
    "print(\"Inglés a Francés:\", translate(text_en_to_es, task=\"translate English to French\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generación de resúmenes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen: <pad> the Beatles were an english rock band formed in 1960. they are regarded as the most influential band of all time. they were integral to the development of 1960s counterculture.</s>\n"
     ]
    }
   ],
   "source": [
    "def sumarize(text, model_name=\"t5-base\", task=\"summarize\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return summarized_text\n",
    "\n",
    "# Ejemplo de resumen\n",
    "text = \"\"\"\n",
    "\"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr. They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\n",
    "\"\"\"\n",
    "print(\"Resumen:\", sumarize(text, task=\"summarize\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPT**\n",
    "\n",
    "El modelo GPT (Generative Pre-trained Transformer) fue introducido por OpenAI en junio de 2018. Este modelo representó un avance significativo en el campo del procesamiento de lenguaje natural (NLP) al combinar técnicas de pre-entrenamiento en grandes corpus de texto con la capacidad de afinar el modelo para tareas específicas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path=\"./models/mistral-7b-instruct-v0.2.Q4_K_S.gguf\",  # Download the model file first\n",
    "  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8187.15 ms\n",
      "llama_print_timings:      sample time =       8.38 ms /    42 runs   (    0.20 ms per token,  5012.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    6022.71 ms /    42 runs   (  143.40 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:       total time =    6264.02 ms /    43 tokens\n"
     ]
    }
   ],
   "source": [
    "# Simple inference example\n",
    "output = llm(\n",
    "  \"<s>[INST] Juan y Ana están en la misma habitación. Juan coloca una manzana dentro de un cajón y se va de la habitación. Cuando Ana sabe que Juan no puede verla, coge la manzana y la esconde bajo la cama. Ahora llega Juan a la habitación, ¿dónde creerá Ana que Juan buscará la manzana? [/INST]\", # Prompt\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-6be337a7-49a5-4613-bfc5-2c9bee699bd1',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1712607693,\n",
       " 'model': './models/mistral-7b-instruct-v0.2.Q4_K_S.gguf',\n",
       " 'choices': [{'text': '<s>[INST] Juan y Ana están en la misma habitación. Juan coloca una manzana dentro de un cajón y se va de la habitación. Cuando Ana sabe que Juan no puede verla, coge la manzana y la esconde bajo la cama. Ahora llega Juan a la habitación, ¿dónde creerá Ana que Juan buscará la manzana? [/INST] Ana creería que Juan buscará la manzana dentro del cajón, ya que ésta fue el lugar donde la colocó inicialmente antes de esconderla.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 97, 'completion_tokens': 41, 'total_tokens': 138}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

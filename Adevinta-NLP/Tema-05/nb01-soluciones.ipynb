{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Atención Q, K y V**\n",
    "\n",
    "Con el objetivo de entender con detalle el funcionamiento de la atención en transformers, implementaremos una versión simplificada de la atención con los vectores Q, K y V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "\n",
      "Score:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3000, 0.6000, 0.9000, 1.2000],\n",
      "        [0.0600, 0.1200, 0.1800, 0.2400],\n",
      "        [0.0900, 0.1800, 0.2700, 0.3600]])\n",
      "\n",
      "Score normalizado:\n",
      " tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
      "        [0.1892, 0.2250, 0.2676, 0.3182],\n",
      "        [0.2372, 0.2455, 0.2542, 0.2631],\n",
      "        [0.2309, 0.2432, 0.2561, 0.2698]])\n",
      "\n",
      "Resultado:\n",
      " tensor([[0.2500, 0.5000, 0.5000],\n",
      "        [0.1892, 0.5432, 0.5857],\n",
      "        [0.2372, 0.5087, 0.5173],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Supongamos que tenemos los siguiente vectores Q, K y V\n",
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])\n",
    "\n",
    "print(Q.shape)\n",
    "print(K.shape)\n",
    "print(V.shape)\n",
    "\n",
    "score = Q @ K.transpose(0, 1)  # @ equivale a la multiplicación de matrices\n",
    "\n",
    "print(\"\\nScore:\\n\", score)\n",
    "\n",
    "score = score / torch.sqrt(torch.tensor(K.shape[1]).float())  # Dividimos por la raíz cuadrada de la dimensión de K\n",
    "score = torch.softmax(score, dim=1)\n",
    "\n",
    "print(\"\\nScore normalizado:\\n\", score)\n",
    "\n",
    "Z = score @ V  \n",
    "\n",
    "print(\"\\nResultado:\\n\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Todo en uno: scaled_dot_product_attention**\n",
    "\n",
    "Todo el cálculo anterior lo realiza eficientemente una función de PyTorch llamada **scaled_dot_product_attention()**. Esta función calcula la atención en los tensores de consulta (query), clave (key) y valor (value), utilizando una máscara de atención opcional si se proporciona, y aplicando *dropout* si se especifica una probabilidad mayor a 0.0.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.5000, 0.5000],\n",
      "        [0.1892, 0.5432, 0.5857],\n",
      "        [0.2372, 0.5087, 0.5173],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "Z = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Atención Q, K y V con enmascaramiento**\n",
    "\n",
    "El enmascaramiento durante la etapa del decodificador en los modelos Transformer es crucial para evitar que el decodificador tenga acceso a información futura, especialmente en tareas de generación secuencial como la traducción automática o la generación de texto. Este concepto se conoce como \"enmascaramiento de atención causal\".\n",
    "\n",
    "En el contexto de los Transformers, el decodificador genera una salida secuencialmente, palabra por palabra. Durante la generación de cada palabra, es importante que el modelo solo tenga en cuenta las palabras anteriores y no las futuras, ya que estas últimas no deberían estar disponibles (en un escenario de generación de texto, por ejemplo, las palabras futuras aún no se han generado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1732, 0.3464, 0.5196, 0.6928],\n",
      "        [0.0346, 0.0693, 0.1039, 0.1386],\n",
      "        [0.0520, 0.1039, 0.1559, 0.2078]])\n",
      "\n",
      "Score masked:\n",
      " tensor([[0.0000,   -inf,   -inf,   -inf],\n",
      "        [0.1732, 0.3464,   -inf,   -inf],\n",
      "        [0.0346, 0.0693, 0.1039,   -inf],\n",
      "        [0.0520, 0.1039, 0.1559, 0.2078]])\n",
      "\n",
      "Score masked with softmax:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449, 0.0000],\n",
      "        [0.2309, 0.2432, 0.2561, 0.2698]])\n",
      "\n",
      "Z:\n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])\n",
    "\n",
    "score = Q @ K.transpose(0, 1)\n",
    "score = score / torch.sqrt(torch.tensor(K.shape[1]).float())\n",
    "print(\"\\nScore:\\n\", score)\n",
    "tril = torch.tril(torch.ones(score.shape[0], score.shape[0]))  # Creamos la máscara\n",
    "score_masked = score.masked_fill(tril == 0, float('-inf'))  # Aplicamos la máscara al score. Todo lo que sea 0 en la máscara, lo reemplazamos por -inf para que al aplicar la softmax, se vuelva 0\n",
    "print(\"\\nScore masked:\\n\", score_masked)\n",
    "score_masked = torch.softmax(score_masked, dim=1)\n",
    "print(\"\\nScore masked with softmax:\\n\", score_masked)\n",
    "Z = score_masked @ V \n",
    "print(\"\\nZ:\\n\", Z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos de nuevo que podemos hacer lo mismo con la función **torch.nn.functional.scaled_dot_product_attention()** estableciendo el parámetro **is_causal** a True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "Z = scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 2: Implementación de la normalización de capa\n",
    "\n",
    "Asumiento los parámetros $\\gamma$ y $\\beta$ como 1 y 0 respectivamente, desarrolla un código que normalice los siguientes vectores de características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2635,  0.0815,  1.1820])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([ 1.2635, -0.0815, -1.1820])\n",
      "tensor([[-1.4963e+00,  1.4963e-01,  1.4963e+00],\n",
      "        [ 1.7837e-07,  1.7837e-07,  1.7837e-07],\n",
      "        [ 1.4963e+00, -1.4963e-01, -1.4963e+00]])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "v1 = torch.tensor([1.0, 2.1, 3.0])\n",
    "v2 = torch.tensor([2.0, 2.0, 2.0])\n",
    "v3 = torch.tensor([3.0, 1.9, 1.0])\n",
    "\n",
    "# Inicializamos listas para guardar los vectores normalizados\n",
    "v1_normalized_manual = torch.zeros_like(v1)\n",
    "v2_normalized_manual = torch.zeros_like(v2)\n",
    "v3_normalized_manual = torch.zeros_like(v3)\n",
    "\n",
    "# Lista de vectores para iterar más fácilmente\n",
    "vectors = [v1, v2, v3]\n",
    "\n",
    "# Calculamos manualmente la normalización de capa para cada vector\n",
    "for i, v in enumerate(vectors):\n",
    "    mean = v.mean()\n",
    "    var = v.var(unbiased=False)  # Calculamos la varianza sin sesgo\n",
    "    std = torch.sqrt(var + 1e-12)  # Añadimos epsilon para estabilidad numérica\n",
    "    vectors[i] = (v - mean) / std  # Normalizamos\n",
    "\n",
    "# Asignamos los vectores normalizados a sus respectivas variables\n",
    "v1_normalized_manual, v2_normalized_manual, v3_normalized_manual = vectors\n",
    "\n",
    "print(v1_normalized_manual)\n",
    "print(v2_normalized_manual)\n",
    "print(v3_normalized_manual)\n",
    "\n",
    "# Para verificar, usamos la implementación de PyTorch de Layer Normalization\n",
    "from torch.nn.functional import layer_norm\n",
    "\n",
    "# Agrupamos los vectores originales para aplicar layer_norm\n",
    "v = torch.stack([v1, v2, v3])\n",
    "\n",
    "# Aplicamos layer_norm a todo el conjunto de vectores\n",
    "#v_normalized_pytorch = layer_norm(v, v.shape[1:], eps=1e-12)\n",
    "v_normalized_pytorch = layer_norm(v, v.shape, eps=1e-12)\n",
    "\n",
    "print(v_normalized_pytorch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114.47514973310177, 89.81829275768195, 112.27286594710245)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Crear una población de alturas (en cm) de 100 personas\n",
    "# np.random.seed(42)  # Semilla para reproducibilidad\n",
    "# poblacion_alturas = np.random.normal(loc=170, scale=10, size=100)  # media=170, desviación estándar=10\n",
    "poblacion_alturas = np.random.uniform(low=150, high=190, size=100)  # uniforme entre 150 y 190\n",
    "\n",
    "# Varianza poblacional\n",
    "varianza_poblacional = np.var(poblacion_alturas)\n",
    "\n",
    "# Tomar 1000 muestras diferentes de tamaño 5 de la población\n",
    "n_muestras = 1000\n",
    "tamaño_muestra = 5\n",
    "varianzas_muestrales_con_sesgo = []\n",
    "varianzas_muestrales_sin_sesgo = []\n",
    "\n",
    "for _ in range(n_muestras):\n",
    "    muestra = np.random.choice(poblacion_alturas, tamaño_muestra, replace=False)\n",
    "    varianzas_muestrales_con_sesgo.append(np.var(muestra, ddof=0))\n",
    "    varianzas_muestrales_sin_sesgo.append(np.var(muestra, ddof=1))\n",
    "\n",
    "# Calcular el promedio de las varianzas muestrales\n",
    "promedio_var_muestrales_con_sesgo = np.mean(varianzas_muestrales_con_sesgo)\n",
    "promedio_var_muestrales_sin_sesgo = np.mean(varianzas_muestrales_sin_sesgo)\n",
    "\n",
    "varianza_poblacional, promedio_var_muestrales_con_sesgo, promedio_var_muestrales_sin_sesgo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entropía**\n",
    "\n",
    "La expresión de la **entropía** tiene la siguiente forma:\n",
    "\n",
    "$$ H(X) = - \\sum_i P(x_i)\\log_2{P(x_i)} $$\n",
    "\n",
    "Veámos qué significa esta fórmula.\n",
    "\n",
    "Podríamos entender la entropía como la cantidad de información que, de promedio, circula por un canal. Por ejemplo, supongamos que tenemos un conjunto de cuatro símbolos *a*, *b*, *c* y *d* que circulan por un canal formando un mensaje. Si queremos codificar esos cuatro símbolos deberemos usar dos bits. Supongamos también que las probabilidades de aparición de cada uno de ellos es de $0.25$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'a' 'b' 'a' 'c' 'c' 'c' 'b' 'a' 'b' 'c' 'c' 'd' 'c' 'a' 'a' 'c' 'b'\n",
      " 'a' 'd' 'a' 'b' 'd' 'b' 'b' 'c' 'a' 'd' 'd' 'b' 'a' 'a' 'b' 'c' 'd' 'b'\n",
      " 'a' 'd' 'b' 'b' 'c' 'c' 'a' 'c' 'b' 'c' 'b' 'd' 'd' 'd']\n",
      "Longitud del mensaje: 2000000 bits\n",
      "Número promedio de bits por símbolo: 2.0 bits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd']\n",
    "symbols_prob = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "symbols_dict = {\n",
    "    'a': '00',\n",
    "    'b': '01',\n",
    "    'c': '10',\n",
    "    'd': '11',\n",
    "}\n",
    "\n",
    "numbers_of_symbols_in_message = 1_000_000\n",
    "\n",
    "message = np.random.choice(symbols, numbers_of_symbols_in_message, p=symbols_prob)\n",
    "print(message[:50])\n",
    "\n",
    "code = \"\"\n",
    "for e in message:\n",
    "    code += symbols_dict[e]\n",
    "\n",
    "print(\"Longitud del mensaje:\", len(code), \"bits\")\n",
    "print(\"Número promedio de bits por símbolo:\", np.round(len(code)/numbers_of_symbols_in_message, 2), \"bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante esta codificación tenemos que la cantidad de dígitos binarios que circulan por el canal corresponde al número de símbolos enviados por dos. Si el mensaje contiene veinte símbolos, la cantidad de dígitos será cuarenta. En otras palabras, la cantidad de dígitos binarios (bits) por símbolo es dos.\n",
    "\n",
    "<img src=\"imgs/binaria.svg\" width=\"330px\">\n",
    "\n",
    "Pero, ¿qué ocurre si la probabilidad de aparición de cada uno de los símbolos no fuera la misma? Supongamos ahora que la probabilidad de cada símbolo es, para: <code>['a', 'b', 'c', 'd']</code>, la siguiente: <code>[0.5, 0.25, 0.125, 0.125]</code>. Si utilizáramos otra codificación distinta, el número de bits promedio por símbolo se podría reducir. Consideremos la siguiente codificación: <code>'a': '0', 'b': '10', 'c': '110','d': '111'</code>. ¿Qué pasaría si generáramos un millón de símbolos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'b' 'b' 'a' 'a' 'a' 'a' 'a' 'a' 'a' 'b' 'a' 'd' 'a' 'b' 'a' 'c' 'b'\n",
      " 'a' 'a' 'b' 'd' 'a' 'b' 'a' 'b' 'a' 'b' 'a' 'c' 'd' 'a' 'c' 'a' 'a' 'a'\n",
      " 'b' 'a' 'a' 'a' 'a' 'b' 'b' 'a' 'b' 'a' 'a' 'a' 'a' 'a']\n",
      "01010000000010011101001101000101110100100100110111\n",
      "Longitud del mensaje: 1750485 bits\n",
      "Número promedio de bits por símbolo: 1.75 bits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd']\n",
    "symbols_prob = [0.5, 0.25, 0.125, 0.125]\n",
    "\n",
    "symbols_dict = {\n",
    "    'a': '0',\n",
    "    'b': '10',\n",
    "    'c': '110',\n",
    "    'd': '111',\n",
    "}\n",
    "\n",
    "numbers_of_symbols_in_message = 1_000_000\n",
    "\n",
    "message = np.random.choice(symbols, numbers_of_symbols_in_message, p=symbols_prob)\n",
    "\n",
    "print(message[:50])\n",
    "\n",
    "code = \"\"\n",
    "for e in message:\n",
    "    code += symbols_dict[e]\n",
    "\n",
    "print(code[:50])\n",
    "\n",
    "print(\"Longitud del mensaje:\", len(code), \"bits\")\n",
    "print(\"Número promedio de bits por símbolo:\", np.round(len(code)/numbers_of_symbols_in_message, 2), \"bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta nueva forma de codificar reduce la cantidad de bits necesarios.\n",
    "\n",
    "<img src=\"imgs/huffman.svg\" width=\"330px\">\n",
    "\n",
    "La regla, por tanto, debe ser algo así como \n",
    "\n",
    "    \"A mayor probabilidad de aparición de un símbolo, menor número de bits para su representación\"\n",
    "\n",
    "Démonos cuenta de que es eso lo que intenta reflejar la fórmula de la entropía. En la figura anterior observamos que en la raíz del árbol, el cual representa nuestra codificación, nos podemos encontrar un 0 o un 1. Si es un 0 ya sabemos que el símbolo actual es una 'a'. Si es un 1 debemos esperar al siguiente bit para determinar cuál es el nuevo símbolo. Si es un 0 ya sabemos que corresponde a una 'b'. Si es un 1, de nuevo tendremos que esperar al siguiente bit. Y así, sucesivamente. \n",
    "\n",
    "Empezamos a verle ahora el sentido a la fórmula de la entropía. La parte $-log_2(P(x_i))$ corresponde al número de bits que debemos asociar a cada símbolo en función de la probabilidad que tenga. $-log_2(0.5) = 1, -log_2(0.25) = 2, -log_2(0.125) = 3, \\dots $ Y la parte $P(x_i)$ corresponde a la probabilidad en que esas cantidades de bits por símbolo se darán en el mensaje. Por tanto, para este ejemplo la entropía será:\n",
    "\n",
    "$$ E = - \\sum_i P(x_i)\\log_2{P(x_i)} = 0.5 \\cdot \\log_2(0.5) + 0.25 \\cdot \\log_2(0.25) +0.125 \\cdot \\log_2(0.125) +0.125 \\cdot \\log_2(0.125) = 1.75$$\n",
    "\n",
    "¿Qué ocurriría con un nuevo conjunto de probabilidades? Cada símbolo lo tenemos que codificar con un número entero de bits, por tanto, para cualquier nuevo conjunto de probabilidades el procedimiento consistiría en ordenar los símbolos por sus probabilidades (de mayor a menor) para ir creando el árbol de codificación. Veamos qué obtendríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El símbolo a se debe codificar con 1.0 bits.\n",
      "El símbolo b se debe codificar con 2.0 bits.\n",
      "El símbolo c se debe codificar con 3.0 bits.\n",
      "El símbolo d se debe codificar con 3.0 bits.\n",
      "['a' 'a' 'b' 'b' 'a' 'd' 'a' 'b' 'a' 'a' 'a' 'a' 'b' 'a' 'a' 'd' 'c' 'a'\n",
      " 'd' 'b' 'a' 'a' 'b' 'b' 'd' 'a' 'a' 'a' 'd' 'a' 'a' 'a' 'b' 'd' 'd' 'd'\n",
      " 'd' 'd' 'a' 'a' 'a' 'a' 'd' 'b' 'b' 'b' 'a' 'a' 'a' 'a']\n",
      "Entropía: 1.75 bits\n",
      "Longitud del mensaje: 1750106 bits\n",
      "Número promedio de bits por símbolo: 1.750106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def entropy(p: list) -> float:\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd']\n",
    "\n",
    "symbols_prob = [0.5, 0.25, 0.125, 0.125]\n",
    "# symbols_prob = [0.75, 0.15, 0.05, 0.05]\n",
    "# symbols_prob = [0.90, 0.05, 0.025, 0.025]\n",
    "\n",
    "for s, e in zip(symbols, symbols_prob):\n",
    "    print(\"El símbolo\",s,\"se debe codificar con\", np.round(-math.log(e, 2), 3),\"bits.\")\n",
    "\n",
    "symbols_dict = {\n",
    "    'a': '0',\n",
    "    'b': '10',\n",
    "    'c': '110',\n",
    "    'd': '111',\n",
    "}\n",
    "\n",
    "\n",
    "print(message[:50])\n",
    "print(\"Entropía:\", np.round( entropy(symbols_prob), 3), \"bits\")\n",
    "\n",
    "numbers_of_symbols_in_message = 1_000_000\n",
    "message = np.random.choice(symbols, numbers_of_symbols_in_message, p=symbols_prob)\n",
    "\n",
    "code = \"\"\n",
    "for e in message:\n",
    "    code += symbols_dict[e]\n",
    "\n",
    "print(\"Longitud del mensaje:\", len(code), \"bits\")\n",
    "print(\"Número promedio de bits por símbolo:\", len(code)/numbers_of_symbols_in_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Por tanto, podemos establecer que:\n",
    "\n",
    ">   la **entropía** nos dice cuál es la cantidad mínima de bits promedio para codificar un mensaje. Pero ojo! No nos dice cuál es el esquema óptimo de codificación que necesitaríamos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicios\n",
    "\n",
    "- En el código anterior, ¿qué entropía nos resulta al cambiar unas probabilidades por otras?\n",
    "- ¿Qué ocurriría si cambiáramos la cantidad de bits para la codificación de cada símbolo  $\\log_2{p(x_i)}$  por otra? ¿Qué valor obtendríamos para el entropía?\n",
    "- ¿Qué significa que la entropía pueda ser menor que el número promedio de bits por símbolo?\n",
    "- ¿A qué corresponde cada sumando de la fórmula de la entropía?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolución del segundo apartado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía: 1.75 bits\n",
      "Entropía: 2.487 bits. Con otras probabilidades.\n",
      "----\n",
      "Entropía: 0.619 bits\n",
      "Entropía: 1.15 bits. Con otras probabilidades.\n"
     ]
    }
   ],
   "source": [
    "# Apartado 2\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def entropy(p1: list, p2: list) -> float:\n",
    "    return -np.sum(p1 * np.log2(p2))\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd']\n",
    "symbols_prob_1 = [0.5, 0.25, 0.125, 0.125]  # Probabilidades de los símbolos\n",
    "symbols_prob_2 = [0.90, 0.05, 0.025, 0.025]  # Otras probabilidades distinas para los símbolos\n",
    "\n",
    "print(\"Entropía:\", np.round( entropy(symbols_prob_1, symbols_prob_1), 3), \"bits\")\n",
    "print(\"Entropía:\", np.round( entropy(symbols_prob_1, symbols_prob_2), 3), \"bits. Con otras probabilidades.\")\n",
    "print(\"----\")\n",
    "print(\"Entropía:\", np.round( entropy(symbols_prob_2, symbols_prob_2), 3), \"bits\")\n",
    "print(\"Entropía:\", np.round( entropy(symbols_prob_2, symbols_prob_1), 3), \"bits. Con otras probabilidades.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la codificación óptima para los símbolos la da la expresión de la entropía. Si cambiamos la codificación de los símbolos, el número de bits para codificar el mensaje aumenta.\n",
    "\n",
    "Por cierto, lo que acabamos del calcular se denomina **entropía cruzada**. \n",
    "\n",
    "$$ H(P,Q) = - \\sum_i P(x_i) \\cdot \\log_2{Q(x_i)} $$\n",
    "\n",
    "Consiste en calcular el número de bits promedio de un mensaje cuando se utiliza una distribución $Q$, distinta a la real $P$, para calcular la codificación de los símbolos.\n",
    "\n",
    "Es importante también notar que:\n",
    "\n",
    "$$ H(P,Q) \\neq H(Q,P) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

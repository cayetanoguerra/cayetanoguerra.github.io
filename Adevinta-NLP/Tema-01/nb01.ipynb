{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocesamiento**\n",
    "\n",
    "El **preprocesamiento** se refiere a un conjunto de técnicas y procesos utilizados para preparar los datos de texto para su análisis y modelado posterior. Estas técnicas ayudan a transformar y normalizar el texto de tal manera que se facilite su procesamiento posterior por algoritmos de PLN.\n",
    "\n",
    "Las tareas comunes de preprocesamiento que veremos son:\n",
    "\n",
    "1. **Tokenización**: Dividir el texto en unidades más pequeñas, como palabras.\n",
    "\n",
    "2. **Eliminación de signos de puntuación y caracteres especiales**: Limpiar el texto de elementos que no aportan significado relevante.\n",
    "\n",
    "3. **Conversión a minúsculas**: Uniformizar el texto para evitar diferencias entre mayúsculas y minúsculas.\n",
    "\n",
    "4. **Eliminación de stop words**: Eliminar palabras comunes que no aportan mucho significado al texto, como \"el\", \"la\", \"y\", \"de\", etc.\n",
    "\n",
    "5. **Lematización y derivación (stemming)**: Reducir las palabras a su raíz o forma base. Por ejemplo, convertir palabras como \"corriendo\" o \"corrió\" a su lema o raíz \"correr\".\n",
    "\n",
    "6. **Etiquetado de partes del discurso (POS tagging)**: Identificar y etiquetar las partes del discurso de cada palabra en el texto, como sustantivo, verbo, adjetivo, etc.\n",
    "\n",
    "7. **Análisis sintáctico (parsing)**: Determinar la estructura gramatical del texto.\n",
    "\n",
    "8. **Reconocimiento de entidades nombradas (NER)**: Identificar y clasificar entidades nombradas en categorías predefinidas como nombres de personas, organizaciones, lugares, expresiones de tiempo, cantidades, valores monetarios, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Tokenización**\n",
    "\n",
    "La **tokenización** es un paso crucial en el procesamiento del lenguaje natural (NLP, por sus siglas en inglés). En este proceso, un texto (que puede ser una oración, un párrafo o un documento completo) se divide en piezas más pequeñas llamadas \"tokens\". Estos tokens pueden ser palabras, conjuntos de palabras, subpalabras o incluso caracteres individuales, dependiendo del nivel de tokenización que se está realizando.\n",
    "\n",
    "Por ejemplo, la frase: \"Este año me voy de vacaciones a la playa\" puede ser dividida en tokens de la siguiente manera:\n",
    "\"Este\", \"año\", \"me\", \"voy\", \"de\", \"vacaciones\", \"a\", \"la\", \"playa\". Aunque la forma más fácil de tokenizar un texto es separarlo en palabras, usando para ello los espacios en blanco entre palabras, recuerda que también es posible tokenizar de otras formas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Este', 'año', 'me', 'voy', 'de', 'vacaciones', 'a', 'la', 'playa']\n"
     ]
    }
   ],
   "source": [
    "txt = \"Este año me voy de vacaciones a la playa\"\n",
    "tokens = txt.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Eliminación de signos de puntuación y caracteres especiales**\n",
    "\n",
    "Los tokens también pueden incluir signos de puntuación. Dependediendo de la aplicación que estemos desarrollando, puede ser útil mantenerlos o eliminarlos. Podemos hacerlo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Este', 'año,', 'como', 'hicimos', 'el', 'anterior,', 'nos', 'iremos', 'a', 'la', 'playa', 'de', 'vacaciones.']\n"
     ]
    }
   ],
   "source": [
    "txt = \"Este año, como hicimos el anterior, nos iremos a la playa de vacaciones.\"\n",
    "tokens = txt.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Este', 'año', 'como', 'hicimos', 'el', 'anterior', 'nos', 'iremos', 'a', 'la', 'playa', 'de', 'vacaciones']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar eliminando signos de puntuación\n",
    "txt = \"Este año, como hicimos el anterior, nos iremos a la playa de vacaciones.\"\n",
    "\n",
    "txt = txt.replace(',', '')\n",
    "txt = txt.replace('.', '')\n",
    "txt = txt.replace(';', '')\n",
    "txt = txt.replace(':', '')\n",
    "txt = txt.replace('?', '')\n",
    "txt = txt.replace('¿', '')\n",
    "txt = txt.replace('!', '')\n",
    "txt = txt.replace('¡', '')\n",
    "\n",
    "tokens = txt.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Conversión a minúsculas**\n",
    "\n",
    "En Python podemos convertir un texto a minúsculas usando el método `lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "este es un texto de ejemplo para convertir a minúsculas.\n"
     ]
    }
   ],
   "source": [
    "texto_original = \"Este es un Texto de Ejemplo para Convertir a MINÚSCULAS.\"\n",
    "texto_en_minusculas = texto_original.lower()\n",
    "\n",
    "print(texto_en_minusculas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Eliminación de stop words**\n",
    "\n",
    "Estas palabras son generalmente las más comunes en un idioma, como artículos, preposiciones, conjunciones y algunas veces verbos auxiliares y pronombres.\n",
    "\n",
    "La razón principal para filtrar las palabras de parada es que, aunque son fundamentales para la estructura gramatical de un idioma, suelen tener poca relevancia semántica en el análisis de texto. Esto es especialmente cierto en tareas como la minería de texto, el análisis de sentimientos, la clasificación de texto, etc., donde el enfoque está en identificar palabras clave que representan mejor el significado y el contenido del texto.\n",
    "\n",
    "Por ejemplo, en español, palabras como \"el\", \"la\", \"y\", \"de\", \"que\" son consideradas *stop words*. Al eliminarlas, se puede reducir el tamaño del conjunto de datos y concentrarse en palabras que aportan más información relevante para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Este', 'texto', 'ejemplo', 'eliminar', 'stop-words']\n"
     ]
    }
   ],
   "source": [
    "# Lista de stop-words en español\n",
    "stop_words = [\"de\", \"la\", \"que\", \"el\", \"en\", \"y\", \"a\", \"los\", \"se\", \"del\", \"las\", \"por\", \"un\", \"para\", \"con\", \"no\", \"una\", \"su\", \"al\", \"lo\", \"como\", \"más\", \"pero\", \"sus\", \"le\", \"ya\", \"o\", \"este\", \"sí\", \"porque\", \"esta\", \"entre\", \"cuando\", \"muy\", \"sin\", \"sobre\", \"también\", \"me\", \"hasta\", \"hay\", \"donde\", \"quien\", \"desde\", \"todo\", \"nos\", \"durante\", \"todos\", \"uno\", \"les\", \"ni\", \"contra\", \"otros\", \"ese\", \"eso\", \"ante\", \"ellos\", \"e\", \"esto\", \"mí\", \"antes\", \"algunos\", \"qué\", \"unos\", \"yo\", \"otro\", \"otras\", \"otra\", \"él\", \"tanto\", \"esa\", \"estos\", \"mucho\", \"quienes\", \"nada\", \"muchos\", \"cual\", \"poco\", \"ella\", \"estar\", \"estas\", \"algunas\", \"algo\", \"nosotros\", \"mi\", \"mis\", \"tú\", \"te\", \"ti\", \"tu\", \"tus\", \"ellas\", \"nosotras\", \"vosotros\", \"vosotras\", \"os\", \"mío\", \"mía\", \"míos\", \"mías\", \"tuyo\", \"tuya\", \"tuyos\", \"tuyas\", \"suyo\", \"suya\", \"suyos\", \"suyas\", \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \"vuestro\", \"vuestra\", \"vuestros\", \"vuestras\", \"esos\", \"esas\", \"estoy\", \"estás\", \"está\", \"estamos\", \"estáis\", \"están\", \"esté\", \"estés\", \"estemos\", \"estéis\", \"estén\", \"estaré\", \"estarás\", \"estará\", \"estaremos\", \"estaréis\", \"estarán\", \"estaría\", \"estarías\", \"estaríamos\", \"estaríais\", \"estarían\", \"estaba\", \"estabas\", \"estábamos\", \"estabais\", \"estaban\", \"estuve\", \"estuviste\", \"estuvo\", \"estuvimos\", \"estuvisteis\", \"estuvieron\", \"estuviera\", \"estuvieras\", \"estuviéramos\", \"estuvierais\", \"estuvieran\", \"estuviese\", \"estuvieses\", \"estuviésemos\", \"estuvieseis\", \"estuviesen\", \"estando\", \"estado\", \"estada\", \"estados\", \"estadas\", \"estad\", \"he\", \"has\", \"ha\", \"hemos\", \"habéis\", \"han\", \"haya\", \"hayas\", \"hayamos\", \"hayáis\", \"hayan\", \"habré\", \"habrás\", \"habrá\", \"habremos\", \"habréis\", \"habrán\", \"habría\", \"habrías\", \"habríamos\", \"habríais\", \"habrían\", \"había\", \"habías\", \"habíamos\", \"habíais\", \"habían\", \"hube\", \"hubiste\", \"hubo\", \"hubimos\", \"hubisteis\", \"hubieron\", \"hubiera\", \"hubieras\", \"hubiéramos\", \"hubierais\", \"hubieran\", \"hubiese\", \"hubieses\", \"hubiésemos\", \"hubieseis\", \"hubiesen\", \"habiendo\", \"habido\", \"habida\", \"habidos\", \"habidas\", \"soy\", \"eres\", \"es\"]\n",
    "\n",
    "txt = \"Este es un texto de ejemplo para eliminar las stop-words\"\n",
    "tokens = txt.split()\n",
    "tokens_filtrados = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in stop_words:\n",
    "        tokens_filtrados.append(token)\n",
    "\n",
    "print(tokens_filtrados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 1\n",
    "\n",
    "- Crea un script para extraer los símbolos de puntuación como tokens independientes.\n",
    "- ¿Qué ocurre si nos encontramos un texto donde no se han separado correctamente las palabras y los signos de puntuación? Por ejemplo: \"¿Cómo estás?Bien,gracias.\"\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NLTK: Natural Language Toolkit**\n",
    "\n",
    "https://www.nltk.org/\n",
    "\n",
    "<code>!pip install nltk</code>\n",
    "\n",
    "NLTK es una biblioteca de Python para el procesamiento del lenguaje natural. Proporciona interfaces fáciles de usar para más de 50 recursos léxicos corporales y gramaticales, como WordNet, junto con una suite de bibliotecas de procesamiento de texto para la clasificación, tokenización, derivación, etiquetado y análisis sintáctico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cayetano/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cayetano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Para tokenización\n",
    "nltk.download('stopwords')  # Para stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo con NLTK podemos realizar las tareas de preprocesamiento que hemos visto anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Este', 'es', 'un', 'ejemplo', 'de', 'texto', 'para', 'usar', 'NLTK', '.', '¡Tokeniza', 'este', 'texto', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"Este es un ejemplo de texto para usar NLTK. ¡Tokeniza este texto!\"\n",
    "palabras = word_tokenize(texto)\n",
    "print(palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ejemplo', 'texto', 'usar', 'NLTK', '.', '¡Tokeniza', 'texto', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Suponiendo que ya tienes una lista de palabras tokenizadas\n",
    "palabras_filtradas = [palabra for palabra in palabras if not palabra.lower() in stop_words]\n",
    "\n",
    "print(palabras_filtradas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Lematización y derivación (stemming)**\n",
    "\n",
    "La **lematización** es un proceso que consiste en reducir las palabras a su forma básica o \"lema\", que es una forma canónica o de diccionario de una palabra. La lematización tiene en cuenta el análisis morfológico de las palabras, es decir, considera el contexto gramatical y sintáctico para convertir una palabra a su forma base.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "\"Corriendo\" → \"correr\"\n",
    "\n",
    "\"Mujeres\" → \"mujer\"\n",
    "\n",
    "La **derivación** o \"stemming\" es un proceso que tiene como objetivo reducir las palabras a su raíz o \"tallo\" (en inglés, \"stem\"). A diferencia de la lematización, que intenta reducir las palabras a su forma base léxica teniendo en cuenta la morfología y el contexto gramatical, el stemming suele ser un proceso más heurístico y rudimentario que simplemente elimina los sufijos (y a veces prefijos) de las palabras.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "\"Corriendo\" → \"corri\"\n",
    "\n",
    "\"Comiendo\" → \"comi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/SpaCy_logo.png\" width=\"20%\">\n",
    "\n",
    "https://spacy.io/\n",
    "\n",
    "Procesar texto no suele ser una tarea trivial. La mayoría de las palabras son raras y es común que palabras que parecen completamente diferentes signifiquen casi lo mismo. Las mismas palabras en diferente orden pueden significar algo completamente diferente. Incluso dividir el texto en unidades útiles similares a palabras puede resultar difícil en muchos idiomas. Si bien es posible resolver algunos problemas a partir únicamente de los caracteres sin procesar, generalmente es mejor utilizar conocimientos lingüísticos para agregar información útil. Eso es exactamente para lo que está diseñado <a href=\"https://spacy.io/\">spaCy</a>: ingresas texto sin formato y obtienes un objeto <code>Doc</code>, que viene con una variedad de anotaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo tokenizar un texto usando Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este\n",
      "año\n",
      ",\n",
      "como\n",
      "hicimos\n",
      "el\n",
      "anterior\n",
      ",\n",
      "nos\n",
      "iremos\n",
      "a\n",
      "la\n",
      "playa\n",
      "de\n",
      "vacaciones\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Este año, como hicimos el anterior, nos iremos a la playa de vacaciones.\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de tokenizar, es posible que queramos separar en frases el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es una oración.\n",
      "Esto es otra oración.\n",
      "U.S.A. es un país.\n",
      "Esta es un frase final.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Esto es una oración. Esto es otra oración. U.S.A. es un país. Esta es un frase final.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a lematizar el texto usando Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este -> este\n",
      "año -> año\n",
      ", -> ,\n",
      "como -> como\n",
      "hicimos -> hacer\n",
      "el -> el\n",
      "anterior -> anterior\n",
      ", -> ,\n",
      "nos -> yo\n",
      "iremos -> irer\n",
      "a -> a\n",
      "la -> el\n",
      "playa -> playa\n",
      "de -> de\n",
      "vacaciones -> vacación\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Este año, como hicimos el anterior, nos iremos a la playa de vacaciones.\")\n",
    "for token in doc:\n",
    "    print(token.text, \"->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy no tiene la posibilidad de hacer stemming de palabras, pero podemos usar NLTK para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corr', 'corr', 'corredor', 'corr', 'rap']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "\n",
    "# Crear una instancia del Snowball Stemmer para español\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "# Lista de palabras para aplicar stemming\n",
    "palabras = [\"correr\", \"corriendo\", \"corredor\", \"corrió\", \"rápido\"]\n",
    "\n",
    "# Aplicar stemming a cada palabra\n",
    "stems = [stemmer.stem(palabra) for palabra in palabras]\n",
    "\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Etiquetado de partes del discurso (POS tagging)**\n",
    "\n",
    "El POS tagging es un proceso que implica asignar a cada palabra en un texto una etiqueta que indica su parte del discurso. Las partes del discurso incluyen categorías gramaticales como sustantivos, verbos, adjetivos, adverbios, preposiciones, conjunciones, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Text     Lemma    POS    Tag      Dep Shape  is_alpha  is_stop\n",
      "0         Este      este    DET    DET      det  Xxxx      True     True\n",
      "1          año       año   NOUN   NOUN      obl   xxx      True    False\n",
      "2            ,         ,  PUNCT  PUNCT    punct     ,     False    False\n",
      "3         como      como  SCONJ  SCONJ     mark  xxxx      True     True\n",
      "4      hicimos     hacer   VERB   VERB    advcl  xxxx      True    False\n",
      "5           el        el    DET    DET      det    xx      True     True\n",
      "6     anterior  anterior    ADJ    ADJ    nsubj  xxxx      True     True\n",
      "7            ,         ,  PUNCT  PUNCT    punct     ,     False    False\n",
      "8          nos        yo   PRON   PRON  expl:pv   xxx      True     True\n",
      "9       iremos      irer   VERB   VERB     ROOT  xxxx      True    False\n",
      "10           a         a    ADP    ADP     case     x      True     True\n",
      "11          la        el    DET    DET      det    xx      True     True\n",
      "12       playa     playa   NOUN   NOUN      obl  xxxx      True    False\n",
      "13          de        de    ADP    ADP     case    xx      True     True\n",
      "14  vacaciones  vacación   NOUN   NOUN     nmod  xxxx      True    False\n",
      "15           .         .  PUNCT  PUNCT    punct     .     False    False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Este año, como hicimos el anterior, nos iremos a la playa de vacaciones.\")\n",
    "\n",
    "data = []\n",
    "\n",
    "for token in doc:\n",
    "    data.append([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Text', 'Lemma', 'POS', 'Tag', 'Dep', 'Shape', 'is_alpha', 'is_stop']) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Análisis sintáctico (parsing)**\n",
    "\n",
    "En análisis sintáctico analiza la estructura gramatical de una oración. El análisis sintáctico asigna una estructura a una oración, como puede ser un árbol de dependencia o un árbol de constituyentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"es\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"es\" id=\"5a120a5e21984cd793fc582957009264-0\" class=\"displacy\" width=\"2325\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Este</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">año,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">como</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">hicimos</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">el</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">anterior,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">nos</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">iremos</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">la</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">playa</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">de</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">vacaciones.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 1275.0,2.0 1275.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,89.5 1270.0,89.5 1270.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">expl:pv</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-8\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-9\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,89.5 1795.0,89.5 1795.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1795.0,354.0 L1803.0,342.0 1787.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-10\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,354.0 L1987,342.0 2003,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5a120a5e21984cd793fc582957009264-0-11\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,177.0 2140.0,177.0 2140.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5a120a5e21984cd793fc582957009264-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2140.0,354.0 L2148.0,342.0 2132.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos también las etiquetas \"Dep\" que indica la dependencia o relación sintáctica entre las palabras de una oración. Algunas de estas etiquetas son:\n",
    "\n",
    "- **nsubj**: Sujeto nominal de la cláusula.\n",
    "- **nsubjpass**: Sujeto nominal de una cláusula pasiva.\n",
    "- **csubj**: Sujeto clausal de la cláusula.\n",
    "- **csubjpass**: Sujeto clausal de una cláusula pasiva.\n",
    "- **pobj**: Objeto de una preposición.\n",
    "- **dobj**: Objeto directo.\n",
    "- **iobj**: Objeto indirecto.\n",
    "- **attr**: Atributo, como en \"El cielo está azul\", donde \"azul\" es un atributo de \"cielo\".\n",
    "- **ROOT**: Palabra central de la oración, desde la que se origina la dependencia.\n",
    "- **cc**: Conjunción coordinada.\n",
    "- **conj**: Palabra conectada por una conjunción.\n",
    "- **det**: Determinante.\n",
    "- **amod**: Modificador adjetival.\n",
    "- **advmod**: Modificador adverbial.\n",
    "- **prep**: Preposición.\n",
    "- **mark**: Marcador, generalmente una palabra que introduce una cláusula subordinada.\n",
    "- **aux**: Verbo auxiliar.\n",
    "- **neg**: Negación.\n",
    "- **nummod**: Modificador numeral.\n",
    "- **relcl**: Cláusula relativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La etiqueta \"is_alpha\" significa que el token es una palabra (no un signo de puntuación, número, etc.). La etiqueta \"is_stop\" significa que el token es una \"stop word\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 2\n",
    "\n",
    "Carga el archivo de texto con la novela \"Cien años de soledad\" y calcula:\n",
    "\n",
    "- El número de palabras que tiene la novela.\n",
    "- El número de palabras únicas que tiene la novela.\n",
    "- El número de veces que aparece la palabra \"Macondo\" en la novela.\n",
    "- Las 100 palabras más frecuentes de la novela, eliminando las palabras vacías (stopwords).\n",
    "\n",
    "Ten en cuenta no diferenciar entre mayúsculas y minúsculas.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Reconocimiento de entidades nombradas (NER)**\n",
    "\n",
    "El reconocimiento de entidades nombradas es una tarea muy común dentro del NLP. Consiste en extraer del texto las entidades que son de interés para el usuario, como por ejemplo nombres de personas, organizaciones, lugares, etc. Vamos a ver cómo hacerlo con la librería spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madrid 0 6 LOC\n",
      "España 24 30 LOC\n",
      "Juan Pérez 53 63 PER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"es\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Madrid\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " es la capital de \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    España\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " y donde vive mi amigo \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Juan Pérez\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ".</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Madrid es la capital de España y donde vive mi amigo Juan Pérez.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EXTRA: Byte Pair Encoding (BPE)**\n",
    "\n",
    "https://github.com/openai/tiktoken\n",
    "\n",
    "\n",
    "El **Byte Pair Encoding** (BPE) es una técnica de compresión de datos que también se ha adaptado para tokenizar texto en el procesamiento del lenguaje natural. Originalmente diseñado para representar datos de manera eficiente, BPE funciona identificando pares de bytes (o caracteres) consecutivos que aparecen con frecuencia y fusionándolos en una sola unidad o token. En el contexto del procesamiento del lenguaje natural, este método ayuda a construir un vocabulario de subpalabras, permitiendo que los modelos de lenguaje manejen palabras raras o desconocidas de manera más eficiente y mitiguen el problema de vocabulario abierto, descomponiendo las palabras en unidades más pequeñas que aún retienen significado semántico.\n",
    "\n",
    "\n",
    "### Paso a Paso:\n",
    "\n",
    "1. **Vocabulario Inicial**: Comienza construyendo un vocabulario inicial. Esto a menudo implica tomar cada palabra en el conjunto de datos y descomponerla en sus caracteres individuales.\n",
    "\n",
    "2. **Conteo de Pares**: En cada iteración, cuenta todos los pares de símbolos/caracteres consecutivos (o pares de byte) en el conjunto de datos.\n",
    "\n",
    "3. **Fusión de Pares más Frecuentes**: Encuentra el par de símbolos más frecuente y los fusiona para formar un nuevo símbolo. Este nuevo símbolo representa ahora una secuencia de caracteres que a menudo aparecen juntos.\n",
    "\n",
    "4. **Iteración**: Se repite el paso 2 y 3 un número predefinido de veces o hasta que se alcance un tamaño de vocabulario deseado. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  token\n",
      "-----  --------\n",
      "69112  'Hola'\n",
      "   11  ','\n",
      "  757  ' me'\n",
      " 9507  ' ll'\n",
      "21781  'amo'\n",
      "29604  ' Juan'\n",
      "   13  '.'\n",
      "29386  ' ¿'\n",
      "96997  'Cómo'\n",
      " 1028  ' te'\n",
      " 9507  ' ll'\n",
      "29189  'amas'\n",
      "90318  ' tú'\n",
      " 4710  '?.'\n",
      " 2206  ' Me'\n",
      " 9507  ' ll'\n",
      "21781  'amo'\n",
      "83305  ' María'\n",
      "   13  '.'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from tabulate import tabulate\n",
    "\n",
    "txt = \"Hola, me llamo Juan. ¿Cómo te llamas tú?. Me llamo María.\"\n",
    "\n",
    "# Convierte txt en una lista de tokens\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# Convierte tokens en una lista de índices\n",
    "ids = enc.encode(txt)\n",
    "\n",
    "data = []\n",
    "\n",
    "for id in ids:\n",
    "    data.append([id, \"'\" + enc.decode([id]) + \"'\" ])\n",
    "    \n",
    "print(tabulate(data, headers=['id', 'token']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Redes neuronales 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Desvanecimiento del gradiente**\n",
    "\n",
    "Las redes convolutivas adolecen de **desvanecimiento del gradiente**, causado por la acumulación de capas consecutivas que, progresivamente, reducen el gradiente hasta hacerlo insignificante.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/dg.svg\">\n",
    "</div>\n",
    "\n",
    "Para recordar cómo se lleva a cabo el entrenamiento sobre una neurona, comenzamos viendo la expresión del error:\n",
    "\n",
    "\n",
    "$$\n",
    "E(\\vec{w}) =  \\sum_j \\left( \\text{Sig}( \\sum_i{w_i  e_i^j} ) - l^j  \\right) ^2\n",
    "$$\n",
    "\n",
    "Si queremos derivar parcialmente con respecto a alguna $w_k$, haremos:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{w_k}} =  \\sum_j \\left[  2 \\left( \\text{Sig}( \\sum_i{w_i  e_i^j} ) - l^j  \\right) \\cdot \\text{Sig}( \\sum_i{w_i  e_i^j} ) \\cdot [1 - \\text{Sig}( \\sum_i{w_i  e_i^j} )  ] \\cdot e_k^j \\right]\n",
    "$$\n",
    "\n",
    "Si nos fijamos en el factor:\n",
    "\n",
    "$$\n",
    "\\text{Sig}( \\sum_i{w_i  e_i^j} ) \\cdot [1 - \\text{Sig}( \\sum_i{w_i  e_i^j} )]\n",
    "$$\n",
    "\n",
    "veremos que si $\\sum_i{w_i  e_i^j}$ crece con valores que se vayan alejando progresivamente del cero, el resultado de la expresión anterior tiende muy rápidamente al cero. Veámoslo con ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivada del sigmoide en 0 =  0.25\n",
      "Derivada del sigmoide en 1 =  0.19661193324148185\n",
      "Derivada del sigmoide en 5 =  0.006648056670790033\n",
      "Derivada del sigmoide en 10 =  4.5395807735907655e-05\n",
      "Derivada del sigmoide en 30 =  9.348077867342945e-14\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sigmoide(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def derivada_sigmoide(x):\n",
    "    return sigmoide(x)*(1-sigmoide(x))\n",
    "\n",
    "print(\"Derivada del sigmoide en 0 = \",derivada_sigmoide(0))\n",
    "print(\"Derivada del sigmoide en 1 = \",derivada_sigmoide(1))\n",
    "print(\"Derivada del sigmoide en 5 = \",derivada_sigmoide(5))\n",
    "print(\"Derivada del sigmoide en 10 = \",derivada_sigmoide(10))\n",
    "print(\"Derivada del sigmoide en 30 = \",derivada_sigmoide(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si analizamos el caso de una red y quisiéramos calcular, por ejemplo, la derivada parcial de $w^2_1$, tendríamos que la expresión es:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{w^2_1}} =  \\sum_j \\left[  2 \\left( \\sigma_1( \\Sigma_1 ) - l^j  \\right) \\cdot \\sigma_1( \\Sigma_1 ) (1- \\sigma_1(\\Sigma_1))  \\cdot \\sigma_2( \\Sigma_2 ) (1- \\sigma_2(\\Sigma_2))  \\cdot \\text{out}_3    \\right]\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/dg_multi.svg\" width=60%>\n",
    "</div>\n",
    "\n",
    "El asunto ahora empeora ya que que tenemos dos factores: $\\sigma_1( \\Sigma_1 ) (1- \\sigma_1(\\Sigma_1))  \\cdot \\sigma_2( \\Sigma_2 ) (1- \\sigma_2(\\Sigma_2))$ en lugar de uno que reducen el gradiente. Esto hace que $w^2_1$ se actualice muy lentamente. Como imaginarás, la cosa va peor a medida que los pesos a actualizar se encuentran en capas más atrás.\n",
    "\n",
    "## **Conexiones residuales**\n",
    "\n",
    "Cuando implementamos una red obligamos, por medio del entrenamiento, que esta mapee una cierta entrada $x$, por medio de una función $F(x)$, en unas determinadas salidas $\\hat{y}$. Es, por tanto, misión de la red encontrar una función $F(x)$ lo suficientemente acertada. En la figura inferior, la red izquierda es la clásica red que hemos estado viendo, pero la red derecha crea un \"enlace\" entre la entrada y la salida de la segunda capa. ¿Para qué? ¿Soluciona eso el problema del desvanecimiento del gradiente?\n",
    "\n",
    "Vemos que las dos capas de red de la izquierda intentan construir $F(x)$, pero en el diseño de la derecha esa conexión directa entre la entrada y la salida obliga a las dos capas de red a que, si quiere tener a la salida la función $F(x)$, debe entonces forzar a que la función $g_2(f(x)) = g_1(f(x)) - f(x)$, para que, después de sumar $f(x)$, la salida sea la $F(x)$ buscada.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/residuals_.svg\" width=60%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función matemática nos va a ayudar a entender la finalidad de esta técnica. Supongamos que tenemos la entrada $x$ a la que se le aplica la función $f$, generando $f(x)$. Ahora el camino del valor $f(x)$ se divide en dos. En primer lugar entra como valor a la función $g$, la cual devuelve $g(f(x))$. Este valor se suma al segundo camino que tomaba $f(x)$, dando como resultado final $g(f(x))+f(x)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"imgs/cadena_derivada_.svg\" width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo interesante de esto viene cuando realizamos su derivada. Si la calculamos, obtendremos la expresión:\n",
    "\n",
    "$$\n",
    "g'(f(x)) \\cdot f'(x) + f'(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si interpretamos que $f$ es una primera capa de una red neuronal y $g$ es la siguiente capa, vemos que la conexión residual nos permite tener la derivada de $f$ sin estar multiplicada por la derivada de $g$, lo cual hacía que el gradiente disminuyera sensiblemente.\n",
    "\n",
    "Esta técnica apareció por primera vez en [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf), pertenciente a Microsoft Research en 2015. Esto dio origen a la red **ResNet**.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/resnet_.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

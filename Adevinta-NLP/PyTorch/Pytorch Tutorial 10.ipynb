{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial de Pytorch 10:  Transfer Learning: VGG16**\n",
    "\n",
    "'Transfer Learning' es un método de entrenamiento de redes neuronales que permite aprovechar el conocimiento adquirido por una red entrenada en un conjunto de datos para entrenar otra red en un conjunto de datos diferente. La idea es que la red pre-entrenada con un conjunto de datos (imágenes) variado y amplio, ha aprendido a extraer características útiles de las imágenes, por lo que se puede utilizar para extraer características de un conjunto de imágenes diferente que, presumiblemente, tiene características similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datos**\n",
    "\n",
    "Preparamos los datos igual que en el modelo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.nn import Dropout\n",
    "\n",
    "# Establecemos semilla\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Definimos las transformaciones que deseas aplicar a las imágenes\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),           # Convierte las imágenes a tensores\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normaliza las imágenes para VGG16\n",
    "])\n",
    "\n",
    "# Ruta donde se encuentran las imágenes\n",
    "data_dir = \"./data/house_rooms\"\n",
    "\n",
    "# Creamos un objeto ImageFolder\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Dividimos el conjunto de datos en entrenamiento (80%) y validación (20%)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Definimos el tamaño del lote y creamos los DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelo**\n",
    "\n",
    "Definimos el modelo usando las \"features\" de un modelo pre-entrenado, en este caso VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo VGG16 preentrenado\n",
    "model_conv = torchvision.models.vgg16()\n",
    "vgg16 = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Congelamos los pesos de las últimas 7 capas convolutivas\n",
    "for param in vgg16.features[:-7].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Obtenemos el número de características de la última capa\n",
    "in_features = vgg16.classifier[0].in_features\n",
    "\n",
    "# Creamos un nuevo clasificador\n",
    "cls = nn.Sequential(\n",
    "    nn.Linear(in_features, 256),  # 4096 -> 256\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, 5),\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Creamos un nuevo modelo con las features de vgg16 y el clasificador personalizado\n",
    "model = nn.Sequential(\n",
    "    vgg16.features,\n",
    "    nn.Flatten(),\n",
    "    cls\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Validación del modelo\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on validation set: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

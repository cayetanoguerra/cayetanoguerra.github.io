
{'loss': 1.1433, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
{'loss': 1.1509, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}
{'loss': 1.1637, 'learning_rate': 3e-06, 'epoch': 0.01}
{'loss': 1.1353, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}
{'loss': 1.1448, 'learning_rate': 5e-06, 'epoch': 0.02}
{'loss': 1.1326, 'learning_rate': 6e-06, 'epoch': 0.02}
{'loss': 1.1448, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.03}
{'loss': 1.1269, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.03}
{'loss': 1.0783, 'learning_rate': 9e-06, 'epoch': 0.04}
{'loss': 1.0713, 'learning_rate': 1e-05, 'epoch': 0.04}
{'loss': 1.0546, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.04}
{'loss': 1.0466, 'learning_rate': 1.2e-05, 'epoch': 0.05}
{'loss': 1.0015, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.05}
{'loss': 0.9986, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.06}
{'loss': 0.965, 'learning_rate': 1.5e-05, 'epoch': 0.06}
{'loss': 0.9482, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.06}
{'loss': 0.8905, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.07}
{'loss': 0.8508, 'learning_rate': 1.8e-05, 'epoch': 0.07}
{'loss': 0.8259, 'learning_rate': 1.9e-05, 'epoch': 0.08}
{'loss': 0.782, 'learning_rate': 2e-05, 'epoch': 0.08}
{'loss': 0.7525, 'learning_rate': 2.1e-05, 'epoch': 0.08}
{'loss': 0.6997, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.09}
{'loss': 0.6464, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.09}
{'loss': 0.6206, 'learning_rate': 2.4e-05, 'epoch': 0.1}
{'loss': 0.5866, 'learning_rate': 2.5e-05, 'epoch': 0.1}
{'loss': 0.5707, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.1}
{'loss': 0.5566, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.11}
{'loss': 0.5183, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.11}
{'loss': 0.5049, 'learning_rate': 2.9e-05, 'epoch': 0.12}
{'loss': 0.496, 'learning_rate': 3e-05, 'epoch': 0.12}
{'loss': 0.4679, 'learning_rate': 3.1e-05, 'epoch': 0.12}
{'loss': 0.463, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.13}
{'loss': 0.4488, 'learning_rate': 3.3e-05, 'epoch': 0.13}
{'loss': 0.4232, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.14}
{'loss': 0.4205, 'learning_rate': 3.5e-05, 'epoch': 0.14}
{'loss': 0.4002, 'learning_rate': 3.6e-05, 'epoch': 0.14}
{'loss': 0.3928, 'learning_rate': 3.7e-05, 'epoch': 0.15}
{'loss': 0.3957, 'learning_rate': 3.8e-05, 'epoch': 0.15}
{'loss': 0.3817, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.16}
{'loss': 0.3787, 'learning_rate': 4e-05, 'epoch': 0.16}
{'loss': 0.3722, 'learning_rate': 4.1e-05, 'epoch': 0.16}
{'loss': 0.3664, 'learning_rate': 4.2e-05, 'epoch': 0.17}
{'loss': 0.358, 'learning_rate': 4.3e-05, 'epoch': 0.17}
{'loss': 0.3467, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.18}
{'loss': 0.3496, 'learning_rate': 4.5e-05, 'epoch': 0.18}
{'loss': 0.3509, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.18}
{'loss': 0.3525, 'learning_rate': 4.7e-05, 'epoch': 0.19}
{'loss': 0.3401, 'learning_rate': 4.8e-05, 'epoch': 0.19}
{'loss': 0.3372, 'learning_rate': 4.9e-05, 'epoch': 0.2}
{'loss': 0.3352, 'learning_rate': 5e-05, 'epoch': 0.2}
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
El número 123456789 en formato textual es: 123456789.sbm', 'C:\Users\bryce\AppData\Roaming\SpaceEngineers\Mods\312976851.sbm\Textures\GUI\Icons\Cubes\Small\Cubes.dds', 'C:\Users
{'loss': 1.1348, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 1.0869, 'learning_rate': 4.993327105298279e-05, 'epoch': 0.01}
{'loss': 1.0311, 'learning_rate': 4.9866542105965566e-05, 'epoch': 0.01}
{'loss': 0.9334, 'learning_rate': 4.9799813158948354e-05, 'epoch': 0.02}
{'loss': 0.8649, 'learning_rate': 4.9733084211931136e-05, 'epoch': 0.02}
{'loss': 0.773, 'learning_rate': 4.9666355264913925e-05, 'epoch': 0.02}
{'loss': 0.713, 'learning_rate': 4.9599626317896706e-05, 'epoch': 0.03}
{'loss': 0.6045, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 0.575, 'learning_rate': 4.979927739863509e-05, 'epoch': 0.01}
{'loss': 0.5435, 'learning_rate': 4.959855479727017e-05, 'epoch': 0.01}
{'loss': 0.4918, 'learning_rate': 4.939783219590526e-05, 'epoch': 0.02}
{'loss': 0.4748, 'learning_rate': 4.919710959454035e-05, 'epoch': 0.02}
{'loss': 0.4511, 'learning_rate': 4.899638699317544e-05, 'epoch': 0.02}
{'loss': 0.454, 'learning_rate': 4.879566439181052e-05, 'epoch': 0.03}
{'loss': 0.4134, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 0.4086, 'learning_rate': 4.5689655172413794e-05, 'epoch': 0.01}
{'loss': 0.4083, 'learning_rate': 4.1379310344827587e-05, 'epoch': 0.01}
{'loss': 0.3852, 'learning_rate': 3.7068965517241385e-05, 'epoch': 0.02}
{'loss': 0.3825, 'learning_rate': 3.275862068965517e-05, 'epoch': 0.02}
{'loss': 0.3715, 'learning_rate': 2.844827586206897e-05, 'epoch': 0.02}
{'loss': 0.3917, 'learning_rate': 2.413793103448276e-05, 'epoch': 0.03}
{'loss': 0.3894, 'learning_rate': 1.9827586206896554e-05, 'epoch': 0.03}
{'loss': 0.3723, 'learning_rate': 1.5517241379310346e-05, 'epoch': 0.04}
{'loss': 0.3718, 'learning_rate': 1.1206896551724138e-05, 'epoch': 0.04}
{'loss': 0.3757, 'learning_rate': 6.896551724137932e-06, 'epoch': 0.04}
{'loss': 0.372, 'learning_rate': 2.586206896551724e-06, 'epoch': 0.05}
{'train_runtime': 46.2408, 'train_samples_per_second': 10.814, 'train_steps_per_second': 2.725, 'train_loss': 0.38665985682654, 'epoch': 0.05}
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/generation/utils.py:1413: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on mps. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('mps') before running `.generate()`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
El número 123456789 en formato textual es: 123456789 hundred hundred thousand hundred thousand hundred thousand hundred hundred hundred!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
{'loss': 0.3541, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 0.3452, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
{'loss': 0.3521, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}
{'loss': 0.3626, 'learning_rate': 3e-06, 'epoch': 0.01}
{'loss': 0.3513, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}
{'loss': 0.3519, 'learning_rate': 5e-06, 'epoch': 0.02}
{'loss': 0.3467, 'learning_rate': 6e-06, 'epoch': 0.02}
{'loss': 0.3685, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.03}
{'loss': 0.3681, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.03}
{'loss': 0.3563, 'learning_rate': 9e-06, 'epoch': 0.04}
{'loss': 0.3551, 'learning_rate': 1e-05, 'epoch': 0.04}
{'loss': 0.3615, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.04}
{'loss': 0.3591, 'learning_rate': 1.2e-05, 'epoch': 0.05}
{'loss': 0.3638, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.05}
{'loss': 0.3514, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.06}
{'loss': 0.3626, 'learning_rate': 1.5e-05, 'epoch': 0.06}
{'loss': 0.3626, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.06}
{'loss': 0.3561, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.07}
{'loss': 0.3522, 'learning_rate': 1.8e-05, 'epoch': 0.07}
{'loss': 0.3551, 'learning_rate': 1.9e-05, 'epoch': 0.08}
{'loss': 0.3421, 'learning_rate': 2e-05, 'epoch': 0.08}
{'loss': 0.352, 'learning_rate': 2.1e-05, 'epoch': 0.08}
{'loss': 0.3451, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.09}
{'loss': 0.3385, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.09}
{'loss': 0.3324, 'learning_rate': 2.4e-05, 'epoch': 0.1}
{'loss': 0.3428, 'learning_rate': 2.5e-05, 'epoch': 0.1}
{'loss': 0.3399, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.1}
{'loss': 0.3407, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.11}
{'loss': 0.3276, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.11}
{'loss': 0.3393, 'learning_rate': 2.9e-05, 'epoch': 0.12}
{'loss': 0.341, 'learning_rate': 3e-05, 'epoch': 0.12}
{'loss': 0.3377, 'learning_rate': 3.1e-05, 'epoch': 0.12}
{'loss': 0.3329, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.13}
{'loss': 0.3286, 'learning_rate': 3.3e-05, 'epoch': 0.13}
{'loss': 0.3284, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.14}
{'loss': 0.3277, 'learning_rate': 3.5e-05, 'epoch': 0.14}
{'loss': 0.3214, 'learning_rate': 3.6e-05, 'epoch': 0.14}
{'loss': 0.325, 'learning_rate': 3.7e-05, 'epoch': 0.15}
{'loss': 0.3243, 'learning_rate': 3.8e-05, 'epoch': 0.15}
{'loss': 0.3235, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.16}
{'loss': 0.3251, 'learning_rate': 4e-05, 'epoch': 0.16}
{'loss': 0.3203, 'learning_rate': 4.1e-05, 'epoch': 0.16}
{'loss': 0.3218, 'learning_rate': 4.2e-05, 'epoch': 0.17}
{'loss': 0.3211, 'learning_rate': 4.3e-05, 'epoch': 0.17}
{'loss': 0.3147, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.18}
{'loss': 0.3147, 'learning_rate': 4.5e-05, 'epoch': 0.18}
{'loss': 0.3188, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.18}
{'loss': 0.3205, 'learning_rate': 4.7e-05, 'epoch': 0.19}
{'loss': 0.3092, 'learning_rate': 4.8e-05, 'epoch': 0.19}
{'loss': 0.3079, 'learning_rate': 4.9e-05, 'epoch': 0.2}
{'loss': 0.311, 'learning_rate': 5e-05, 'epoch': 0.2}
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
El número 123456789 en formato textual es: 123456789, hundred million hundred six eight seven nine nine ninety thousand hundred!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
{'loss': 0.2933, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
{'loss': 0.307, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}
{'loss': 0.3069, 'learning_rate': 3e-06, 'epoch': 0.01}
{'loss': 0.3094, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}
{'loss': 0.2998, 'learning_rate': 5e-06, 'epoch': 0.02}
{'loss': 0.2967, 'learning_rate': 6e-06, 'epoch': 0.02}
{'loss': 0.312, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.03}
{'loss': 0.3083, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.03}
{'loss': 0.2999, 'learning_rate': 9e-06, 'epoch': 0.04}
{'loss': 0.305, 'learning_rate': 1e-05, 'epoch': 0.04}
{'loss': 0.3101, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.04}
{'loss': 0.3097, 'learning_rate': 1.2e-05, 'epoch': 0.05}
{'loss': 0.3097, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.05}
{'loss': 0.3019, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.06}
{'loss': 0.3206, 'learning_rate': 1.5e-05, 'epoch': 0.06}
{'loss': 0.3093, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.06}
{'loss': 0.3027, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.07}
{'loss': 0.3055, 'learning_rate': 1.8e-05, 'epoch': 0.07}
{'loss': 0.3109, 'learning_rate': 1.9e-05, 'epoch': 0.08}
{'loss': 0.2979, 'learning_rate': 2e-05, 'epoch': 0.08}
{'loss': 0.3074, 'learning_rate': 2.1e-05, 'epoch': 0.08}
{'loss': 0.2977, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.09}
{'loss': 0.2952, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.09}
{'loss': 0.2971, 'learning_rate': 2.4e-05, 'epoch': 0.1}
{'loss': 0.3038, 'learning_rate': 2.5e-05, 'epoch': 0.1}
{'loss': 0.3028, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.1}
{'loss': 0.3005, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.11}
{'loss': 0.2888, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.11}
{'loss': 0.3021, 'learning_rate': 2.9e-05, 'epoch': 0.12}
{'loss': 0.3025, 'learning_rate': 3e-05, 'epoch': 0.12}
{'loss': 0.2963, 'learning_rate': 3.1e-05, 'epoch': 0.12}
{'loss': 0.2904, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.13}
{'loss': 0.291, 'learning_rate': 3.3e-05, 'epoch': 0.13}
{'loss': 0.2951, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.14}
{'loss': 0.2924, 'learning_rate': 3.5e-05, 'epoch': 0.14}
{'loss': 0.2899, 'learning_rate': 3.6e-05, 'epoch': 0.14}
{'loss': 0.2907, 'learning_rate': 3.7e-05, 'epoch': 0.15}
{'loss': 0.2938, 'learning_rate': 3.8e-05, 'epoch': 0.15}
{'loss': 0.2939, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.16}
{'loss': 0.2971, 'learning_rate': 4e-05, 'epoch': 0.16}
{'loss': 0.2908, 'learning_rate': 4.1e-05, 'epoch': 0.16}
{'loss': 0.293, 'learning_rate': 4.2e-05, 'epoch': 0.17}
{'loss': 0.2994, 'learning_rate': 4.3e-05, 'epoch': 0.17}
{'loss': 0.2911, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.18}
{'loss': 0.289, 'learning_rate': 4.5e-05, 'epoch': 0.18}
{'loss': 0.2939, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.18}
{'loss': 0.2928, 'learning_rate': 4.7e-05, 'epoch': 0.19}
{'loss': 0.2826, 'learning_rate': 4.8e-05, 'epoch': 0.19}
{'loss': 0.2823, 'learning_rate': 4.9e-05, 'epoch': 0.2}
{'loss': 0.2915, 'learning_rate': 5e-05, 'epoch': 0.2}
{'loss': 0.2786, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
{'loss': 0.2881, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}
{'loss': 0.2864, 'learning_rate': 3e-06, 'epoch': 0.01}
{'loss': 0.2936, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}
{'loss': 0.2813, 'learning_rate': 5e-06, 'epoch': 0.02}
{'loss': 0.2785, 'learning_rate': 6e-06, 'epoch': 0.02}
{'loss': 0.2915, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.03}
{'loss': 0.2898, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.03}
{'loss': 0.2828, 'learning_rate': 9e-06, 'epoch': 0.04}
{'loss': 0.2882, 'learning_rate': 1e-05, 'epoch': 0.04}
{'loss': 0.2925, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.04}
{'loss': 0.2906, 'learning_rate': 1.2e-05, 'epoch': 0.05}
{'loss': 0.2886, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.05}
{'loss': 0.2839, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.06}
{'loss': 0.3036, 'learning_rate': 1.5e-05, 'epoch': 0.06}
{'loss': 0.2909, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.06}
{'loss': 0.2849, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.07}
{'loss': 0.2878, 'learning_rate': 1.8e-05, 'epoch': 0.07}
{'loss': 0.2929, 'learning_rate': 1.9e-05, 'epoch': 0.08}
{'loss': 0.2806, 'learning_rate': 2e-05, 'epoch': 0.08}
{'loss': 0.2888, 'learning_rate': 2.1e-05, 'epoch': 0.08}
{'loss': 0.2772, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.09}
{'loss': 0.2765, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.09}
{'loss': 0.2789, 'learning_rate': 2.4e-05, 'epoch': 0.1}
{'loss': 0.2848, 'learning_rate': 2.5e-05, 'epoch': 0.1}
{'loss': 0.2841, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.1}
{'loss': 0.2816, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.11}
{'loss': 0.2721, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.11}
{'loss': 0.2838, 'learning_rate': 2.9e-05, 'epoch': 0.12}
{'loss': 0.2844, 'learning_rate': 3e-05, 'epoch': 0.12}
{'loss': 0.2769, 'learning_rate': 3.1e-05, 'epoch': 0.12}
{'loss': 0.2694, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.13}
{'loss': 0.274, 'learning_rate': 3.3e-05, 'epoch': 0.13}
{'loss': 0.277, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.14}
{'loss': 0.274, 'learning_rate': 3.5e-05, 'epoch': 0.14}
{'loss': 0.2734, 'learning_rate': 3.6e-05, 'epoch': 0.14}
{'loss': 0.2722, 'learning_rate': 3.7e-05, 'epoch': 0.15}
{'loss': 0.2768, 'learning_rate': 3.8e-05, 'epoch': 0.15}
{'loss': 0.2747, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.16}
{'loss': 0.2802, 'learning_rate': 4e-05, 'epoch': 0.16}
{'loss': 0.2746, 'learning_rate': 4.1e-05, 'epoch': 0.16}
{'loss': 0.2769, 'learning_rate': 4.2e-05, 'epoch': 0.17}
{'loss': 0.2835, 'learning_rate': 4.3e-05, 'epoch': 0.17}
{'loss': 0.2726, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.18}
{'loss': 0.273, 'learning_rate': 4.5e-05, 'epoch': 0.18}
{'loss': 0.2767, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.18}
{'loss': 0.272, 'learning_rate': 4.7e-05, 'epoch': 0.19}
{'loss': 0.2622, 'learning_rate': 4.8e-05, 'epoch': 0.19}
{'loss': 0.2633, 'learning_rate': 4.9e-05, 'epoch': 0.2}
{'loss': 0.2754, 'learning_rate': 5e-05, 'epoch': 0.2}
{'loss': 0.2831, 'learning_rate': 4.975012493753124e-05, 'epoch': 0.2}
{'loss': 0.2754, 'learning_rate': 4.950024987506247e-05, 'epoch': 0.21}
{'loss': 0.2778, 'learning_rate': 4.9250374812593707e-05, 'epoch': 0.21}
{'loss': 0.2754, 'learning_rate': 4.900049975012494e-05, 'epoch': 0.22}
{'loss': 0.2786, 'learning_rate': 4.8750624687656176e-05, 'epoch': 0.22}
{'loss': 0.2817, 'learning_rate': 4.850074962518741e-05, 'epoch': 0.22}
{'loss': 0.268, 'learning_rate': 4.8250874562718645e-05, 'epoch': 0.23}
{'loss': 0.2711, 'learning_rate': 4.800099950024988e-05, 'epoch': 0.23}
{'loss': 0.2704, 'learning_rate': 4.7751124437781115e-05, 'epoch': 0.24}
{'loss': 0.2732, 'learning_rate': 4.750124937531234e-05, 'epoch': 0.24}
{'loss': 0.2674, 'learning_rate': 4.7251374312843584e-05, 'epoch': 0.24}
{'loss': 0.2692, 'learning_rate': 4.700149925037481e-05, 'epoch': 0.25}
{'loss': 0.2665, 'learning_rate': 4.6751624187906054e-05, 'epoch': 0.25}
{'loss': 0.2727, 'learning_rate': 4.650174912543728e-05, 'epoch': 0.26}
{'loss': 0.2682, 'learning_rate': 4.625187406296852e-05, 'epoch': 0.26}
{'loss': 0.2732, 'learning_rate': 4.600199900049975e-05, 'epoch': 0.26}
{'loss': 0.2677, 'learning_rate': 4.5752123938030986e-05, 'epoch': 0.27}
{'loss': 0.2745, 'learning_rate': 4.550224887556222e-05, 'epoch': 0.27}
{'loss': 0.2622, 'learning_rate': 4.5252373813093455e-05, 'epoch': 0.28}
{'loss': 0.2799, 'learning_rate': 4.500249875062469e-05, 'epoch': 0.28}
{'loss': 0.2744, 'learning_rate': 4.4752623688155925e-05, 'epoch': 0.28}
{'loss': 0.2636, 'learning_rate': 4.450274862568716e-05, 'epoch': 0.29}
{'loss': 0.2613, 'learning_rate': 4.4252873563218394e-05, 'epoch': 0.29}
{'loss': 0.2768, 'learning_rate': 4.400299850074963e-05, 'epoch': 0.3}
{'loss': 0.269, 'learning_rate': 4.3753123438280864e-05, 'epoch': 0.3}
{'loss': 0.2665, 'learning_rate': 4.350324837581209e-05, 'epoch': 0.3}
{'loss': 0.2575, 'learning_rate': 4.325337331334333e-05, 'epoch': 0.31}
{'loss': 0.2736, 'learning_rate': 4.300349825087456e-05, 'epoch': 0.31}
{'loss': 0.2639, 'learning_rate': 4.27536231884058e-05, 'epoch': 0.32}
{'loss': 0.2563, 'learning_rate': 4.250374812593703e-05, 'epoch': 0.32}
{'loss': 0.2593, 'learning_rate': 4.225387306346827e-05, 'epoch': 0.32}
{'loss': 0.2691, 'learning_rate': 4.20039980009995e-05, 'epoch': 0.33}
{'loss': 0.2657, 'learning_rate': 4.1754122938530734e-05, 'epoch': 0.33}
{'loss': 0.2712, 'learning_rate': 4.150424787606197e-05, 'epoch': 0.34}
{'loss': 0.2583, 'learning_rate': 4.1254372813593204e-05, 'epoch': 0.34}
{'loss': 0.2514, 'learning_rate': 4.100449775112444e-05, 'epoch': 0.34}
{'loss': 0.2463, 'learning_rate': 4.075462268865567e-05, 'epoch': 0.35}
{'loss': 0.2545, 'learning_rate': 4.050474762618691e-05, 'epoch': 0.35}
{'loss': 0.2595, 'learning_rate': 4.025487256371814e-05, 'epoch': 0.36}
{'loss': 0.2523, 'learning_rate': 4.000499750124938e-05, 'epoch': 0.36}
{'loss': 0.2559, 'learning_rate': 3.975512243878061e-05, 'epoch': 0.36}
{'loss': 0.2476, 'learning_rate': 3.950524737631185e-05, 'epoch': 0.37}
{'loss': 0.2684, 'learning_rate': 3.925537231384308e-05, 'epoch': 0.37}
{'loss': 0.2514, 'learning_rate': 3.900549725137431e-05, 'epoch': 0.38}
{'loss': 0.2502, 'learning_rate': 3.875562218890555e-05, 'epoch': 0.38}
{'loss': 0.2625, 'learning_rate': 3.850574712643678e-05, 'epoch': 0.38}
{'loss': 0.2567, 'learning_rate': 3.825587206396802e-05, 'epoch': 0.39}
{'loss': 0.2638, 'learning_rate': 3.800599700149925e-05, 'epoch': 0.39}
{'loss': 0.2585, 'learning_rate': 3.775612193903049e-05, 'epoch': 0.4}
{'loss': 0.2563, 'learning_rate': 3.7506246876561725e-05, 'epoch': 0.4}
{'loss': 0.2562, 'learning_rate': 3.725637181409295e-05, 'epoch': 0.4}
{'loss': 0.253, 'learning_rate': 3.7006496751624194e-05, 'epoch': 0.41}
{'loss': 0.2608, 'learning_rate': 3.675662168915542e-05, 'epoch': 0.41}
{'loss': 0.2506, 'learning_rate': 3.6506746626686664e-05, 'epoch': 0.42}
{'loss': 0.2553, 'learning_rate': 3.625687156421789e-05, 'epoch': 0.42}
{'loss': 0.2536, 'learning_rate': 3.6006996501749126e-05, 'epoch': 0.42}
{'loss': 0.2568, 'learning_rate': 3.575712143928036e-05, 'epoch': 0.43}
{'loss': 0.2595, 'learning_rate': 3.5507246376811596e-05, 'epoch': 0.43}
{'loss': 0.2557, 'learning_rate': 3.525737131434283e-05, 'epoch': 0.44}
{'loss': 0.2553, 'learning_rate': 3.5007496251874065e-05, 'epoch': 0.44}
{'loss': 0.2567, 'learning_rate': 3.47576211894053e-05, 'epoch': 0.44}
{'loss': 0.2518, 'learning_rate': 3.4507746126936534e-05, 'epoch': 0.45}
{'loss': 0.2484, 'learning_rate': 3.425787106446777e-05, 'epoch': 0.45}
{'loss': 0.2445, 'learning_rate': 3.4007996001999004e-05, 'epoch': 0.46}
{'loss': 0.2479, 'learning_rate': 3.375812093953024e-05, 'epoch': 0.46}
{'loss': 0.2446, 'learning_rate': 3.350824587706147e-05, 'epoch': 0.46}
{'loss': 0.2498, 'learning_rate': 3.32583708145927e-05, 'epoch': 0.47}
{'loss': 0.2564, 'learning_rate': 3.300849575212394e-05, 'epoch': 0.47}
{'loss': 0.2609, 'learning_rate': 3.275862068965517e-05, 'epoch': 0.48}
{'loss': 0.2487, 'learning_rate': 3.250874562718641e-05, 'epoch': 0.48}
{'loss': 0.2575, 'learning_rate': 3.225887056471764e-05, 'epoch': 0.48}
{'loss': 0.2556, 'learning_rate': 3.200899550224888e-05, 'epoch': 0.49}
{'loss': 0.2474, 'learning_rate': 3.175912043978011e-05, 'epoch': 0.49}
{'loss': 0.2482, 'learning_rate': 3.1509245377311344e-05, 'epoch': 0.5}
{'loss': 0.2559, 'learning_rate': 3.125937031484258e-05, 'epoch': 0.5}
{'loss': 0.2413, 'learning_rate': 3.1009495252373814e-05, 'epoch': 0.5}
{'loss': 0.2437, 'learning_rate': 3.075962018990505e-05, 'epoch': 0.51}
{'loss': 0.2515, 'learning_rate': 3.0509745127436283e-05, 'epoch': 0.51}
{'loss': 0.2341, 'learning_rate': 3.0259870064967514e-05, 'epoch': 0.52}
{'loss': 0.2515, 'learning_rate': 3.0009995002498753e-05, 'epoch': 0.52}
{'loss': 0.2414, 'learning_rate': 2.9760119940029984e-05, 'epoch': 0.52}
{'loss': 0.2501, 'learning_rate': 2.9510244877561222e-05, 'epoch': 0.53}
{'loss': 0.2396, 'learning_rate': 2.9260369815092453e-05, 'epoch': 0.53}
{'loss': 0.247, 'learning_rate': 2.901049475262369e-05, 'epoch': 0.54}
{'loss': 0.2518, 'learning_rate': 2.8760619690154923e-05, 'epoch': 0.54}
{'loss': 0.2438, 'learning_rate': 2.8510744627686158e-05, 'epoch': 0.54}
{'loss': 0.2466, 'learning_rate': 2.826086956521739e-05, 'epoch': 0.55}
{'loss': 0.2598, 'learning_rate': 2.8010994502748627e-05, 'epoch': 0.55}
{'loss': 0.2471, 'learning_rate': 2.7761119440279858e-05, 'epoch': 0.56}
{'loss': 0.2478, 'learning_rate': 2.7511244377811096e-05, 'epoch': 0.56}
{'loss': 0.2495, 'learning_rate': 2.7261369315342328e-05, 'epoch': 0.56}
{'loss': 0.24, 'learning_rate': 2.7011494252873566e-05, 'epoch': 0.57}
{'loss': 0.2482, 'learning_rate': 2.6761619190404797e-05, 'epoch': 0.57}
{'loss': 0.2396, 'learning_rate': 2.6511744127936032e-05, 'epoch': 0.58}
{'loss': 0.2431, 'learning_rate': 2.6261869065467267e-05, 'epoch': 0.58}
{'loss': 0.2435, 'learning_rate': 2.60119940029985e-05, 'epoch': 0.58}
{'loss': 0.2452, 'learning_rate': 2.5762118940529733e-05, 'epoch': 0.59}
{'loss': 0.2473, 'learning_rate': 2.551224387806097e-05, 'epoch': 0.59}
{'loss': 0.2424, 'learning_rate': 2.526236881559221e-05, 'epoch': 0.6}
{'loss': 0.2362, 'learning_rate': 2.501249375312344e-05, 'epoch': 0.6}
{'loss': 0.2532, 'learning_rate': 2.4762618690654675e-05, 'epoch': 0.6}
{'loss': 0.2487, 'learning_rate': 2.451274362818591e-05, 'epoch': 0.61}
{'loss': 0.2484, 'learning_rate': 2.426286856571714e-05, 'epoch': 0.61}
{'loss': 0.239, 'learning_rate': 2.4012993503248376e-05, 'epoch': 0.62}
{'loss': 0.2431, 'learning_rate': 2.376311844077961e-05, 'epoch': 0.62}
{'loss': 0.2427, 'learning_rate': 2.3513243378310845e-05, 'epoch': 0.62}
{'loss': 0.2464, 'learning_rate': 2.326336831584208e-05, 'epoch': 0.63}
{'loss': 0.2414, 'learning_rate': 2.3013493253373314e-05, 'epoch': 0.63}
{'loss': 0.2399, 'learning_rate': 2.276361819090455e-05, 'epoch': 0.64}
{'loss': 0.246, 'learning_rate': 2.2513743128435784e-05, 'epoch': 0.64}
{'loss': 0.239, 'learning_rate': 2.2263868065967015e-05, 'epoch': 0.64}
{'loss': 0.2436, 'learning_rate': 2.201399300349825e-05, 'epoch': 0.65}
{'loss': 0.2385, 'learning_rate': 2.1764117941029485e-05, 'epoch': 0.65}
{'loss': 0.2401, 'learning_rate': 2.151424287856072e-05, 'epoch': 0.66}
{'loss': 0.2426, 'learning_rate': 2.1264367816091954e-05, 'epoch': 0.66}
{'loss': 0.238, 'learning_rate': 2.101449275362319e-05, 'epoch': 0.66}
{'loss': 0.2433, 'learning_rate': 2.0764617691154424e-05, 'epoch': 0.67}
{'loss': 0.2375, 'learning_rate': 2.0514742628685658e-05, 'epoch': 0.67}
{'loss': 0.2385, 'learning_rate': 2.0264867566216893e-05, 'epoch': 0.68}
{'loss': 0.2406, 'learning_rate': 2.0014992503748124e-05, 'epoch': 0.68}
{'loss': 0.2347, 'learning_rate': 1.976511744127936e-05, 'epoch': 0.68}
{'loss': 0.2456, 'learning_rate': 1.9515242378810594e-05, 'epoch': 0.69}
{'loss': 0.2288, 'learning_rate': 1.9265367316341832e-05, 'epoch': 0.69}
{'loss': 0.2347, 'learning_rate': 1.9015492253873067e-05, 'epoch': 0.7}
{'loss': 0.2398, 'learning_rate': 1.87656171914043e-05, 'epoch': 0.7}
{'loss': 0.2444, 'learning_rate': 1.8515742128935533e-05, 'epoch': 0.7}
{'loss': 0.2474, 'learning_rate': 1.8265867066466767e-05, 'epoch': 0.71}
{'loss': 0.2385, 'learning_rate': 1.8015992003998002e-05, 'epoch': 0.71}
{'loss': 0.2386, 'learning_rate': 1.7766116941529237e-05, 'epoch': 0.72}
{'loss': 0.2349, 'learning_rate': 1.751624187906047e-05, 'epoch': 0.72}
{'loss': 0.2373, 'learning_rate': 1.7266366816591706e-05, 'epoch': 0.72}
{'loss': 0.2439, 'learning_rate': 1.701649175412294e-05, 'epoch': 0.73}
{'loss': 0.2373, 'learning_rate': 1.6766616691654176e-05, 'epoch': 0.73}
{'loss': 0.2366, 'learning_rate': 1.651674162918541e-05, 'epoch': 0.74}
{'loss': 0.2449, 'learning_rate': 1.626686656671664e-05, 'epoch': 0.74}
{'loss': 0.2413, 'learning_rate': 1.6016991504247876e-05, 'epoch': 0.74}
{'loss': 0.2296, 'learning_rate': 1.576711644177911e-05, 'epoch': 0.75}
{'loss': 0.2343, 'learning_rate': 1.5517241379310346e-05, 'epoch': 0.75}
{'loss': 0.2473, 'learning_rate': 1.526736631684158e-05, 'epoch': 0.76}
{'loss': 0.245, 'learning_rate': 1.5017491254372815e-05, 'epoch': 0.76}
{'loss': 0.2312, 'learning_rate': 1.4767616191904048e-05, 'epoch': 0.76}
{'loss': 0.2412, 'learning_rate': 1.4517741129435283e-05, 'epoch': 0.77}
{'loss': 0.242, 'learning_rate': 1.4267866066966518e-05, 'epoch': 0.77}
{'loss': 0.2374, 'learning_rate': 1.4017991004497752e-05, 'epoch': 0.78}
{'loss': 0.2378, 'learning_rate': 1.3768115942028985e-05, 'epoch': 0.78}
{'loss': 0.2398, 'learning_rate': 1.351824087956022e-05, 'epoch': 0.78}
{'loss': 0.2395, 'learning_rate': 1.3268365817091455e-05, 'epoch': 0.79}
{'loss': 0.2363, 'learning_rate': 1.301849075462269e-05, 'epoch': 0.79}
{'loss': 0.2395, 'learning_rate': 1.2768615692153924e-05, 'epoch': 0.8}
{'loss': 0.235, 'learning_rate': 1.2518740629685157e-05, 'epoch': 0.8}
{'loss': 0.2372, 'learning_rate': 1.2268865567216392e-05, 'epoch': 0.8}
{'loss': 0.2403, 'learning_rate': 1.2018990504747627e-05, 'epoch': 0.81}
{'loss': 0.227, 'learning_rate': 1.1769115442278861e-05, 'epoch': 0.81}
{'loss': 0.2451, 'learning_rate': 1.1519240379810094e-05, 'epoch': 0.82}
{'loss': 0.2361, 'learning_rate': 1.126936531734133e-05, 'epoch': 0.82}
{'loss': 0.2389, 'learning_rate': 1.1019490254872564e-05, 'epoch': 0.82}
{'loss': 0.2421, 'learning_rate': 1.0769615192403799e-05, 'epoch': 0.83}
{'loss': 0.2324, 'learning_rate': 1.0519740129935032e-05, 'epoch': 0.83}
{'loss': 0.2311, 'learning_rate': 1.0269865067466268e-05, 'epoch': 0.84}
{'loss': 0.2445, 'learning_rate': 1.0019990004997503e-05, 'epoch': 0.84}
{'loss': 0.2338, 'learning_rate': 9.770114942528738e-06, 'epoch': 0.84}
{'loss': 0.2375, 'learning_rate': 9.52023988005997e-06, 'epoch': 0.85}
{'loss': 0.2377, 'learning_rate': 9.270364817591205e-06, 'epoch': 0.85}
{'loss': 0.2371, 'learning_rate': 9.02048975512244e-06, 'epoch': 0.86}
{'loss': 0.2286, 'learning_rate': 8.770614692653675e-06, 'epoch': 0.86}
{'loss': 0.2414, 'learning_rate': 8.520739630184908e-06, 'epoch': 0.86}
{'loss': 0.2334, 'learning_rate': 8.270864567716142e-06, 'epoch': 0.87}
{'loss': 0.2443, 'learning_rate': 8.020989505247377e-06, 'epoch': 0.87}
{'loss': 0.2399, 'learning_rate': 7.771114442778612e-06, 'epoch': 0.88}
{'loss': 0.2256, 'learning_rate': 7.521239380309846e-06, 'epoch': 0.88}
{'loss': 0.2467, 'learning_rate': 7.27136431784108e-06, 'epoch': 0.88}
{'loss': 0.2333, 'learning_rate': 7.021489255372314e-06, 'epoch': 0.89}
{'loss': 0.2369, 'learning_rate': 6.771614192903548e-06, 'epoch': 0.89}
{'loss': 0.2398, 'learning_rate': 6.521739130434783e-06, 'epoch': 0.9}
{'loss': 0.236, 'learning_rate': 6.271864067966017e-06, 'epoch': 0.9}
{'loss': 0.2374, 'learning_rate': 6.0219890054972515e-06, 'epoch': 0.9}
{'loss': 0.2414, 'learning_rate': 5.772113943028486e-06, 'epoch': 0.91}
{'loss': 0.232, 'learning_rate': 5.522238880559721e-06, 'epoch': 0.91}
{'loss': 0.2282, 'learning_rate': 5.272363818090955e-06, 'epoch': 0.92}
{'loss': 0.2363, 'learning_rate': 5.0224887556221895e-06, 'epoch': 0.92}
{'loss': 0.2377, 'learning_rate': 4.772613693153423e-06, 'epoch': 0.92}
{'loss': 0.2371, 'learning_rate': 4.522738630684658e-06, 'epoch': 0.93}
{'loss': 0.2289, 'learning_rate': 4.272863568215892e-06, 'epoch': 0.93}
{'loss': 0.2308, 'learning_rate': 4.022988505747127e-06, 'epoch': 0.94}
{'loss': 0.231, 'learning_rate': 3.773113443278361e-06, 'epoch': 0.94}
{'loss': 0.2348, 'learning_rate': 3.5232383808095952e-06, 'epoch': 0.94}
{'loss': 0.2477, 'learning_rate': 3.2733633183408295e-06, 'epoch': 0.95}
{'loss': 0.2381, 'learning_rate': 3.0234882558720643e-06, 'epoch': 0.95}
{'loss': 0.2407, 'learning_rate': 2.7736131934032985e-06, 'epoch': 0.96}
{'loss': 0.2272, 'learning_rate': 2.523738130934533e-06, 'epoch': 0.96}
{'loss': 0.2405, 'learning_rate': 2.273863068465767e-06, 'epoch': 0.96}
{'loss': 0.2355, 'learning_rate': 2.0239880059970014e-06, 'epoch': 0.97}
{'loss': 0.2235, 'learning_rate': 1.7741129435282361e-06, 'epoch': 0.97}
{'loss': 0.2223, 'learning_rate': 1.5242378810594704e-06, 'epoch': 0.98}
{'loss': 0.2392, 'learning_rate': 1.2743628185907047e-06, 'epoch': 0.98}
{'loss': 0.2314, 'learning_rate': 1.024487756121939e-06, 'epoch': 0.98}
{'loss': 0.2311, 'learning_rate': 7.746126936531734e-07, 'epoch': 0.99}
{'loss': 0.2314, 'learning_rate': 5.247376311844078e-07, 'epoch': 0.99}
{'loss': 0.229, 'learning_rate': 2.748625687156422e-07, 'epoch': 1.0}
{'loss': 0.2326, 'learning_rate': 2.4987506246876563e-08, 'epoch': 1.0}
{'train_runtime': 923.8345, 'train_samples_per_second': 10.826, 'train_steps_per_second': 2.707, 'train_loss': 0.25427277436832196, 'epoch': 1.0}
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
El número 123456789 en formato textual es: 123456789 nine forty million hundred five seven six seven eight seven ninety!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
El número 987654321 en formato textual es: 987654321 seven seventy million hundred three seven four three forty thousand hundred one!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
El número 98 en formato textual es: 989914, hundred million hundred thousand hundred one fourteen!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
El número 372409679 en formato textual es: 372409679 one twenty million hundred nine nine six nine sixty!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/hf-logo-with-title.svg\" width=\"500\">\n",
    "\n",
    "https://huggingface.co/docs/hub/index\n",
    "\n",
    "Hugging Face es una empresa enfocada en inteligencia artificial y aprendizaje automático, conocida principalmente por su biblioteca \"Transformers\" que proporciona modelos preentrenados para procesamiento de lenguaje natural y otras tareas. Ofrecen una plataforma colaborativa, el Hugging Face Hub, donde los usuarios pueden compartir y trabajar en modelos de IA, conjuntos de datos y aplicaciones. La compañía promueve la democratización y accesibilidad de la IA a través de soluciones de código abierto y ciencia abierta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8676055073738098}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"Yesterday my mother in law died, finally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, este **pipeline** selecciona un modelo preentrenado específico que ha sido ajustado para el análisis de sentimientos en inglés. El modelo se descarga y almacena en caché cuando creas el objeto clasificador. Si vuelves a ejecutar el comando, se utilizará el modelo almacenado en caché en lugar de descargar el modelo de nuevo. Vamos a cargar un modelo preentrenado para el análisis de sentimientos en multi-idoma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '2 stars', 'score': 0.4662598669528961}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_multi = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "classifier_multi(\"No me gusta esa playa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pasos principales cuando pasas algún texto a un pipeline son:\n",
    "\n",
    "1. El texto se preprocesa en un formato que el modelo puede entender.\n",
    "2. Las entradas preprocesadas se pasan al modelo.\n",
    "3. Las predicciones del modelo se post-procesan, para que puedas entenderlas.\n",
    "\n",
    "Algunas de los pipeline disponibles son:\n",
    "\n",
    "- Extracción de la representación vectorial de un texto\n",
    "- Rellenar-enmascaramientos\n",
    "- Reconocimiento de entidades nombradas (NER)\n",
    "- Preguntas y respuestas\n",
    "- Análisis de sentimientos\n",
    "- Resúmenes\n",
    "- Generación de texto\n",
    "- Traducción\n",
    "- Clasificación \"zero-shot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clasificación *zero-shot***\n",
    "\n",
    "Esta tarea puede ser muy desafiante ya que necesitamos clasificar textos que no han sido etiquetados. Este es un escenario común en proyectos del mundo real porque anotar texto suele ser un proceso que consume mucho tiempo y requiere experiencia en el dominio. Para este caso de uso, el *pipeline* de clasificación *zero-shot* es muy útil: te permite especificar qué etiquetas usar para la clasificación, por lo que no tienes que depender de las etiquetas del modelo preentrenado. Ya has visto cómo el modelo puede clasificar una oración como positiva o negativa, pero también puede clasificar el texto usando cualquier otro conjunto de etiquetas que consideres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library in Natural Language Processing',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8677834868431091, 0.09560371190309525, 0.03661278635263443]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library in Natural Language Processing\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reconocimiento de entidades nombradas**\n",
    "\n",
    "El reconocimiento de entidades nombradas (NER) es una tarea donde el modelo tiene que encontrar qué partes del texto de entrada corresponden a entidades como personas, ubicaciones u organizaciones. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9991522,\n",
       "  'word': 'Juan Pérez',\n",
       "  'start': 11,\n",
       "  'end': 21},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.80377805,\n",
       "  'word': 'University of Las Palmas',\n",
       "  'start': 40,\n",
       "  'end': 64},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.7861867,\n",
       "  'word': 'of Gran Canaria',\n",
       "  'start': 65,\n",
       "  'end': 80},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9997646,\n",
       "  'word': 'Spain',\n",
       "  'start': 84,\n",
       "  'end': 89}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Juan Pérez and I work at the University of Las Palmas of Gran Canaria in Spain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preguntas y respuestas**\n",
    "\n",
    "El *pipeline* de preguntas y respuestas responde preguntas utilizando información de un contexto dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9867210984230042, 'start': 99, 'end': 109, 'answer': 'colleagues'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"What are María Pérez and Juan Pérez?\",\n",
    "    context=\"Juan Pérez and María Pérez work at the University of Las Palmas of Gran Canaria in Spain. They are colleagues. María Pérez is 32 years old and Juan Pérez is 45.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resúmenes**\n",
    "\n",
    "El resumen es la tarea de reducir un texto a un texto más corto mientras se mantienen todos (o la mayoría) de los aspectos importantes mencionados en el texto. Aquí hay un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Speech to text**\n",
    "\n",
    "Conversión del habla en texto escrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "t2s = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n",
    "t2s(\"data/voz.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Todo en uno: Transformer**\n",
    "\n",
    "Un Transformer es un modelo de aprendizaje profundo que se ha vuelto extremadamente popular en el campo del procesamiento del lenguaje natural (NLP). Fue introducido en el artículo \"Attention is All You Need\" por Vaswani et al. en 2017. Los Transformers eliminan la necesidad de métodos de aprendizaje supervisado previos, como las redes neuronales recurrentes (RNN) o las redes neuronales convolutivas (CNN), al introducir una nueva arquitectura llamada codificador-decodificador. Los Transformers han demostrado ser muy eficaces en una variedad de tareas de NLP, incluido el modelado de lenguaje, la traducción automática y el resumen de texto.\n",
    "\n",
    "El modelo T5 (que veremos en el ejemplo), o Text-to-Text Transfer Transformer, fue presentado en un artículo titulado \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" publicado por investigadores de Google en octubre de 2019. Este trabajo introdujo el modelo T5 junto con su enfoque innovador para manejar todas las tareas de procesamiento de lenguaje natural (NLP) como problemas de generación de texto, utilizando un formato de texto a texto.\n",
    "\n",
    "\n",
    "`T5ForConditionalGeneration` es una clase en la biblioteca transformers de Hugging Face que proporciona una implementación del modelo T5 diseñada específicamente para tareas de generación de texto, como traducción de idiomas, resumen de texto, y generación de texto basada en prompts. La idea detrás de T5 es tratar todos los problemas de procesamiento de lenguaje natural como problemas de generación de texto. Por ejemplo, en lugar de tener una arquitectura específica para traducción y otra diferente para resumen, T5 puede ser entrenado para realizar ambas tareas (y muchas otras) simplemente cambiando el texto de entrada que se le proporciona. Esto se logra prefijando el texto de entrada con un indicador de la tarea a realizar, como \"traducir del inglés al alemán\" o \"resumir\", seguido por el texto a procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Traducción**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglés a Francés: Les Beatles sont un groupe rock anglais formé à Liverpool en 1960, composé de John Lennon, Paul McCartney, George Harrison et Ringo Starr.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglés a Francés: Ils sont considérés comme le groupe le plus influent de tous les temps et ont joué un rôle essentiel dans le développement de la contreculture des années 1960 et la reconnaissance de la musique populaire comme forme d'art.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def translate(text, model_name=\"t5-base\", task=\"translate English to French\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Ejemplo de traducción del inglés al francés\n",
    "text_en_to_es = \"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr.\"\n",
    "print(\"Inglés a Francés:\", translate(text_en_to_es, task=\"translate English to French\"))\n",
    "text_en_to_es = \"They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\"\n",
    "print(\"Inglés a Francés:\", translate(text_en_to_es, task=\"translate English to French\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generación de resúmenes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen: the Beatles were an english rock band formed in 1960. they are regarded as the most influential band of all time. they were integral to the development of 1960s counterculture.\n"
     ]
    }
   ],
   "source": [
    "def sumarize(text, model_name=\"t5-base\", task=\"summarize\"):\n",
    "    # Cargamos el tokenizador y el modelo\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Preparamos la entrada\n",
    "    input_text = f\"{task}: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generamos la salida\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return summarized_text\n",
    "\n",
    "# Ejemplo de resumen\n",
    "text = \"\"\"\n",
    "\"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr. They are regarded as the most influential band of all time and were integral to the development of 1960s counterculture and the recognition of popular music as an art form.\n",
    "\"\"\"\n",
    "print(\"Resumen:\", sumarize(text, task=\"summarize\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path=\"./models/mistral-7b-instruct-v0.2.Q4_K_S.gguf\",  # Download the model file first\n",
    "  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference example\n",
    "output = llm(\n",
    "  \"<s>[INST] Juan y Ana están en la misma habitación. Juan coloca una manzana dentro de un cajón y se va de la habitación. Cuando Ana sabe que Juan no puede verla, coge la manzana y la esconde bajo la cama. Ahora llega Juan a la habitación, ¿dónde creerá Ana que Juan buscará la manzana? [/INST]\", # Prompt\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Hugging Face Cheat Sheet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principales clases y métodos de la biblioteca Hugging Face Transformers, enfocándonos en los aspectos más comunes utilizados para el manejo de modelos, tokenización, entrenamiento, y más. Este esquema te ayudará a entender mejor cómo interactúan estas clases y métodos dentro del ecosistema de Hugging Face.\n",
    "\n",
    "### 1. **Modelos**\n",
    "\n",
    "- **`AutoModel`**\n",
    "  - Métodos:\n",
    "    - `from_pretrained('model-name')`: Carga un modelo preentrenado basado en el nombre.\n",
    "    - `save_pretrained('directory')`: Guarda el modelo en un directorio especificado.\n",
    "  - Uso: Para cargar o guardar modelos de manera genérica sin especificar la arquitectura exacta.\n",
    "\n",
    "- **`AutoModelForSequenceClassification`**, **`AutoModelForTokenClassification`**, **`AutoModelForQuestionAnswering`**\n",
    "  - Uso: Clases especializadas para tareas específicas como clasificación de secuencias, clasificación de tokens y pregunta-respuesta, respectivamente.\n",
    "  - Permiten cargar modelos con cabezales de predicción adecuados para cada tipo de tarea.\n",
    "\n",
    "### 2. **Tokenización**\n",
    "\n",
    "- **`AutoTokenizer`**\n",
    "  - Métodos:\n",
    "    - `from_pretrained('model-name')`: Carga un tokenizador que está alineado con un modelo preentrenado.\n",
    "    - `__call__(texts)`: Aplica la tokenización a uno o más textos.\n",
    "  - Uso: Para tokenizar textos de manera que sean compatibles con los modelos de Transformers.\n",
    "\n",
    "### 3. **Entrenamiento y Evaluación**\n",
    "\n",
    "- **`Trainer`**\n",
    "  - Métodos:\n",
    "    - `train()`: Comienza el entrenamiento del modelo.\n",
    "    - `evaluate()`: Evalúa el modelo en un conjunto de datos de prueba o validación.\n",
    "    - `predict(test_dataset)`: Realiza predicciones sobre un conjunto de datos no visto.\n",
    "  - Uso: Proporciona una manera simple y eficiente para entrenar y evaluar modelos de Transformers.\n",
    "\n",
    "- **`TrainingArguments`**\n",
    "  - Uso: Configura argumentos para el entrenamiento como la ubicación de los logs, tamaño de batch, número de epochs, etc.\n",
    "\n",
    "### 4. **Clases de Datasets**\n",
    "\n",
    "- **`Dataset` y `DatasetDict` de la biblioteca `datasets`**\n",
    "  - Métodos:\n",
    "    - `from_dict(data)`: Crea un dataset desde un diccionario de datos.\n",
    "    - `map(function)`: Aplica una función a todos los elementos del dataset.\n",
    "    - `train_test_split()`: Divide el dataset en subconjuntos de entrenamiento y prueba.\n",
    "  - Uso: Para manipular y preparar datos para el entrenamiento o evaluación.\n",
    "\n",
    "### 5. **Data Collators**\n",
    "\n",
    "- **`DataCollatorWithPadding`**\n",
    "  - Uso: Prepara batches de datos, asegurando que todos los inputs tengan la misma longitud mediante padding.\n",
    "  - Se pasa como argumento a `DataLoader` para manejar el batching de manera eficiente.\n",
    "\n",
    "### 6. **Utility Classes**\n",
    "\n",
    "- **`pipeline`**\n",
    "  - Uso: Facilita la ejecución de tareas comunes como clasificación, generación de texto, o análisis de sentimiento, encapsulando los procesos de tokenización, ejecución del modelo, y post-procesamiento.\n",
    "  - Ejemplo: `pipeline('sentiment-analysis', model='model-name')`\n",
    "\n",
    "Este esquema no cubre todas las funcionalidades y clases disponibles en Hugging Face Transformers, pero proporciona una visión general de las más comunes y útiles para la mayoría de las tareas de procesamiento de lenguaje natural y otras aplicaciones de inteligencia artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Clase/Método                            | Descripción                                                                                       | Métodos Principales y Uso                                                                                                  |\n",
    "|-----------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|\n",
    "| **AutoModel**                           | Clase genérica para cargar modelos preentrenados.                                                 | `from_pretrained('model-name')`: Carga un modelo. <br> `save_pretrained('directory')`: Guarda el modelo.                  |\n",
    "| **AutoModelFor...**                     | Clases especializadas para tareas específicas (clasificación, QA, etc.).                          | `AutoModelForSequenceClassification`,<br>`AutoModelForTokenClassification`,<br> `AutoModelForQuestionAnswering`,<br>`AutoModelForSeq2SeqLM`,<br>`AutoModelForTranslation`,<br>`AutoModelForMultiLabelClassification`,<br>`AutoModelForCausalLM`                 |\n",
    "| **AutoTokenizer**                       | Clase para cargar tokenizadores que corresponden a modelos preentrenados.                         | `from_pretrained('model-name')`: Carga un tokenizador. <br> `__call__(texts)`: Tokeniza texto.                            |\n",
    "| **Trainer**                             | Facilita el entrenamiento y evaluación de modelos de Transformers.                                | `train()`: Inicia el entrenamiento. <br> `evaluate()`: Evalúa el modelo. <br> `predict(test_dataset)`: Predice datos.     |\n",
    "| **TrainingArguments**                   | Configura parámetros para el entrenamiento con `Trainer`.                                         | Configura argumentos como tamaño del batch, epochs, directorio de salida, etc.                                            |\n",
    "| **Dataset & DatasetDict**               | Clases para manejar datasets, facilitando operaciones como map, split, etc.                       | `from_dict(data)`: Crea datasets. <br> `map(function)`: Aplica funciones. <br> `train_test_split()`: Divide datasets.    |\n",
    "| **DataCollatorWithPadding**             | Prepara batches asegurando que todos los inputs tengan la misma longitud mediante padding.        | Se utiliza como argumento en `DataLoader` para manejar la creación de batches.                                            |\n",
    "| **pipeline**                            | Facilita la ejecución de pipelines de procesamiento para tareas comunes como análisis de texto.   | `pipeline('task-name', model='model-name')`: Crea y ejecuta un pipeline para tareas como `sentiment-analysis`.            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la biblioteca Hugging Face Transformers, además de las clases que comienzan con \"Auto\", existen muchas otras clases diseñadas para cargar modelos específicos de manera directa. Estas clases están diseñadas para trabajar con una arquitectura de modelo particular y requieren que el usuario sepa exactamente con qué modelo está trabajando. Algunas de estas clases son:\n",
    "\n",
    "1. **Model Classes específicas**: Por ejemplo, `BertModel`, `GPT2Model`, `T5Model`, `RobertaModel`, entre otros. Cada una de estas clases está diseñada para cargar y trabajar con la arquitectura específica del modelo mencionado. Estas clases permiten un control más detallado sobre la configuración y la utilización del modelo.\n",
    "\n",
    "2. **Tokenizer Classes**: Como `BertTokenizer`, `GPT2Tokenizer`, etc., que son específicos para los modelos para los que están diseñados. Estos tokenizadores preparan los datos de entrada de manera que sean adecuados para su modelo correspondiente.\n",
    "\n",
    "3. **Pipeline Classes**: Clases como `TextClassificationPipeline`, `TranslationPipeline`, entre otros, que no solo cargan modelos y tokenizadores, sino que también encapsulan el flujo completo de procesamiento para tareas específicas.\n",
    "\n",
    "4. **Modelos para tareas específicas**: Clases como `BertForSequenceClassification`, `GPT2LMHeadModel`, etc., que son variantes de los modelos base adaptados para tareas particulares como clasificación de secuencias, generación de lenguaje, etc.\n",
    "\n",
    "La elección entre usar una clase específica del modelo o una clase \"Auto\" depende de tus necesidades:\n",
    "- **Uso de clases específicas**: Si sabes exactamente con qué modelo estás trabajando y necesitas un control fino sobre la configuración y el comportamiento del modelo, usar una clase específica puede ser beneficioso.\n",
    "- **Uso de clases \"Auto\"**: Si prefieres escribir código más genérico que pueda adaptarse fácilmente a diferentes modelos o si estás explorando diferentes modelos, usar una clase \"Auto\" puede ser más conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

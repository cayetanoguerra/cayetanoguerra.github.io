{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generative Pretrained Transformer (GPT)**\n",
    "\n",
    "Consituyen una serie de modelos del lenguaje desarrollados por OpenAI. Estos modelos son conocidos por su capacidad para generar texto, responder preguntas, traducir idiomas, resumir documentos y realizar una variedad de otras tareas lingüísticas. La arquitectura de GPT se basa en el concepto de *Transformer* usando solo la parte *decoder* de la arquitectura original.\n",
    "\n",
    "Estudiaremos el <a href=\"https://openai.com/research/gpt-2-1-5b-release\">modelo GPT-2, que es un modelo de lenguaje desarrollado por OpenAI en 2019</a>. Es la segunda versión de la serie de modelos GPT y representa un avance significativo respecto a su predecesor, GPT-1, tanto en términos de tamaño como de capacidad. GPT-2 fue diseñado para predecir la siguiente palabra en una secuencia de texto, basándose en todas las palabras anteriores dentro de esa secuencia, lo que le permite generar texto <a href=\"https://openai.com/research/better-language-models\">coherente y sorprendentemente humano a partir de un prompt dado</a>.\n",
    "\n",
    "Usaremos el código creado por Andrej Karpathy en su <a href=\"https://github.com/karpathy/nanoGPT\">repositorio de GitHub</a>. Este código es una versión simplificada de GPT-2 que se puede ejecutar en una sola GPU o, incluso, en una CPU y se ha adaptado para ser más fácil de entender y modificar.\n",
    "\n",
    "Tenemos también un <a href=\"https://www.youtube.com/embed/kCc8FmEb1nY\">tutorial en vídeo</a> en el que se explica cómo funciona el modelo GPT-2 y cómo se ha implementado en el código de Karpathy.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./imgs/gpt_model.png\" width=\"400px\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d24e20dfb574918a28be6748580c5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f939e4d6f17245bb90ca289d9b6d5bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q5_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: 12\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"¿Si tengo 15 manzanas y me como 3, cuántas me quedan?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/LMStudio.png\" width=\"300\">\n",
    "\n",
    "Descarga e instala LM Studio desde [aquí](https://lmstudio.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Para mejorar tu concentración en el trabajo, considera las siguientes recomendaciones:\n",
      "\n",
      "1. Elimina distracciones: Identifica los factores que te desvían de tu trabajo y eliminalos. Esto puede incluir notificaciones de teléfono, correo electrónico, o sitios web no relacionados con tu trabajo.\n",
      "\n",
      "2. Establece un entorno favorable: Crear un ambiente tranquilo y distraído que te permita concentrarte mejor. Puede incluir una buena iluminación, una temperatura cómoda, y un ruido blanco o música suave en el fondo.\n",
      "\n",
      "3. Toma cuidados de tu salud: Asegúrate de mantenerte hígado y bien alimentado para que tus niveles de energía y concentración se mantengan altos. También considera la posibilidad de hacer pequeñas pausas para realizar ejercicios físicos o practicar relajación.\n",
      "\n",
      "4. Utiliza herramientas y técnicas: Hay varias técnicas que pueden ayudarte a mejorar tu concentración, como el método del cuadro de 2x2 (elimina cuatro distracciones y se centra en una tarea), la tecnología del pomodoro (trabaja en intervalos de 25 minutos seguidos por un descanso de cinco minutos), o el uso de herramientas como calendarios, listas de tareas, o aplicaciones de bloqueo de sitios web para evitar distracciones.\n",
      "\n",
      "5. Desarrolla habilidades mentales: Practicar meditación, yoga mental, memorización de hechos, o ejercicios de lógica pueden ayudarte a mejorar tu capacidad de concentrarse y retener información.\n",
      "\n",
      "6. Planifica tu trabajo: Organiza tus tareas en prioridades y dividirlas en pequeños pasos. Esto puede ayudarte a sentirte menos sobrecargado y más capaz de concentrarse en cada tarea individual.\n",
      "\n",
      "7. Mantente húmedo: La falta de agua puede deshidratar el cerebro y reducir tu capacidad de concentración, por lo que asegúrate de beber suficiente agua durante el día.\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"mistral-7b-instruct-v0.2.Q4_K_S.gguf\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Eres un aistente amable y educado. Responde siempre en español a lo que se te pregunte.\"},\n",
    "    {\"role\": \"user\", \"content\": \"¿Cómo puedo mejorar mi concentración en el trabajo? Responde en español\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3955bc73b44a2a95178d6b1bffd82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Autenticación en un entorno de notebook\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "print(encodeds)\n",
    "\n",
    "device = \"mps\" # the device to load the model onto\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=10, do_sample=False)\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

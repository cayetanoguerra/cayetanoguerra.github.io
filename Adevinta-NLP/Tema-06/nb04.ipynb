{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Parameter-Efficient Fine-Tuning (PEFT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Parameter-Efficient Fine-Tuning (PEFT) se refiere a técnicas de ajuste fino que buscan mejorar o adaptar modelos de aprendizaje profundo preentrenados, especialmente modelos de lenguaje de gran escala, a tareas específicas sin necesidad de reentrenar completamente todos los parámetros del modelo. Este enfoque es particularmente valioso dada la creciente escala de los modelos de lenguaje natural (como GPT, BERT, y similares), cuyo entrenamiento completo requiere una cantidad significativa de recursos computacionales y energéticos.\n",
    "\n",
    "PEFT aborda el desafío de cómo hacer que el ajuste fino de estos modelos sea más accesible y práctico, permitiendo personalizar los modelos a tareas específicas con menos recursos. Algunas de las técnicas dentro de este enfoque incluyen:\n",
    "\n",
    "### 1. **Low-Rank Adaptation**\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/2106.09685.pdf\">LoRA: Low-Rank Adaptation of Large Language Models</a>\n",
    "\n",
    "Esta técnica se basa en la adición de matrices de bajo rango que modifican las representaciones intermedias del modelo. Al ajustar estas matrices de bajo rango, se puede influir en el comportamiento del modelo con un costo computacional relativamente bajo.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./imgs/LoRA.png\" width=\"350px\">\n",
    "</div>\n",
    "\n",
    "\n",
    "#### **QLoRA: Quantized Low-Rank Adaptation for Efficient Fine-Tuning of Pretrained Transformers**\n",
    "\n",
    "### 2. **Adapters**\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1902.00751.pdf\">Parameter-Efficient Transfer Learning for NLP</a>\n",
    "\n",
    "Son módulos pequeños que se insertan entre las capas preexistentes del modelo. Estos adaptadores son entrenados mientras que los parámetros originales del modelo permanecen congelados. Esto permite que el modelo se adapte a nuevas tareas con un número relativamente pequeño de parámetros adicionales siendo ajustados.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./imgs/adapters.webp\" width=\"650px\">\n",
    "</div>\n",
    "\n",
    "### 3. **Prompt Tuning**\n",
    "Consiste en diseñar y optimizar un conjunto de \"prompts\" (indicaciones) que se añaden a la entrada del modelo para guiar su generación de texto o predicciones hacia la tarea deseada. Estos prompts pueden ser fijos o aprendidos durante un proceso de ajuste fino limitado.\n",
    "\n",
    "### 4. **Layer Tuning**\n",
    "En esta técnica, solo un subconjunto de las capas del modelo (por ejemplo, las últimas capas) se ajustan para una tarea específica, mientras que el resto de las capas permanecen congeladas. Esto reduce el número de parámetros que necesitan ser ajustados.\n",
    "\n",
    "### 5. **BitFit**\n",
    "Se refiere a ajustar únicamente los parámetros de sesgo (bias) del modelo mientras se mantienen fijos los pesos de las conexiones. A pesar de su simplicidad, esta técnica ha demostrado ser sorprendentemente efectiva para el ajuste fino de modelos de lenguaje.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Parameter-Efficient Fine-Tuning (PEFT)**\n",
    "\n",
    "https://huggingface.co/docs/peft/index  <-- Empezar por aquí\n",
    "\n",
    "https://huggingface.co/docs/peft/en/conceptual_guides/prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Parameter-Efficient Fine-Tuning (PEFT) se refiere a técnicas de ajuste fino que buscan mejorar o adaptar modelos de aprendizaje profundo preentrenados, especialmente modelos de lenguaje de gran escala, a tareas específicas sin necesidad de reentrenar completamente todos los parámetros del modelo. Este enfoque es particularmente valioso dada la creciente escala de los modelos de lenguaje natural (como GPT, BERT, y similares), cuyo entrenamiento completo requiere una cantidad significativa de recursos computacionales y energéticos.\n",
    "\n",
    "PEFT aborda el desafío de cómo hacer que el ajuste fino de estos modelos sea más accesible y práctico, permitiendo personalizar los modelos a tareas específicas con menos recursos. Algunas de las técnicas dentro de este enfoque incluyen:\n",
    "\n",
    "### 1. **Low-Rank Adaptation**\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/2106.09685.pdf\">LoRA: Low-Rank Adaptation of Large Language Models</a>\n",
    "\n",
    "Esta técnica se basa en la adición de matrices de bajo rango que modifican las representaciones intermedias del modelo. Al ajustar estas matrices de bajo rango, se puede influir en el comportamiento del modelo con un costo computacional relativamente bajo.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./imgs/LoRA.png\" width=\"350px\">\n",
    "</div>\n",
    "\n",
    "Estas matrices de bajo rango se insertan en los parámetros de un modelo preentrenado, específicamente en los puntos de interacción clave como las matrices de transformación en capas de atención y capas feed-forward. \n",
    "\n",
    "Matemáticamente, esto se logra ya que la matriz de adaptación es el producto de dos matrices más pequeñas, facilitando así una modificación eficiente del comportamiento del modelo con una cantidad mínima de parámetros adicionales.\n",
    "\n",
    "Consideremos una capa específica en un modelo, donde la operación lineal original es $ W $, una matriz que transforma la entrada $ x $ en la salida $ y $, es decir, $ y = Wx $.\n",
    "\n",
    "1. **Introducción del Adaptador de Bajo Rango:**\n",
    "   En LoRA, en lugar de modificar $ W $ directamente, se introduce una adaptación $ \\Delta W $ que es de bajo rango. Matemáticamente, $ \\Delta W $ se expresa como el producto de dos matrices de menor dimensión $ A $ y $ B $, es decir,\n",
    "   $$\n",
    "   \\Delta W = AB\n",
    "   $$\n",
    "   donde $ A $ es una matriz de $ d \\times r $ y $ B $ es una matriz de $ r \\times d $. Aquí, $ r $ es el rango de la adaptación y es mucho menor que $ d $, la dimensión de la entrada y salida.\n",
    "\n",
    "2. **Modificación de la Transformación Lineal:**\n",
    "   La nueva transformación lineal del modelo con la adaptación de bajo rango se convierte en:\n",
    "   $$\n",
    "   y = (W + \\Delta W)x = (W + AB)x\n",
    "   $$\n",
    "   Aquí, $ W $ es la matriz original y $ AB $ es la adaptación de bajo rango que ajusta la transformación de $ W $ para adaptarse mejor a una tarea específica.\n",
    "\n",
    "3. **Cálculo de la Salida Modificada:**\n",
    "   Al descomponer $ \\Delta W $ en el producto de $ A $ y $ B $, reducimos el número de parámetros a entrenar desde $ d^2 $ (si se modificara $ W $ completamente) a $ d \\times r + r \\times d $, lo cual es significativamente menor si $ r $ es pequeño.\n",
    "\n",
    "##### **Ejemplo en un Transformer**\n",
    "\n",
    "Consideremos la capa de atención donde $ Q, K, V $ (consultas, claves y valores) son transformados por las matrices $ W^Q, W^K, W^V $. Con LoRA, se introducirían adaptaciones $ \\Delta W^Q, \\Delta W^K, \\Delta W^V $ tal que:\n",
    "   $$\n",
    "   Q = (W^Q + \\Delta W^Q)X, \\quad K = (W^K + \\Delta W^K)X, \\quad V = (W^V + \\Delta W^V)X\n",
    "   $$\n",
    "donde $ X $ es la entrada a la capa de atención y cada $ \\Delta W $ es una adaptación de bajo rango específica para $ Q, K, $ o $ V $.\n",
    "\n",
    "Este enfoque permite una flexibilidad considerable en la adaptación del modelo a nuevas tareas, permitiendo ajustes finos donde es más necesario con un mínimo impacto en el tamaño y la complejidad del modelo.\n",
    "\n",
    "\n",
    "#### **QLoRA: Quantized Low-Rank Adaptation for Efficient Fine-Tuning of Pretrained Transformers**\n",
    "\n",
    "QLoRA y LoRA son técnicas de adaptación para modelos de lenguaje de gran escala, pero QLoRA presenta cuantización, lo que significa que utiliza versiones de menor precisión de los números para reducir los requisitos de memoria y computación. Esto es especialmente útil para despliegues en dispositivos con recursos limitados.\n",
    "\n",
    "### 2. **Adapters**\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1902.00751.pdf\">Parameter-Efficient Transfer Learning for NLP</a>\n",
    "\n",
    "Un \"adapter\" es esencialmente un módulo pequeño y entrenable que se inserta entre las capas preexistentes de un modelo de lenguaje grande. Estos módulos tienen una estructura específica, generalmente consistiendo en una capa de reducción de dimensionalidad, una transformación no lineal, y una capa de expansión de dimensionalidad. La idea es que estos adapters aprendan ajustes específicos para la tarea en cuestión, mientras que los parámetros originales del modelo (los de las grandes capas preentrenadas) permanecen congelados.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./imgs/adapters.webp\" width=\"650px\">\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./imgs/adapters.png\" width=\"250px\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### 3. **Prompt Tuning**\n",
    "Consiste en diseñar y optimizar un conjunto de *prompts* que se añaden a la entrada del modelo para guiar su generación de texto o predicciones hacia la tarea deseada. Estos prompts pueden ser fijos o aprendidos durante un proceso de ajuste fino limitado.\n",
    "\n",
    "### 4. **Layer Tuning**\n",
    "En esta técnica, solo un subconjunto de las capas del modelo (por ejemplo, las últimas capas) se ajustan para una tarea específica, mientras que el resto de las capas permanecen congeladas. Esto reduce el número de parámetros que necesitan ser ajustados.\n",
    "\n",
    "### 5. **BitFit**\n",
    "Se refiere a ajustar únicamente los parámetros de sesgo (bias) del modelo mientras se mantienen fijos los pesos de las conexiones. A pesar de su simplicidad, esta técnica ha demostrado ser sorprendentemente efectiva para el ajuste fino de modelos de lenguaje.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT\n",
    "Accelerate\n",
    "bitsandbytes\n",
    "tlr\n",
    "evaluate\n",
    "tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ROUGE**\n",
    "https://huggingface.co/spaces/evaluate-metric/rouge\n",
    "\n",
    "La métrica **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) es una familia de métricas utilizada para evaluar la calidad de los resúmenes generados automáticamente comparándolos con uno o más resúmenes de referencia hechos por humanos. Fue desarrollada principalmente para tareas de evaluación en sistemas de resumen automático de textos, pero también se usa en otras aplicaciones de procesamiento de lenguaje natural donde se requiere la comparación de secuencias de texto, como en la evaluación de respuestas de modelos de lenguaje.\n",
    "\n",
    "#### **Variantes Principales de ROUGE**\n",
    "\n",
    "**1. ROUGE-N:**\n",
    "   - Mide la coincidencia de n-gramas entre el texto generado y los textos de referencia. \"N\" se refiere al tamaño de los n-gramas (por ejemplo, ROUGE-1 para unigramas, ROUGE-2 para bigramas, etc.). Esta métrica se centra en la precisión y el recall de n-gramas específicos.\n",
    "\n",
    "**2. ROUGE-L:**\n",
    "   - Basada en la longitud de la secuencia más larga común (LCS, Longest Common Subsequence) entre el texto generado y la referencia. ROUGE-L es sensible al orden de las palabras y es útil para evaluar la coherencia de largos segmentos de texto en el resumen.\n",
    "\n",
    "**3. ROUGE-S:**\n",
    "   - Considera la co-ocurrencia de bigramas en las secuencias pero permite saltos entre las palabras (skip-bigram). Es menos restrictiva en términos del orden exacto de las palabras, siendo útil para evaluar la relevancia de los contenidos sin penalizar tanto la reorganización de palabras.\n",
    "\n",
    "**4. ROUGE-W:**\n",
    "   - Es una variación de ROUGE-L que incorpora un factor de ponderación para dar más importancia a las secuencias más largas, ayudando a enfocar la evaluación en la coherencia global del texto en lugar de solo fragmentos coincidentes.\n",
    "\n",
    "#### **Cómo Funciona ROUGE**\n",
    "\n",
    "- **Recall en ROUGE** se refiere a la proporción de n-gramas en los resúmenes de referencia que también aparecen en el resumen generado. Un recall alto indica que el resumen generado cubre mucho de lo que está en los resúmenes de referencia.\n",
    "- **Precision en ROUGE** mide cuántos n-gramas del resumen generado están presentes en los resúmenes de referencia, lo cual indica la relevancia de la información generada respecto al contenido de referencia.\n",
    "- A menudo, se calcula el **F1-score**, que es el promedio armónico de la precisión y el recall, proporcionando un balance entre ambos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/evaluate/a_quick_tour\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 1.0\n",
      "rouge2: 0.91\n",
      "rougeL: 0.58\n",
      "rougeLsum: 0.58\n"
     ]
    }
   ],
   "source": [
    "# Compute the score\n",
    "prediction = [\"Estaba jugando con mi perro cuando sonó el timbre de la puerta\"]\n",
    "reference = [\"Cuando sonó el timbre de la puerta, estaba jugando con mi perro\"]\n",
    "score = metric.compute(predictions=prediction, references=reference)\n",
    "\n",
    "for key, value in score.items():\n",
    "    print(f\"{key}: {round(value, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GPT**\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cayetano/Propio/Notebooks/Machine Learning/RL/env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, block_size=128):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "        self.examples = []\n",
    "        for line in lines:\n",
    "            tokens = tokenizer.encode(line, add_special_tokens=True)\n",
    "            if len(tokens) > block_size:\n",
    "                tokens = tokens[:block_size]\n",
    "            tokens += [tokenizer.eos_token_id]\n",
    "            tokens += [tokenizer.pad_token_id] * (block_size - len(tokens))\n",
    "            self.examples.append(torch.tensor(tokens))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        res = {\"input_ids\": self.examples[i][:-1], \"labels\": self.examples[i][1:], \"attention_mask\": (self.examples[i] != 0).float()}\n",
    "        # res = {\"input_ids\": self.examples[i]}\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones\n",
    "file_path = 'data/numbers_gpt.csv'  # Ajusta esto al path de tu archivo de texto\n",
    "block_size = 128  # Longitud máxima de las secuencias de tokens\n",
    "\n",
    "# Cargar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Preparar el dataset\n",
    "dataset = TextDataset(tokenizer, file_path, block_size)\n",
    "\n",
    "# Crear DataLoader para iterar sobre el dataset durante el entrenamiento\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 124439808 parámetros entrenables\n",
      "El modelo tiene 124439808 parámetros\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "# Número de parámetros entrenables del modelo\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"El modelo tiene {num_params} parámetros entrenables\")\n",
    "\n",
    "# Número de parámetros del modelo\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"El modelo tiene {num_params} parámetros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar LoRA\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 1179648 parámetros entrenables\n",
      "El modelo tiene 125619456 parámetros\n"
     ]
    }
   ],
   "source": [
    "# Número de parámetros entrenables del modelo\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"El modelo tiene {num_params} parámetros entrenables\")\n",
    "\n",
    "# Número de parámetros del modelo\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"El modelo tiene {num_params} parámetros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_save_function(output_dir):\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"modelo_entrenado\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "# Crear el objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    # eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save_pretrained(\"modelo_entrenado\")\n",
    "tokenizer.save_pretrained(\"modelo_entrenado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número 123456789 en formato textual es: 123456789 one one one one one one one one one one one one one one\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Cargar el modelo y el tokenizador entrenados\n",
    "model_path = \"modelo_entrenado\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Generar número en formato textual\n",
    "def generate_textual_number(numeric_number):\n",
    "    input_text = str(numeric_number)\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"mps\")\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "    textual_number = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return textual_number\n",
    "\n",
    "# Ejemplo de uso\n",
    "numeric_number = 123456789\n",
    "textual_number = generate_textual_number(numeric_number)\n",
    "print(f\"El número {numeric_number} en formato textual es: {textual_number}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Estrategias de clasificación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aprendizaje supervisado en clasificación de texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clasificador Naive Bayes**\n",
    "\n",
    "El algoritmo **Naive Bayes** es un clasificador probabilístico basado en el **teorema de Bayes** con supuestos de independencia entre las características. Es especialmente adecuado para la clasificación de texto. Antes, recordemos algunos conceptos básicos:\n",
    "\n",
    "**Teorema de Bayes:**\n",
    "El fundamento detrás del algoritmo Naive Bayes es el teorema de Bayes, que se formula como:\n",
    "\n",
    "$$ P(C|D) = \\frac{P(D|C) \\cdot P(C)}{P(D)} $$\n",
    "\n",
    "Donde:\n",
    "- $P(C|D)$ es la probabilidad posterior de la clase C dado un dato D.\n",
    "- $P(D|C)$ es la probabilidad de observar el dato D dado que pertenece a la clase C.\n",
    "- $P(C)$ es la probabilidad a priori de la clase C.\n",
    "- $P(D)$ es la probabilidad total de observar el dato D.\n",
    "\n",
    "\n",
    "**Independencia:**\n",
    "Dentro del Teorema de Bayes, el concepto de **independencia** se refiere a la suposición de que ciertas variables o eventos no afectan la probabilidad de otros eventos o variables.\n",
    "\n",
    "Cuando hablamos del clasificador Naive Bayes, nos referimos específicamente a la **independencia condicional** de las características dado un resultado o clase particular. Esta es la suposición \"ingenua\" (naive) que le da nombre al método.\n",
    "\n",
    "En términos matemáticos, la independencia condicional en Naive Bayes se expresa así:\n",
    "\n",
    "$ P(X_1, X_2, \\ldots , X_n | Y) = P(X_1 | Y) \\cdot P(X_2 | Y) \\ldots P(X_n | Y) $\n",
    "\n",
    "Donde:\n",
    "- $ X_1, X_2, ..., X_n $ son las características (por ejemplo, en clasificación de texto, estas podrían ser palabras o frases).\n",
    "- $ Y $ es una clase particular (por ejemplo, una etiqueta como \"spam\" o \"no spam\").\n",
    "\n",
    "Lo que esto significa es que, dado un valor particular de $ Y $, la probabilidad conjunta de todas las características es simplemente el producto de sus probabilidades individuales. En otras palabras, estamos asumiendo que la presencia (o ausencia) de una característica (palabra) no afecta la presencia (o ausencia) de cualquier otra característica (palabra), siempre que conozcamos la clase $ Y $.\n",
    "\n",
    "Este supuesto simplifica enormemente los cálculos y, aunque rara vez es cierto en la práctica (especialmente en el procesamiento del lenguaje natural donde las palabras están frecuentemente relacionadas entre sí), el clasificador Naive Bayes puede ser sorprendentemente eficaz en muchas situaciones a pesar de su suposición de independencia.\n",
    "\n",
    "\n",
    "\n",
    "#### **Ejemplo**\n",
    "\n",
    "Veamos un ejemplo. Supongamos que queremos clasificar frases entre las categorías \"Cine\" y \"Literatura\". Las frases de entrenamiento son:\n",
    "\n",
    "**Frases de entrenamiento:**\n",
    "1. \"La película fue emocionante y llena de acción.\" - Cine\n",
    "2. \"Ese libro tiene una trama intrigante.\" - Literatura\n",
    "3. \"Los actores hicieron un trabajo excelente.\" - Cine\n",
    "4. \"El autor describe paisajes con gran detalle.\" - Literatura\n",
    "5. \"El cine de autor siempre me ha fascinado.\" - Cine\n",
    "6. \"La novela estaba llena de giros inesperados.\" - Literatura\n",
    "7. \"El guion de esa película fue escrito por un famoso novelista.\" - Cine\n",
    "8. \"Los personajes del libro eran muy realistas.\" - Literatura\n",
    "9. \"Esa película está basada en un libro aclamado.\" - Cine\n",
    "\n",
    "Algunas palabras, como \"libro\", \"película\", y \"autor\", aparecen en ambas categorías.\n",
    "\n",
    "La probabilidad a priori de cada categoría es:\n",
    "\n",
    "$$ P(Cine) = \\frac{5}{9} $$\n",
    "$$ P(Literatura) = \\frac{4}{9} $$\n",
    "\n",
    "Esto viene a significar que una frase a clasificar tiene, a priori, una probabilidad de $ \\frac{5}{9} $ de ser de la categoría \"Cine\" y una probabilidad de $ \\frac{4}{9} $ de ser de la categoría \"Literatura\".\n",
    "\n",
    "Ahora, calculamos las probabilidades condicionales para cada palabra en cada categoría (eliminamos previamente las stop-words). Fíjate que hay palabras que aparecen en ambas categorías. Por ejemplo, la palabra \"libro\" aparece en dos frases de \"Literatura\" y en una frase de \"Cine\". La palabra \"autor\" aparece en una frase de \"Literatura\" y en otra frase de \"Cine\". Y así sucesivamente. Por tanto, tenemos que:\n",
    "\n",
    "$$ P(libro|Literatura) = \\frac{2}{18} $$\n",
    "$$ P(libro|Cine) = \\frac{1}{24} $$\n",
    "$$ P(autor|Cine) = \\frac{1}{24} $$\n",
    "$$ P(autor|Literatura) = \\frac{1}{18} $$\n",
    "\n",
    "...y así sucesivamente para las demás palabras.\n",
    "\n",
    "Nos podemos dar cuenta de que puede haber palabras que no aparezcan en una categoría. Por ejemplo, la palabra \"paisajes\" no aparece en ninguna frase de \"Cine\". En este caso, la probabilidad condicional es cero:\n",
    "\n",
    "$$ P(paisajes|Cine) = 0 $$\n",
    "\n",
    "Esto es un problema, porque si multiplicamos muchas probabilidades condicionales, el resultado será cero. Para evitar esto, podemos usar un **suavizado** (smoothing) para evitar que las probabilidades condicionales sean cero. Por ejemplo, podemos usar el suavizado de Laplace, que consiste en sumar un valor $\\alpha$ (normalmente, 1) al numerador y el número de palabras únicas (vocabulario) en el denominador.\n",
    "\n",
    "$$ P(w_i|C_k) = \\frac{\\text{Número de veces que } w_i \\text{ aparece en } C_k + \\alpha}{\\text{Total de palabras en } C_k + \\alpha \\times \\text{Tamaño del vocabulario}} $$\n",
    "\n",
    "...y así sucesivamente para las demás palabras.\n",
    "\n",
    "El motivo de sumar el tamaño del vocabulario en el denominador es para que la suma de todas las probabilidades condicionales sea 1.\n",
    "\n",
    "Vayamos haciendo un script en Python para calcular las probabilidades condicionales de cada palabra en cada categoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 palabras en el vocabulario\n",
      "Número de palabras en cine: 24\n",
      "Número de palabras en literatura: 18\n",
      "Probabilidades a priori:\n",
      "Cine 0.556\n",
      "Literatura 0.444\n",
      "\n",
      "  idx  Palabra        Pr. Cine    Pr. Literatura\n",
      "-----  -----------  ----------  ----------------\n",
      "    0  siempre          0.0407            0.0048\n",
      "    1  escrito          0.0407            0.0048\n",
      "    2  aclamado         0.0407            0.0048\n",
      "    3  libro            0.0407            0.1\n",
      "    4  película         0.1148            0.0048\n",
      "    5  paisajes         0.0037            0.0524\n",
      "    6  trama            0.0037            0.0524\n",
      "    7  inesperados      0.0037            0.0524\n",
      "    8  actores          0.0407            0.0048\n",
      "    9  hicieron         0.0407            0.0048\n",
      "   10  novela           0.0037            0.0524\n",
      "   11  novelista        0.0407            0.0048\n",
      "   12  acción           0.0407            0.0048\n",
      "   13  realistas        0.0037            0.0524\n",
      "   14  gran             0.0037            0.0524\n",
      "   15  fascinado        0.0407            0.0048\n",
      "   16  excelente        0.0407            0.0048\n",
      "   17  famoso           0.0407            0.0048\n",
      "   18  describe         0.0037            0.0524\n",
      "   19  basada           0.0407            0.0048\n",
      "   20  detalle          0.0037            0.0524\n",
      "   21  autor            0.0407            0.0524\n",
      "   22  personajes       0.0037            0.0524\n",
      "   23  intrigante       0.0037            0.0524\n",
      "   24  emocionante      0.0407            0.0048\n",
      "   25  giros            0.0037            0.0524\n",
      "   26  llena            0.0407            0.0524\n",
      "   27  guion            0.0407            0.0048\n",
      "   28  cine             0.0407            0.0048\n",
      "   29  trabajo          0.0407            0.0048\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "documents = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "label_names = {0: \"Cine\", 1: \"Literatura\"}\n",
    "\n",
    "# Preprocesamiento\n",
    "def preprocess(docs):\n",
    "    txts = [doc.lower().replace(\".\", \"\") for doc in docs]\n",
    "    txts = [doc.split() for doc in txts]\n",
    "    # Eliminación de stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"spanish\")\n",
    "    txts = [[word for word in doc if word not in stopwords] for doc in txts]\n",
    "    return txts\n",
    "\n",
    "documents = preprocess(documents)\n",
    "\n",
    "# Clasificador Naive Bayes\n",
    "\n",
    "# Creación del vocabulario\n",
    "def get_vocab(docs):\n",
    "    vocab = set()\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            vocab.add(word)\n",
    "    vocab = list(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(documents)\n",
    "print(len(vocab), \"palabras en el vocabulario\")\n",
    "\n",
    "# Creación de la matriz de características\n",
    "import numpy as np\n",
    "X = np.zeros((len(documents), len(vocab)))\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    for word in doc:\n",
    "        j = vocab.index(word)\n",
    "        X[i][j] += 1\n",
    "\n",
    "# Probabilidades a priori\n",
    "def get_prior(labels):\n",
    "    prior = np.zeros(2)\n",
    "    for label in labels:\n",
    "        prior[label] += 1\n",
    "    prior = prior / len(labels)\n",
    "    return prior\n",
    "\n",
    "prior = get_prior(labels)\n",
    "\n",
    "# Probabilidades condicionales\n",
    "def get_conditional(X, labels, alpha=1):\n",
    "    conditional = np.ones((2, X.shape[1])) * alpha\n",
    "    for i, label in enumerate(labels):\n",
    "        conditional[label] += X[i]\n",
    "\n",
    "    print(\"Número de palabras en cine:\", int(conditional[0].sum()))\n",
    "    print(\"Número de palabras en literatura:\", int(conditional[1].sum()))\n",
    "\n",
    "    conditional = conditional / (conditional.sum(axis=1).reshape(-1, 1) + alpha * len(vocab))\n",
    "    return conditional\n",
    "\n",
    "conditional = get_conditional(X, labels, 0.1)\n",
    "\n",
    "print(\"Probabilidades a priori:\")\n",
    "print(label_names[0], round(prior[0], 3))\n",
    "print(label_names[1], round(prior[1], 3))\n",
    "print()\n",
    "\n",
    "from tabulate import tabulate\n",
    "table = []\n",
    "for i, w in enumerate(vocab):\n",
    "    table.append([i, w, round(conditional[0][i],4), round(conditional[1][i],4)])\n",
    "print(tabulate(table, headers=[\"idx\", \"Palabra\", \"Pr. Cine\", \"Pr. Literatura\"]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autor 0.041\n",
      "libro 0.041\n",
      "------ Probabilidad de ser de la clase Cine: 0.0009221 \n",
      "\n",
      "autor 0.052\n",
      "libro 0.1\n",
      "------ Probabilidad de ser de la clase Literatura: 0.002328 \n",
      "\n",
      "\n",
      "Predicción: Literatura\n"
     ]
    }
   ],
   "source": [
    "# Clasificación\n",
    "def predict(doc, vocab, prior, conditional):\n",
    "    x = np.zeros(len(vocab))\n",
    "    for word in doc:\n",
    "        j = vocab.index(word)\n",
    "        x[j] += 1\n",
    "    p = prior.copy()\n",
    "    for i in range(len(p)):\n",
    "        for j in range(len(x)):\n",
    "            if vocab[j] in doc:\n",
    "                print(vocab[j],  round(conditional[i][j],3))\n",
    "            p[i] *= conditional[i][j] ** x[j]\n",
    "        print(\"------ Probabilidad de ser de la clase \" + label_names[i] + \":\", round(p[i],7), \"\\n\")\n",
    "    return np.argmax(p)\n",
    "\n",
    "docs = [\"el libro es de un autor.\"]\n",
    "# docs = [\"el libro es de un autor de trama.\"]\n",
    "# docs = [\"el libro es de un autor aclamado.\"]\n",
    "\n",
    "docs = preprocess(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"\\nPredicción: \" + label_names[predict(doc, vocab, prior, conditional)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate en las probabilidades que nos resultan para cada clase, son muy pequeñas. Si el tamaño de nuestro vocabulario fuera mucho mayor (lo que sería muy normal) las probabilidades serían aún mucho más pequeñas y podríamos tener problemas de precisión numérica para calcularlas. Es una práctica común y recomendada transformar las probabilidades con logaritmos cuando se trabaja con Naive Bayes, precisamente para evitar problemas de precisión numérica. Los productos de probabilidades pequeñas pueden acercarse a cero en la aritmética de punto flotante, lo que puede dar lugar a errores o inestabilidades.\n",
    "\n",
    "Dada la propiedad del logaritmo:\n",
    "$$ \\log(a \\cdot b) = \\log(a) + \\log(b) $$\n",
    "\n",
    "Puedes transformar las multiplicaciones de probabilidades en sumas de logaritmos. \n",
    "\n",
    "Si estás calculando:\n",
    "$$ P(C_k|\\text{documento}) =  \\frac{P(C_k) \\cdot \\prod_{i} P(w_i|C_k)}{ P(documento)} $$\n",
    "\n",
    "Puedes tomar el logaritmo en ambos lados:\n",
    "$$ \\log(P(C_k|\\text{documento})) =  \\log(P(C_k)) + \\sum_{i} \\log(P(w_i|C_k)) - \\log(P(documento)) $$\n",
    "\n",
    "Al clasificar un documento, calculas el valor anterior para cada clase y eliges la clase con el valor más alto. No es necesario convertir estos valores de nuevo usando la función exponencial porque el logaritmo es una función monótona creciente. Por lo tanto, si $ \\log(a) > \\log(b) $, entonces $ a > b $.\n",
    "\n",
    "Al trabajar con sumas en lugar de productos, evitas los problemas de precisión numérica y, además, el cálculo se vuelve computacionalmente más eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "autor -3.201\n",
      "libro -3.201\n",
      "------ Probabilidad de ser de la clase Cine: -6.988840037302129 \n",
      "\n",
      "autor -2.949\n",
      "libro -2.303\n",
      "------ Probabilidad de ser de la clase Literatura: -6.062727567129474 \n",
      "\n",
      "\n",
      "Predicción: Literatura\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def predict(doc, vocab, prior, conditional):\n",
    "    x = np.zeros(len(vocab))\n",
    "    for word in doc:\n",
    "        j = vocab.index(word)\n",
    "        x[j] += 1\n",
    "    # Tomar el logaritmo de los priors\n",
    "    p = [math.log(prior_val) for prior_val in prior]\n",
    "    for i in range(len(p)):\n",
    "        for j in range(len(x)):\n",
    "            if vocab[j] in doc:\n",
    "                print(vocab[j], round(math.log(conditional[i][j]), 3))\n",
    "            # Sumar el logaritmo de las probabilidades\n",
    "            p[i] += x[j] * math.log(conditional[i][j])\n",
    "        print(\"------ Probabilidad de ser de la clase \" + label_names[i] + \":\", p[i], \"\\n\")\n",
    "    return np.argmax(p)\n",
    "\n",
    "docs = [\"el libro es de un autor.\"]\n",
    "\n",
    "docs = preprocess(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nPredicción: \" + label_names[predict(doc, vocab, prior, conditional)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 1\n",
    "\n",
    "Modifica el código anterior para poder hacer la clasificación entre tres categorías: \"Cine\", \"Literatura\" y \"Música\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\",\n",
    "    \"El nuevo álbum de la banda es increíble.\",\n",
    "    \"El concierto en el estadio estuvo lleno.\",\n",
    "    \"La guitarra eléctrica tiene un sonido potente y claro.\",\n",
    "    \"Los festivales de música al aire libre son mis favoritos.\",\n",
    "    \"El pianista interpretó una pieza clásica maravillosamente.\",\n",
    "    \"La lista de reproducción incluye varios géneros, desde jazz hasta rock.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2]\n",
    "\n",
    "label_names = {0: \"Cine\", 1: \"Literatura\", 2: \"Música\"}\n",
    "\n",
    "documents_test = [\n",
    "    \"El libro estaba basado en emocionantes paisajes.\",\n",
    "    \"El guion fue aclamado por su trama intrigante.\",\n",
    "    \"La banda tocó jazz y rock en el concierto.\",\n",
    "    \"El cine muestra películas emocionantes de acción.\",\n",
    "    \"El pianista tocó una pieza de música clásica.\"\n",
    "]\n",
    "\n",
    "labels_test = [1, 0, 2, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 2\n",
    "\n",
    "¿Qué ocurre cuando tenemos palabras en un conjunto de test que no están en el vocabulario del conjunto de entrenamiento? ¿Cómo podríamos solucionarlo?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NLTK**\n",
    "https://www.nltk.org/index.html\n",
    "\n",
    "Vamos a utilizar la librería NLTK para llevar a cabo la clasificación de texto mediante su propia implementación Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Datos de entrenamiento\n",
    "texts = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\",\n",
    "    \"El nuevo álbum de la banda es increíble.\",\n",
    "    \"El concierto en el estadio estuvo lleno.\",\n",
    "    \"La guitarra eléctrica tiene un sonido potente y claro.\",\n",
    "    \"Los festivales de música al aire libre son mis favoritos.\",\n",
    "    \"El pianista interpretó una pieza clásica maravillosamente.\",\n",
    "    \"La lista de reproducción incluye varios géneros, desde jazz hasta rock.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2]\n",
    "\n",
    "label_names = {0: \"Cine\", 1: \"Literatura\", 2: \"Música\"}\n",
    "\n",
    "# Convertir texto a formato NLTK\n",
    "documents = [(word_tokenize(text), label) for text, label in zip(texts, labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 15), ('de', 7), ('el', 7), ('la', 5), ('un', 4), ('película', 3), ('libro', 3), ('los', 3), ('fue', 2), ('y', 2)]\n",
      "['.', 'de', 'el', 'la', 'un', 'película', 'libro', 'los', 'fue', 'y']\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in word_tokenize(' '.join(texts)))\n",
    "word_features = list(all_words)[:2000]  # Usamos las 2000 palabras más comunes como características, aunque en este ejemplo no es necesario ya que son pocas palabras.\n",
    "\n",
    "print(all_words.most_common(10))\n",
    "print(word_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(.)': True, 'contains(de)': False, 'contains(el)': False, 'contains(la)': False, 'contains(un)': False, 'contains(película)': False, 'contains(libro)': False, 'contains(los)': False, 'contains(fue)': False, 'contains(y)': True, 'contains(llena)': False, 'contains(tiene)': False, 'contains(una)': False, 'contains(autor)': False, 'contains(esa)': False, 'contains(en)': False, 'contains(emocionante)': False, 'contains(acción)': False, 'contains(ese)': False, 'contains(trama)': False, 'contains(intrigante)': False, 'contains(actores)': False, 'contains(hicieron)': False, 'contains(trabajo)': False, 'contains(excelente)': False, 'contains(describe)': False, 'contains(paisajes)': False, 'contains(con)': False, 'contains(gran)': False, 'contains(detalle)': False, 'contains(cine)': False, 'contains(siempre)': False, 'contains(me)': False, 'contains(ha)': False, 'contains(fascinado)': False, 'contains(novela)': False, 'contains(estaba)': False, 'contains(giros)': False, 'contains(inesperados)': False, 'contains(guion)': False, 'contains(escrito)': False, 'contains(por)': False, 'contains(famoso)': False, 'contains(novelista)': False, 'contains(personajes)': False, 'contains(del)': False, 'contains(eran)': False, 'contains(muy)': False, 'contains(realistas)': False, 'contains(está)': False, 'contains(basada)': False, 'contains(aclamado)': False, 'contains(nuevo)': False, 'contains(álbum)': False, 'contains(banda)': False, 'contains(es)': False, 'contains(increíble)': False, 'contains(concierto)': False, 'contains(estadio)': False, 'contains(estuvo)': False, 'contains(lleno)': False, 'contains(guitarra)': False, 'contains(eléctrica)': False, 'contains(sonido)': False, 'contains(potente)': False, 'contains(claro)': False, 'contains(festivales)': False, 'contains(música)': False, 'contains(al)': False, 'contains(aire)': False, 'contains(libre)': False, 'contains(son)': False, 'contains(mis)': False, 'contains(favoritos)': False, 'contains(pianista)': False, 'contains(interpretó)': False, 'contains(pieza)': False, 'contains(clásica)': False, 'contains(maravillosamente)': False, 'contains(lista)': False, 'contains(reproducción)': False, 'contains(incluye)': False, 'contains(varios)': False, 'contains(géneros)': False, 'contains(,)': False, 'contains(desde)': False, 'contains(jazz)': False, 'contains(hasta)': False, 'contains(rock)': False}\n"
     ]
    }
   ],
   "source": [
    "print(document_features(texts[0]))  # Ejemplo de cómo se vería el primer documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(document_features(d), c) for (d,c) in documents]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"El libro estaba basado en emocionantes paisajes.\",\n",
    "    \"El guion fue aclamado por su trama intrigante.\",\n",
    "    \"La banda tocó jazz y rock en el concierto.\",\n",
    "    \"El cine muestra películas emocionantes de acción.\",\n",
    "    \"El pianista tocó una pieza de música clásica.\"\n",
    "]\n",
    "\n",
    "labels_test = [1, 0, 2, 0, 2]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(classifier.classify(document_features(word_tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/sklearn-logo.png\" width=\"250\"/>\n",
    "\n",
    "## **scikit-learn**\n",
    "https://scikit-learn.org/stable/index.html\n",
    "\n",
    "Veamos cómo hacerlo con la librería scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Datos de entrenamiento\n",
    "texts = [\n",
    "    \"La película fue emocionante y llena de acción.\",\n",
    "    \"Ese libro tiene una trama intrigante.\",\n",
    "    \"Los actores hicieron un trabajo excelente.\",\n",
    "    \"El autor describe paisajes con gran detalle.\",\n",
    "    \"El cine de autor siempre me ha fascinado.\",\n",
    "    \"La novela estaba llena de giros inesperados.\",\n",
    "    \"El guion de esa película fue escrito por un famoso novelista.\",\n",
    "    \"Los personajes del libro eran muy realistas.\",\n",
    "    \"Esa película está basada en un libro aclamado.\",\n",
    "    \"El nuevo álbum de la banda es increíble.\",\n",
    "    \"El concierto en el estadio estuvo lleno.\",\n",
    "    \"La guitarra eléctrica tiene un sonido potente y claro.\",\n",
    "    \"Los festivales de música al aire libre son mis favoritos.\",\n",
    "    \"El pianista interpretó una pieza clásica maravillosamente.\",\n",
    "    \"La lista de reproducción incluye varios géneros, desde jazz hasta rock.\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `scikit-learn`, un **Pipeline** es una forma de automatizar y simplificar un flujo de trabajo que involucra múltiples pasos de procesamiento y modelado. A menudo, en el aprendizaje automático, los datos pasan por una serie de etapas de preprocesamiento antes de ser utilizados por un algoritmo de aprendizaje. Un Pipeline ayuda a definir y automatizar estos pasos secuenciales.\n",
    "\n",
    "`CountVectorizer` implementa el modelo \"Bag-of-Words\" (BoW). El enfoque BoW se refiere al proceso de convertir texto en una representación numérica basada en la frecuencia de las palabras en el texto, sin tener en cuenta el orden o la estructura de las frases.\n",
    "\n",
    "Cuando utilizas `CountVectorizer`, básicamente estás aplicando el proceso de BoW:\n",
    "\n",
    "1. **Tokenización**: Divide el texto en palabras individuales (o tokens).\n",
    "  \n",
    "2. **Construcción del vocabulario**: Se crea un vocabulario con todas las palabras únicas que aparecen en el conjunto de datos.\n",
    "\n",
    "3. **Vectorización**: Se codifica cada documento en función de la frecuencia de las palabras del vocabulario en ese documento.\n",
    "\n",
    "El resultado es una matriz en la que cada fila representa un documento y cada columna representa una palabra del vocabulario. El valor en una posición específica de la matriz indica la frecuencia con la que la palabra (columna) aparece en el documento (fila).\n",
    "\n",
    "`MultinomialNB` implementa el algoritmo de Naive Bayes multinomial diseñado especialmente para características discretas (como cuentas de palabras en textos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "model.fit(texts, labels)  # Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Frases de test\n",
    "test_texts = [\n",
    "    \"El libro estaba basado en emocionantes paisajes.\",\n",
    "    \"El guion fue aclamado por su trama intrigante.\",\n",
    "    \"La banda tocó jazz y rock en el concierto.\",\n",
    "    \"El cine muestra películas emocionantes de acción.\",\n",
    "    \"El pianista tocó una pieza de música clásica.\"\n",
    "]\n",
    "\n",
    "labels_test = [1, 0, 2, 0, 2]\n",
    "\n",
    "# Predicciones\n",
    "predicted_labels = model.predict(test_texts)\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

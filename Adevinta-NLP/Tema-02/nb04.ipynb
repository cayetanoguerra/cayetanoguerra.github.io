{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Indexación y recuperación de información**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag-of-Words (BoW)**\n",
    "\n",
    "El modelo **Bag-of-Words (BoW)** es una técnica comúnmente utilizada en el procesamiento del lenguaje natural (NLP) y la minería de texto para convertir texto en una forma que se pueda ser analizada y procesada por algoritmos de aprendizaje automático. \n",
    "\n",
    "El modelo BoW trata cada documento como una 'bolsa' de palabras, ignorando completamente el orden de las palabras y enfocándose únicamente en la frecuencia (o presencia) de palabras. Representa los documentos como vectores en los cuales cada dimensión corresponde a una palabra del vocabulario, y el valor puede representar la frecuencia de esa palabra en el documento.\n",
    "\n",
    "### **Pasos para Implementar BoW**\n",
    "\n",
    "1. **Tokenización**: Divide el texto en palabras o 'tokens'.\n",
    "   \n",
    "2. **Construcción de Vocabulario**: Crea un vocabulario de todas las palabras únicas presentes en el conjunto de datos.\n",
    "\n",
    "3. **Vectorización**: Cada documento se convierte en un vector, donde cada dimensión representa una palabra del vocabulario, y el valor en esa dimensión representa la frecuencia de esa palabra en el documento (o simplemente la presencia/ausencia de la palabra).\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "Tomemos un pequeño corpus de tres frases para ilustrar cómo funciona:\n",
    "\n",
    "1. \"El gato juega en el jardín.\"\n",
    "2. \"El perro duerme en la casa.\"\n",
    "3. \"El niño juega con el perro.\"\n",
    "\n",
    "Primero, realizamos la tokenización (en este caso, palabras) y construimos un vocabulario con todas las palabras únicas en nuestro corpus:\n",
    "\n",
    "Vocabulario: {el, gato, juega, en, jardín, perro, duerme, casa, niño, con}\n",
    "\n",
    "Tenemos 10 palabras únicas en nuestro vocabulario. Por tanto, cada frase puede ser representada como un vector de 10 dimensiones. Es decir, cada palabra del vocabulario es un vector de 10 dimensiones, donde todas las dimensiones son cero excepto la dimensión correspondiente a la palabra, que es 1.\n",
    "\n",
    "\n",
    "<table align=\"center\">\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">el</th><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">gato</th><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">juega</th><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">en</th><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">jardín</th><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">perro</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">duerme</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">casa</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">niño</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>\n",
    "  <tr><th style=\"background-color: #DAE8FC;\">con</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ahora, representamos cada frase como un vector en este espacio de vocabulario donde el vector resultante se obtiene sumando los vectores de las palabras que aparecen en la frase, como se muestra a continuación:\n",
    "\n",
    "1. \"El gato juega en el jardín.\"\n",
    "   - [2, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "   \n",
    "2. \"El perro duerme en la casa.\"\n",
    "   - [1, 0, 0, 1, 0, 1, 1, 1, 0, 0]\n",
    "   \n",
    "3. \"El niño juega con el perro.\"\n",
    "   - [2, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
    "\n",
    "\n",
    "En la primera frase el primer número representa la cantidad de veces que aparece la palabra \"el\", el segundo número representa \"gato\", el tercero \"juega\", y así sucesivamente según el orden del vocabulario que hemos establecido. Por ejemplo, en la tercera frase, \"el\" aparece dos veces, \"juega\" una vez, \"perro\" una vez, \"niño\" una vez y \"con\" una vez, lo que se refleja en el vector correspondiente.\n",
    "\n",
    "Este proceso convierte las frases en vectores numéricos que pueden ser utilizados en varias tareas de NLP, como la clasificación de texto, la recuperación de información, etc. Espero que esto te ayude a entender mejor cómo funciona el modelo Bag-of-Words!\n",
    "\n",
    "\n",
    "### **Ventajas de BoW**\n",
    "\n",
    "1. **Simplicidad**: Es fácil de entender e implementar.\n",
    "   \n",
    "2. **Eficiencia**: Es computacionalmente menos intensivo en comparación con modelos más complejos.\n",
    "\n",
    "### **Desventajas de BoW**\n",
    "\n",
    "1. **Pérdida de Información**: Al ignorar el orden de las palabras, se puede perder información importante sobre el contexto y la semántica.\n",
    "   \n",
    "2. **Dimensionalidad Alta**: Puede resultar en vectores de dimensión muy alta si el vocabulario es grande, lo que puede aumentar los requerimientos computacionales.\n",
    "\n",
    "3. **Problemas con Palabras Infrecuentes**: Palabras con baja frecuencia pueden no aportar mucho a la similitud entre documentos y pueden incluso introducir ruido.\n",
    "\n",
    "\n",
    "## **Similitud del Coseno**\n",
    "\n",
    "La **similitud del coseno** es una métrica utilizada para determinar el parecido entre dos vectores en un espacio multidimensional. Es calculada como el coseno del ángulo formado entre dos vectores, lo que permite medir la orientación, más que la magnitud, para determinar la similitud. Matemáticamente, se calcula utilizando la fórmula:\n",
    "\n",
    "$$ \\text{Similitud del coseno} = \\frac{A \\cdot B}{||A|| \\cdot ||B||} $$\n",
    "\n",
    "donde:\n",
    "- $ A $ y $ B $ son dos vectores.\n",
    "- $ A \\cdot B $ es el producto punto de $ A $ y $ B $.\n",
    "- $ ||A|| $ y $ ||B|| $ son las magnitudes de los vectores $ A $ y $ B $, respectivamente.\n",
    "\n",
    "El valor resultante estará en el rango de -1 a 1, donde 1 indica que los vectores son idénticos, 0 indica que son ortogonales (sin similitud) y -1 indica que están opuestos. Esta métrica es ampliamente utilizada en análisis de texto, recuperación de información, y ciencia de datos para medir la similitud entre documentos o elementos en general.\n",
    "\n",
    "\n",
    "## **Aplicaciones**\n",
    "### **Similitud de documentos**\n",
    "\n",
    "Utilizando **BoW** y la **similitud del coseno** podemos calcular la similitud entre documentos. Supongamos que queremos medir el parecido entre diferentes frases (o documentos). Podemos utilizar el modelo BoW para convertir cada frase en un vector y luego medir la similitud entre los vectores utilizando una métrica de similitud como la distancia euclidiana o la similitud del coseno. Esto puede ser útil en tareas como la recuperación de información, la clasificación de texto, etc.\n",
    "\n",
    "Veamos un ejemplo sencillo. Supongamos que tenemos un corpus de cuatro frases:\n",
    "\n",
    "- A: \"El gato juega en el jardín.\"\n",
    "- B: \"El perro duerme en la casa.\"\n",
    "- C: \"El niño juega con el perro.\"\n",
    "- D: \"El coche es azul.\"\n",
    "\n",
    "Ahora nuestro vocabulario será:\n",
    "\n",
    "Vocabulario: {el, gato, juega, en, jardín, perro, duerme, casa, niño, con, coche, es, azul}\n",
    "\n",
    "Por tanto, nuestros nuevos vectores de frases serán:\n",
    "\n",
    "- A = [2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "- B = [1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "- C = [2, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
    "- D = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
    "\n",
    "\n",
    "**1. Similitud entre Frase (A) y Frase (B):**\n",
    "- Producto punto (A·B): $ 1\\cdot1 + 1\\cdot0 + 1\\cdot0 + 1\\cdot1 + 1\\cdot0 + 0\\cdot1 + 0\\cdot1 + 0\\cdot1 + 0\\cdot0 + 0\\cdot0 + 0\\cdot0 + 0\\cdot0 + 0\\cdot0 = 2 $\n",
    "- Magnitudes: $||A|| = \\sqrt{5} $, $ ||B|| = \\sqrt{5} $\n",
    "- Similitud del coseno: $ \\frac{2}{\\sqrt{5}\\cdot\\sqrt{5}} = \\frac{2}{5} = 0.4$\n",
    "\n",
    "**2. Similitud entre Frase (A) y Frase (C):**\n",
    "- Producto punto (A·C): $ 1\\cdot2 + 1\\cdot0 + 1\\cdot1 + 1\\cdot0 + 1\\cdot0 + 0\\cdot1 + 0\\cdot0 + 0\\cdot0 + 0\\cdot1 + 0\\cdot1 + 0\\cdot0 + 0\\cdot0 + 0\\cdot0 = 3 $\n",
    "- Magnitudes: $ ||A|| = \\sqrt{5} $, $ ||C|| = \\sqrt{7} $\n",
    "- Similitud del coseno: $ \\frac{3}{\\sqrt{5}\\cdot\\sqrt{7}} = 0.507 $\n",
    "\n",
    "**3. Similitud entre Frase (A) y Frase (D):**\n",
    "- Producto punto (A·D): $ 1\\cdot1 + 1\\cdot0 + 1\\cdot0 + 1\\cdot0 + 1\\cdot0 + 0\\cdot0 + 0\\cdot0 + 0\\cdot0 + 0\\cdot0 + 0\\cdot0 + 0\\cdot1 + 0\\cdot1 + 0\\cdot1 = 1 $\n",
    "- Magnitudes: $ ||A|| = \\sqrt{5} $, $ ||D|| = \\sqrt{3} $\n",
    "- Similitud del coseno: $ \\frac{1}{\\sqrt{5}\\cdot\\sqrt{3}} = 0.258$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 1\n",
    "\n",
    "Calcula la similitud entre las frases B y C usando la distancia del coseno.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF**\n",
    "\n",
    "TF-IDF (en inglés \"Term Frequency – Inverse Document Frequency\") es una técnica utilizada en procesamiento de lenguaje natural (NLP) y recuperación de información (IR) para representar documentos como vectores numéricos. Está diseñada para reflejar la importancia de una palabra en un documento en relación con un conjunto de documentos o corpus.\n",
    "\n",
    "Sus dos componentes son:\n",
    "\n",
    "1. **Frecuencia de Término (TF)**:\n",
    "   - Representa la frecuencia de una palabra en un documento específico.\n",
    "   - Se calcula como el número de veces que una palabra aparece en un documento dividido por el número total de palabras en ese documento.\n",
    "   - $ \\text{TF}(t, d) = \\frac{\\text{número de veces que el término } t \\text{ aparece en el documento } d}{\\text{número total de palabras en el documento } d} $\n",
    "\n",
    "2. **Inversa de la Frecuencia del Documento (IDF)**:\n",
    "   - Mide lo común o raro que es un término en todo el corpus.\n",
    "   - Se calcula como el logaritmo natural del número total de documentos dividido por el número de documentos que contienen el término de interés.\n",
    "   - $ \\text{IDF}(t, D) = \\log \\frac{\\text{número total de documentos en el corpus } D}{\\text{número de documentos que contienen el término } t} $\n",
    "   - Este valor será mayor para palabras que son raras en el corpus y más pequeño para palabras que son comunes.\n",
    "\n",
    "El valor TF-IDF para un término en un documento es simplemente el producto de TF e IDF:\n",
    "\n",
    "$$ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $$\n",
    "\n",
    "¿Por qué es útil? Palabras como \"y\", \"de\", \"en\", etc., aparecerán con mucha frecuencia en muchos documentos (alto TF), pero no son únicas o relevantes para un documento en particular, por lo que su IDF será bajo, reduciendo su relevancia. Por otro lado, si una palabra aparece frecuentemente en un documento, pero raramente en otros documentos del corpus, tendrá un alto valor TF-IDF, indicando que es una palabra clave importante para ese documento en particular.\n",
    "\n",
    "En la práctica, TF-IDF es utilizado para tareas como la recuperación de información (por ejemplo, motores de búsqueda) y clasificación de documentos, entre otras. Permite que se pueda hacer una distinción entre los términos comunes y los términos relevantes en grandes conjuntos de datos textuales.\n",
    "\n",
    "Veamos un ejemplo de cómo calcular el valor TF-IDF para una palabra dentro de un documento en un corpus de documentos sencillo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 0.16666666666666666\n",
      "IDF: 1.0986122886681098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1831020481113516"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Ejemplo de documentos\n",
    "documents = [\n",
    "    \"El sol brilla en el cielo\",\n",
    "    \"La lluvia cae sobre la ciudad\",\n",
    "    \"El gato duerme en el sofá\"\n",
    "]\n",
    "\n",
    "# Calcular TF-IDF\n",
    "def tf_idf(term, document, documents):\n",
    "    # TF\n",
    "    words = document.split()  # Dividir el documento en palabras\n",
    "    tf = words.count(term) / len(words)  # Calcular TF\n",
    "    print(\"TF:\", tf)\n",
    "    \n",
    "    # IDF\n",
    "    n_documents_with_term = 0  # Contador de documentos con el término\n",
    "    for document in documents:\n",
    "        words = document.split()  # Dividir el documento en palabras\n",
    "        if term in words:  # Si el término está en el documento\n",
    "            n_documents_with_term += 1  # Incrementar el contador\n",
    "    if n_documents_with_term != 0:\n",
    "        idf = math.log(len(documents) / n_documents_with_term)  # Calcular IDF\n",
    "    else:\n",
    "        idf = 0\n",
    "    print(\"IDF:\", idf)\n",
    "\n",
    "    # TF-IDF\n",
    "    tf_idf = tf * idf  # Calcular TF-IDF\n",
    "\n",
    "    return tf_idf\n",
    "        \n",
    "tf_idf(\"gato\", documents[2], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Práctica: similitud de documentos con bag-of-words y similitud del coseno\n",
    "\n",
    "#### **Objetivo:**\n",
    "Adquirir experiencia práctica en el cálculo de la similitud entre documentos utilizando el método de bag-of-words, la métrica de similitud del coseno y la utilización de funciones de PyTorch.\n",
    "\n",
    "#### **Contexto:**\n",
    "Se proporciona un conjunto de 12 documentos relacionados con tecnología móvil y vehículos eléctricos. En el conjunto, algunos documentos tratan solo de tecnología móvil, otros solo de vehículos eléctricos, y hay documentos que abordan ambos temas simultáneamente.\n",
    "\n",
    "<div align=\"center\">\n",
    "<a src=\"./data/docs.zip\">\n",
    "<img src=\"../imgs/zip.png\" width=\"10%\"> \n",
    "<br>Documentos\n",
    "</a>\n",
    "</div>\n",
    "\n",
    "#### **Instrucciones:**\n",
    "1. Preprocesamiento de datos:\n",
    "   - Convertir todos los textos a minúsculas.\n",
    "   - Eliminar signos de puntuación y otros caracteres no alfabéticos.\n",
    "   - (Opcional) realizar una eliminación de palabras comunes o \"stop words\".\n",
    "2. Construcción de bag-of-words:\n",
    "   - Crear un vocabulario global basado en todas las palabras únicas presentes en los documentos.\n",
    "   - Representar cada documento como un vector en el espacio del vocabulario.\n",
    "3. Cálculo de la similitud del coseno:\n",
    "   - Calcular la similitud del coseno entre todos los pares de documentos utilizando sus representaciones vectoriales. Utilizar para ello la función `cosine_similarity` de PyTorch.\n",
    "    \n",
    "4. Análisis:\n",
    "   - Identificar qué documentos son más similares entre sí y cuáles son menos similares.\n",
    "   - Observar la relación entre la similitud y los temas tratados en los documentos. Por ejemplo, determinar si los documentos que tratan sobre el mismo tema tienden a ser más similares entre sí que aquellos que tratan temas diferentes.\n",
    " \n",
    "\n",
    "Entregables:\n",
    "   - Código fuente utilizado para el preprocesamiento, construcción del bag-of-words y cálculo de la similitud del coseno.\n",
    "   - Breve informe que detalle los hallazgos en cuanto a la similitud entre documentos.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Adevinta-ULPGC-logo.jpg\" width=\"530px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aprendizaje por refuerzo 2**\n",
    "\n",
    "Hemos visto ya cómo funciona el algoritmo **Q-Learning** en su versión simplificada. Ahora dotaremos de mayor formalismo al aprendizaje por refuerzo. Comenzaremos estudiando lo que significa que un sistema tenga la **propiedad de Markov**. Esto nos permitirá modelar un problema de aprendizaje por refuerzo como un conjunto de estados y transiciones probabilísticas entre estados por donde un agente va a transitar.\n",
    "\n",
    "Analizaremos seguidamente lo que significa **recompensa** y **retorno**, y cómo afecta un **factor de descuento** al retorno obtenido por un agente.\n",
    "\n",
    "Luego, aplicaremos **retorno con descuento** a un sistema con la propiedad de Markov, lo que llamaremos **proceso de recompensa de Markov**. Aquí veremos con más amplitud las **ecuaciones de Bellman**.\n",
    "\n",
    "Por último, estudiaremos los dos métodos iniciales del aprendizaje por refuerzo: **value iteration** y **policy value**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Propiedad de Markov**\n",
    "\n",
    "Consiste en que la evolución de un sistema dependa exclusivamente de su estado y de la acción realizada. Es decir, su evolución no depende de los estados anteriores ni de las acciones anteriores. Se dice que el sistema \"no tiene memoria\". Matemáticamente se expresaría así:\n",
    "\n",
    "\n",
    "$$ P[s_{t+1} | s_t] = P[s_{t+1} | s_t, s_{t-1}, \\cdots ,s_1 ] $$\n",
    "\n",
    "> El futuro depende únicamente del presente, no del pasado.\n",
    "\n",
    "Por tanto, un sistema con la **propiedad de Markov** puede ser representado mediante una serie de estados conectados con ciertas probabilidades de transición entre un estado y otro. Por ejemplo, el tiempo atmosférico de día a día podría ser simulado de esta forma:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/markov.svg\" width=50%>\n",
    "</div>\n",
    "\n",
    "Esto quiere decir, por ejemplo, que si el martes está nublado hay un 10% de probabilidades de que el miércoles esté soleado. Las probabilidades las podemos representar también mediante una **tabla de probabilidades de transición**, donde la suma de cada fila debe ser 1.\n",
    "\n",
    "|          | soleado | nublado | lluvioso |\n",
    "|----------|---------|---------|----------|\n",
    "| **soleado**  | 0\\.6    | 0\\.3    | 0\\.1     |\n",
    "| **nublado**  | 0\\.1    | 0\\.5    | 0\\.4     |\n",
    "| **lluvioso** | 0\\.6    | 0\\.2    | 0\\.2     |\n",
    "\n",
    "\n",
    "Esto puede generar cadenas de estados, como por ejemplo \"$soleado \\rightarrow nublado \\rightarrow nublado \\rightarrow lluvioso$\" con una cierta probabilidad de ocurrencia. A estas cadenas las denominamos **cadenas de Markov**.\n",
    "\n",
    "\n",
    "## **Recompensa y retorno**\n",
    "\n",
    "Las recompensas son los valores numéricos que recibe el agente al realizar alguna acción en algunos estados del entorno. El valor numérico puede ser positivo o negativo en función de las acciones del agente. En el aprendizaje por refuerzo nos preocupamos por maximizar la recompensa acumulada (todas las recompensas que el agente recibe del entorno) en lugar de las recompensas que recibe en el estado actual (también llamada recompensa inmediata). Esta suma total de recompensas que el agente recibe del entorno lo denominaremos **retorno**.\n",
    "\n",
    "Por lo tanto, definiremos **retorno** como:\n",
    "\n",
    "$$ G_t = r_{t+1} + r_{t+2} + \\cdots + r_{T} $$\n",
    "\n",
    "donde $r_{t+1}$ es la recompensa que recibirá el agente en $t$ al realizar la acción $a$. $T$ es el instante final. El objetivo del aprendizaje por refuerzo es maximizar el retorno esperado.\n",
    "\n",
    "\n",
    "### **Retorno con descuento**\n",
    "\n",
    "El **factor de descuento $\\gamma$** determina cuánta importancia se debe dar a la recompensa inmediata y cuánta a las recompensas futuras. Básicamente, esto nos ayuda a evitar un valor de retorno infinito en tareas continuas. El factor $\\gamma$ debe tener un valor mayor que 0 y menor que 1. Un valor cercano al 0 significa que se le da más importancia a la recompensa inmediata y un valor cercano a 1 significa que se le da más importancia a las recompensas futuras. Por lo tanto, los valores comunes para el factor de descuento se encuentran entre 0.2 y 0.8. Por tanto, definimos el **retorno** con **factor de descuento** como: \n",
    "\n",
    "$$ G_t = r_{t+1} + \\gamma \\cdot r_{t+2} + \\gamma^2 \\cdot r_{t+3} + \\cdots$$\n",
    "\n",
    "\n",
    "Esto significa que a la recompensa $r_{t+1}$ que obtendremos en el próximo paso le daremos todo su valor, pero a la recompensa $r_{t+2}$ que obtendremos después de dos pasos solo le daremos una fracción $\\gamma$ de su valor. A la que esté a tres pasos $r_{t+3}$ le daremos una fracción $\\gamma$ de la fracción $\\gamma$, es decir, $\\gamma^2$. Y así sucesivamente.\n",
    "\n",
    "\n",
    "Supongamos un sistema trivial formado por un solo estado $s$ cuya probabilidad de pasar del estado $s$ a sí mismo es del 100%. La recompensa por hacer esa transición es de, por ejemplo, 27. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/unestado.svg\" width=20%>\n",
    "</div>\n",
    "\n",
    "\n",
    "¿Cuál será la recompensa total acumulada (**retorno**) al cabo de $n$ transiciones? Pues será la recompensa de ese momento: 27, más un porcentaje decreciente de las futuras recompensas. Si suponemos que $\\gamma$ vale 0.8, la recompensa total acumulada en 40 pasos será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno: 134.98205542205693\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "r = 27\n",
    "gamma = 0.8\n",
    "G = 0\n",
    "hist = []\n",
    "steps = 40\n",
    "\n",
    "for step in range(steps):\n",
    "    G = G + gamma**step * r\n",
    "    # G = r + gamma*G # Otra forma de calcular lo mismo\n",
    "    hist.append(G)\n",
    "    \n",
    "print(\"Retorno:\", G)\n",
    "plt.plot(hist)\n",
    "plt.xlabel(\"Número de pasos\")\n",
    "plt.ylabel(\"Retorno\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que el factor $\\gamma$ hace que el retorno converja a un valor concreto. ¿Qué pasaría si el valor de $\\gamma$ fuera $1.1$? Pruébalo.\n",
    "\n",
    "Este proceso iterativo que hemos visto:\n",
    "\n",
    "$$ G_t = r_{t+1} + \\gamma \\cdot r_{t+2} + \\gamma^2 \\cdot r_{t+3} + \\cdots$$\n",
    "\n",
    "también puede expresarse de manera recursiva:\n",
    "\n",
    "$$ G_t = r_{t+1} + \\gamma \\cdot G_{t+1}$$\n",
    "\n",
    "Por lo tanto, si la recompensa fuera siempre la misma, podríamos calcular el resultado de manera directa:\n",
    "\n",
    "$$ G = 27 + 0.8 \\cdot G$$\n",
    "\n",
    "$$ G(1-0.8) = 27 $$\n",
    "\n",
    "$$ G = \\frac{27}{0.2} = 135 $$\n",
    "\n",
    "\n",
    "### **Proceso de recompensa de Markov (MRP)**\n",
    "\n",
    "La siguiente figura muestra un grafo con varias transiciones entre estados y sus probabilidades de ocurrencia. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/grafo.svg\">\n",
    "</div>\n",
    "\n",
    "Hemos visto cómo la **propiedad de Markov** en un grafo define la dinámica de un entorno utilizando un conjunto de estados $S$ y una **matriz de probabilidad de transición** $P$. También sabemos ya que el **aprendizaje por refuerzo** tiene que ver con el objetivo de maximizar el retorno, así que añadimos recompensas a nuestras cadenas de Markov. La tabla muestra la recompensa que se obtiene al llegar a un estado.\n",
    "\n",
    "| a | b  | c  | d |\n",
    "|---|----|----|---|\n",
    "| 5 | 37 | 11 | 9 |\n",
    "\n",
    "\n",
    "y esto nos lleva al **proceso de recompensa de Markov**.\n",
    "\n",
    "\n",
    "$$ r_s = \\mathbb E[r_{t+1} | s_t] $$\n",
    "\n",
    "\n",
    "Esta fórmula quiere decir que la recompensa inmediata esperada en un estado $s$ viene dada por la [esperanza matemática](https://es.wikipedia.org/wiki/Esperanza_matem%C3%A1tica) de las recompensas de los estados a los que tenga conexión. Por ejemplo, vemos en la figura que el estado $b$ tiene transiciones a los estados $c$ y $a$ con probabilidades 0.05 y 0.95 respectivamente. Por tanto:\n",
    "\n",
    "\n",
    "$$ r_b = p_{b,c} \\cdot r_c + p_{b,a} \\cdot r_a = 0.05 \\cdot 11 + 0.95 \\cdot 5 $$\n",
    "\n",
    "Finalmente, lo que nos interesa es tener el retorno para cada estado $G_s$, y para eso debemos añadirle un factor de descuento $\\gamma$ a nuestro cálculo. Recordemos que partiendo de un estado $s$ generamos una sucesión de estados seleccionados de manera aleatoria (cadena de Markov) por lo que el retorno obtenido en cada cadena es cambiante. Es decir, partiendo del estado $a$ la cadena de estados por la que se pasa puede ser:\n",
    "\n",
    "$a,b,a,b,c,d,d,d,d \\dots$\n",
    "\n",
    "o\n",
    "\n",
    "$a,d,d,d,d,d,d,d,d \\dots$\n",
    "\n",
    "Lo cual da resultados diferentes. Al ser aleatoria la dinámica del grafo ¿cómo podemos calcular el retorno promedio para cada estado $G_s$? Una forma podría ser, para cada estado $s$, generar múltiples cadenas de estados y calcular el retorno promedio sobre estas. El siguiente código implementa este método y nos permite calcular cuál es el retorno obtenido para cada estado del grafo. Por ejemplo, vemos que, si partimos desde el estado $a$, el retorno esperado es 90.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno medio para el estado 0 : 90.95110629090061\n",
      "Retorno medio para el estado 1 : 108.20844548983852\n",
      "Retorno medio para el estado 2 : 57.60614596515345\n",
      "Retorno medio para el estado 3 : 57.74389486364507\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matriz de transiciones\n",
    "P = [[0,0.98,0.01,0.01], # a\n",
    "      [0.95,0,0.05,0],  # b\n",
    "      [0,0,0,1.0], # c\n",
    "      [0.1,0,0,0.9]] # d\n",
    "\n",
    "P = np.array(P)\n",
    "\n",
    "# Vector de recompensas\n",
    "R = [5,37,11,9]\n",
    "\n",
    "\n",
    "states = [0,1,2,3]\n",
    "steps = 50\n",
    "episodes = 2000\n",
    "gamma = 0.8\n",
    "\n",
    "for s in states:\n",
    "    GE = 0\n",
    "    hist = []\n",
    "    for episode in range(episodes):\n",
    "        G = 0\n",
    "        state = s\n",
    "        for step in range(steps):\n",
    "            new_state = np.random.choice(states,p=P[state])\n",
    "            G += gamma**step * R[state]\n",
    "            state = new_state\n",
    "        GE += G\n",
    "        hist.append(GE/(episode+1))\n",
    "\n",
    "    print(\"Retorno medio para el estado\",s,\":\", hist[-1])\n",
    "    #plt.plot(hist)\n",
    "    #plt.xlabel(\"Número de episodios\")\n",
    "    #plt.ylabel(\"Retorno medio\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problemas episódicos y problemas continuos**\n",
    "\n",
    "Un problema **episódido** es aquel en el que existe un estado final. Por ejemplo, un juego en el que se gana o se pierde. Un **episodio** consiste en una secuencia de pasos desde un estado inicial a un estado final.\n",
    "\n",
    "Un problema en el que no existe un estado final se denomina **continuo**. El agente continúa su funcionamiento de forma ilimitada $(T \\rightarrow \\infty)$.\n",
    "\n",
    "Para unificar el tratamiento de los problemas continuos y episódicos se asume que a partir del estado final, todas las transiciones conducen al estado final y tienen recompensa 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ecuaciones de Bellman**\n",
    "\n",
    "Podemos llegar a otra forma de calcular lo anterior mediante las ecuaciones de Bellman. Para esto, calcularemos una función de valor $V(s)$ que nos dará para cada estado $s \\in S$ el retorno esperado.\n",
    "\n",
    "\n",
    "$$\n",
    "V_{t+1}(s) \\leftarrow R + \\gamma \\cdot \\mathbb E[ V_t(S_{t+1} | S_t=s)] \n",
    "$$\n",
    "\n",
    "\n",
    "Dado que $V(S)$ es un vector, es posible expresar lo anterior mediante el siguiente cálculo matricial:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}v_1 \\\\ \\vdots \\\\v_n  \\end{bmatrix} \\leftarrow \\begin{bmatrix}r_1 \\\\ \\vdots \\\\r_n  \\end{bmatrix} + \\gamma \n",
    "\\begin{bmatrix}\n",
    "p_{1,1} & \\cdots & p_{1,n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "p_{n,1} & \\cdots & p_{n,n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}v_1 \\\\ \\vdots \\\\v_n  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "El siguiente código muestra esta función de manera algorítmica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V = [ 90.80343965 108.29467228  57.40289979  58.02570534]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matriz de probabilidades de transición\n",
    "P = [[0,0.98,0.01,0.01], # a\n",
    "      [0.95,0,0.05,0],  # b\n",
    "      [0,0,0,1.0], # c\n",
    "      [0.1,0,0,0.9]] # d\n",
    "\n",
    "P = np.array(P)\n",
    "\n",
    "# Vector de recompensas\n",
    "R = np.array([5,37,11,9])\n",
    "\n",
    "states = [0,1,2,3]\n",
    "gamma = 0.8\n",
    "\n",
    "V = np.zeros(len(states), dtype=float)\n",
    "\n",
    "while True:\n",
    "    V_new =  R + gamma * np.dot(P,V)  # np.dot(P,V) es la multiplicación de la matriz P por el vector V\n",
    "    conv = np.abs(V - V_new).sum()\n",
    "    # print(\"Convergencia:\", conv)\n",
    "    V = V_new\n",
    "    if conv < 0.1:\n",
    "        break\n",
    "\n",
    "print(\"V =\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método nos permite calcular el retorno esperado para cada estado del grafo. Por ejemplo, $V(0)$ corresponde al retorno promedio para una cadena de Markov con recompensa que comienza en el estado 0.\n",
    "\n",
    "Esta matriz, que pudo haber sido inicializada de manera totalmente aleatoria, nos ha servido para **modelar** el problema de obtención de los valores $V$. A partir de ahora, prescindiremos de esta matriz de probabilidades y nos moveremos entre estados con el único objetivo de maximizar el **retorno**. Para ello vamos a utilizar una función que llamaremos **política**, y la denotaremos con el símbolo $\\pi$.\n",
    "\n",
    "\n",
    "#### **Política**\n",
    "\n",
    "<div style=\"background-color: #fdebd0; border-left: 5px solid #ffb366; padding: 1.5em; margin: 30px; width: 600px\">\n",
    "La <b>política</b> es una función que otorga, a partir de un estado, probabilidades de elección sobre las acciones a tomar por el agente con el objetivo de maximizar el <b>retorno</b>.\n",
    "</div>\n",
    "\n",
    "Por tanto, $\\pi(a|s)$ nos indica la probabilidad de que el agente tome la acción $a$ en el estado $s$. Por ejemplo, supongamos que el agente está en el estado $s$ y hay tres posibles acciones a tomar: $a_1$, $a_2$ y $a_3$. La función $\\pi$ nos podría devolver: $\\pi(a_1|s)$ = 0.9,  $\\pi(a_2|s)$ = 0.0 y $\\pi(a_3|s)$ = 0.1. Esto nos permite calcular la esperanza matemática en función de $\\pi$ y, por tanto, tener: $\\mathbb E_\\pi$.\n",
    "\n",
    "$$\n",
    "V_\\pi(s) \\leftarrow R + \\gamma \\cdot \\mathbb E_\\pi[ V_\\pi(S_{t+1}) | S_t=s] \n",
    "$$\n",
    "\n",
    "Ahora, $V_\\pi$ simplemente significa que los valores son calculados a partir de la función $\\pi$.\n",
    "\n",
    "Digamos que $\\mathbb E_\\pi$ prescinde de la anterior matriz de probabilidades de transición e  incorpora nuestra nueva función $\\pi(a|s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Matriz de recompensas**\n",
    "\n",
    "Hasta ahora hemos utilizado el vector de recompensas $R$, lo cual indica que llegados a un estado $s$ obtendremos la recompensa $R(s)$. A partir de ahora generalizaremos las recompensas con la matriz $R^a_{s,s'}$ que indica que la recompensa vendrá dada por el estado $s$ del que se parte, por la acción $a$ que se tome y por el estado $s'$ al que llegue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función $V$ y función $Q$**\n",
    "\n",
    "Hemos introducido la función $V$, que denominaremos **función de valor de estado**. Introduciremos ahora la función $Q$ que llamaremos **función de valor de estado-acción**. Como ya hemos visto, $V(s)$ representa el retorno esperado que tendríamos a partir de un estado $s$. $Q(s,a)$ es el retorno esperado que tendríamos a partir del estado $s$ si ejecutamos la acción $a$. Por tanto:\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\sum_a \\pi(a|s) \\cdot Q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "Y, a su vez, si partimos del estado $s$ y mediante la acción $a$ llegamos al estado $s'$:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = R^a_{s,s'} + \\gamma \\cdot V_\\pi(s')\n",
    "$$\n",
    "\n",
    "Por tanto:\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\sum_a \\pi(a|s) \\cdot [R^a_{s,s'} + \\gamma \\cdot V_\\pi(s')]\n",
    "$$\n",
    "\n",
    "Y también:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = R^a_{s,s'} + \\gamma \\cdot [\\sum_{a'} \\pi(a'|s') \\cdot Q_\\pi(s',a')]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Veámoslo un poco más claro con la figura siguiente. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/VyQ.svg\" width=50%>\n",
    "</div>\n",
    "\n",
    "\n",
    "Tenemos tres estados $s_1$, $s_2$ y $s_3$. Cada uno tiene su valor $V_\\pi(s_1)$, $V_\\pi(s_2)$ y $V_\\pi(s_3)$ . $s_1$ está conectado a $s_2$ mediante la acción $a’$ y a $s_3$ mediante la acción $a’’$. Estos enlaces tienen asociadas sus respectivas recompensas $R_{s_1,s_2}^{a'}$ y $R_{s_1,s_3}^{a'’}$. Ahora es importante recordar que el agente escogerá ir desde $s_1$ a $s_2$ o a $s_3$ en función de las probabilidades que genere la función $\\pi$ o, dicho de otra forma, la política $\\pi$. Supongamos que escoger la acción $a’$ tiene una probabilidad del $80\\%$ y escoger la acción $a’’$ el restante $20\\%$.\n",
    "\n",
    "$$\n",
    "V_\\pi(s_1) = 0.8 \\cdot [R_{s_1,s_2}^{a'} +  \\gamma \\cdot V_\\pi(s_2)] + 0.2 \\cdot [R_{s_1,s_3}^{a''} +  \\gamma \\cdot V_\\pi(s_3)]\n",
    "$$ \n",
    "\n",
    "o lo que es lo mismo:\n",
    "\n",
    "$$\n",
    "V_\\pi(s_1) = 0.8 \\cdot Q_\\pi(s_1, a') + 0.2 \\cdot Q_\\pi(s_1, a'') = \\sum_{a\\in(a',a'')} \\pi(a|s_1) \\cdot Q_\\pi(s_1,a)\n",
    "$$ \n",
    "\n",
    "\n",
    "#### **Transiciones no deterministas**\n",
    "\n",
    "Hasta ahora hemos supuesto que al estar en el estado $s$ y ejecutar la acción $a$ nos vamos al estado $s’$. Pero esto no siempre tiene por qué ser así. Si estamos programando un agente para que aprenda a jugar al ajedrez y le decimos que ejecute la acción “mover el peón ‘x’ una casilla hacia adelante”, pasaremos de un estado del juego a otro de una forma totalmente determinista. Pero si estamos enseñando al agente a jugar al parchís y ejecutamos la acción “tirar el dado” podemos irnos a seis estados distintos de una manera estocástica. Por lo tanto, en muchos contextos distintos, ejecutar la acción $a$ no nos garantiza llegar al estado $s’$, sino solo una probabilidad de llegar a ese estado.\n",
    "\n",
    "\n",
    "Para manejar esta nueva situación vamos a hacer uso de la matriz $P_{s,s'}^a$, la cual nos dará la probabilidad de que partiendo del estado $s$ y ejecutando la acción $a$ lleguemos al estado $s’$. Por tanto, $Q_\\pi(s,a)$ corresponderá al promediado mediante $P_{s,s’}^a$ de todos los estados a los que se llegue a partir del estado $s$ ejecutando la acción $a$.\n",
    "\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = \\sum_{s'} P_{s,s'}^a [ R^a_{s,s'} + \\gamma \\cdot V_\\pi(s') ]\n",
    " $$\n",
    "\n",
    "\n",
    "Veámoslo con más claramente con esta ilustración.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"imgs/VyQ_P.svg\" width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Value Iteration**\n",
    "\n",
    "Ha llegado el momento de concretar nuestra política $\\pi$. ¿Cómo vamos a seleccionar las acciones a realizar para que el retorno sea máximo desde cada estado? Teniendo presente lo que acabamos de ver sobre las transiciones no deterministas, volvamos a retomar, por sencillez, la funcion $V$ y $Q$ con transiciones deterministas.\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) =  R^a_{s,s'} + \\gamma \\cdot V_\\pi(s')\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\sum_a \\pi(a|s) \\cdot Q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "En \"pseudopython\", vamos a implementar el algoritmo de aprendizaje **value interation**, que será parecido al algoritmo anterior, pero no contaremos con una matriz de probabilidades de transición. Los nuevos valores de $V$ no se calcularán a partir de un promediado sino de la mejor opción de cada estado.\n",
    "\n",
    "\n",
    "```python\n",
    "1 Initialize V to arbitrary values\n",
    "2 while V not converge:\n",
    "3    for s in S:\n",
    "4        for a in A:\n",
    "5            Q[s,a] = R[s,a,s_] + gamma * V[s_]  # s_ corresponde al estado al que se llega\n",
    "6        V[s] = max(Q[s,:])\n",
    "```\n",
    "\n",
    "\n",
    "Como vemos, recorremos el conjunto $S$ de todos los estados y, en cada estado $s$, analizamos cada acción $a$ posible desde $s$. Vemos a qué **valor de estado** $V(s')$ nos lleva (valor de retorno esperado), lo multiplicamos por el factor $\\gamma$ y le sumamos la recompensa por ir de $s$ a $s'$. Eso nos va generando para cada estado $s$ la fila de la matriz $Q[s,a_1], \\dots, Q[s,a_n]$. Una vez calculada la fila, nos quedamos con el máximo valor `max(Q[s,:])` para actualizar $V(s)$. \n",
    "\n",
    "Este algoritmo nos puede recordar al método **Q-Learning** ya visto. Sí, se parece, pero ¿por qué no es el mismo método? Hay una diferencia fundamental entre ambos. Cuando usamos el método **value iteration** podemos calcular directamente todos los valores de $V$ y $Q$ porque conocemos *a priori* todos los estados, transiciones y recompensas del entorno. Sin embargo, en el método **Q-Learning** no conocemos nada de antemano y, por tanto, el agente debe dedicarse a explorar antes. Es decir, el entorno es totalmente desconocido.\n",
    "\n",
    "El método **Q-Learning** tiene una política $\\pi$ que lo lleva a **explorar** para obtener nuevo conocimiento o a **explotar** para hacer uso del conocimiento que ya tiene y obtener recompensas seguras. El método **value iteration** se limita a calcular todos los valores $Q$ y escoger los máximos de cada estado para actualizar las $V$. Una vez los tenga calculados, la tabla $Q$ será estática y el agente la consultará para seleccionar la acción que más le convenga.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Policy Iteration**\n",
    "\n",
    "El método **value iteration** termina cuando los valores de $V$ convergen. En ese momento, la tabla $Q$ actuará como política y podrá guiar las acciones del agente de manera óptima. Sin embargo, podríamos obtener una política óptima incluso antes de que la tabla $V$ converja. Esto es lo que intenta hacer el método **policy iteration**.\n",
    "\n",
    "\n",
    "**Policy iteration** comienza generando una política $\\pi$ al azar y con unos valores de $V$ inicializados a cero. A partir de ahí se generan las acciones a tomar por cada estado $s$ del conjunto total de estados $S$. Es decir, cada acción viene dada por la función $a=\\pi(s)$.\n",
    "\n",
    "$$\n",
    "V_\\pi(s) \\leftarrow R(s,\\pi(s),s’) + \\gamma \\cdot V_\\pi(s')\n",
    "$$\n",
    "\n",
    "\n",
    "De nuevo, por simplicidad, supongamos que las transiciones son deterministas. La expresión anterior es realmente un sistema lineal que podemos resolver de forma algebraica o de forma iterativa.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}V(s_1) \\\\ V(s_2) \\\\ \\vdots \\\\V(s_n) \\end{bmatrix} = \\begin{bmatrix}r(s_1,\\pi(s_1),s') \\\\ r(s_2,\\pi(s_2),s'') \\\\ \\vdots \\\\r(s_n,\\pi(s_n),s^{(n)}) \\end{bmatrix} + \\gamma \\begin{bmatrix}V(s') \\\\ V(s'') \\\\ \\vdots \\\\V(s^{(n)}) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Los estados $s’, s’’, \\dots, s^{(n)}$ hacen referencia a los estados a los que se llega, y, por supuesto, pertenecen al conjunto de estados $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos un ejemplo muy simple a modo de ilustración. Supongamos que tenemos tres estados $s_0$, $s_1$ y $s_2$ y queremos calcular sus valores $V$ sabiendo que tenemos una política $\\pi$ que hará pasar del estado $s_0 \\rightarrow s_1$, del $s_1 \\rightarrow s_1$ y del $s_2 \\rightarrow s_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.999500463307562, 34.99950046330756, 25.79960037064605]\n"
     ]
    }
   ],
   "source": [
    "V = [0,0,0]\n",
    "R = [3,7,1]\n",
    "\n",
    "gamma = 0.8\n",
    "\n",
    "for i in range(50):\n",
    "    V[0] = R[0] + gamma*V[1]\n",
    "    V[1] = R[1] + gamma*V[1]\n",
    "    V[2] = R[2] + gamma*V[0]\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este sistema lineal lo podemos resolver también así:\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "V(s_0) &=  3 + 0.8\\cdot V(s_1) \\\\ \n",
    "V(s_1) &=  7 + 0.8\\cdot V(s_1) \\\\ \n",
    "V(s_2) &=  1 + 0.8\\cdot V(s_0)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V(s_1) = \\frac{7}{1-0.8} = 35 \\\\\n",
    "V(s_0) = 3 + 0.8 \\cdot 35 = 31 \\\\\n",
    "V(s_2) = 1 + 0.8 \\cdot 31 = 25.8 \\\\\n",
    "$$\n",
    "\n",
    "Sea cual sea el método de resolución que elijamos, al final tendremos unos primeros valores de $V$. \n",
    "\n",
    "Volviendo al asunto que nos preocupa, ahora ya podríamos empezar a modificar la política aleatoria con la que empezamos, ya que tenemos unos valores de $V$ que nos servirán como criterio. Así que la nueva versión de nuestra política será:\n",
    "\n",
    "$$\n",
    "\\pi’(s) \\leftarrow argmax_a[ R(s,a,s’) + \\gamma \\cdot V_\\pi(s')]\n",
    "$$\n",
    "\n",
    "Es decir, la política actualizada $\\pi’(s)$ para cada estado $s$ la calcularemos escogiendo la acción $a$ (de sus posibles acciones) cuyo $R(s,a,s’) + \\gamma \\cdot V_\\pi(s')$ sea máximo. Sustituiremos la política antigua por la nueva $\\pi \\leftarrow \\pi’$ y comenzaremos de nuevo el proceso. Esto hará que, iteración tras iteración, la política vaya mejorando hasta que llegue el momento en que no haya ningún nuevo cambio en ella. Esa será la condición de parada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el algoritmo en pseudocódigo para hacernos una idea del proceso completo:\n",
    "\n",
    "```python\n",
    "1 Initialize a policy π´arbitrarily\n",
    "2 repeat:\n",
    "3    V[s] = R[s,π(s),s´] + gamma * V[s´] # Ojo! aquí resolvemos el sistema lineal\n",
    "4    π´(s) ← argmax_a(R[s,a,s´] + gamma * V(s´))\n",
    "5    π ← π´\n",
    "6 until π == π´\n",
    "```\n",
    "\n",
    "\n",
    "Al igual que ocurría con **value iteration**, con el método **policy iteration** también necesitamos contar *a priori* con toda la información del entorno para poder calcular la política. \n",
    "\n",
    "En la mayoría de las ocasiones, **policy iteration** llega a una política correcta antes que **value iteration** ya que no es necesario que la tabla $V$ converja completamente.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

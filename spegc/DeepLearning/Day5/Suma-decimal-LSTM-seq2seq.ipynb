{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suma decimal LSTM seq2seq\n",
    "\n",
    "Los modelos **Seq2Seq** corresponden a arquitecturas de red ideadas para convertir secuencias de un dominio (por ejemplo, oraciones en inglés) a secuencias en otro dominio (por ejemplo, las mismas oraciones traducidas al español).\n",
    "\n",
    "\n",
    "Esto se puede usar para la traducción automática o para sistemas de pregunta/respuesta (generar una respuesta en lenguaje natural dada una pregunta en lenguaje natural); en general, es aplicable en cualquier situación en que se necesite generar texto.\n",
    "\n",
    "<img src=\"images/seq2seq.png\"/>\n",
    "\n",
    "Hay varias formas de abordar estos problemas, utilizando RNN o utilizando convnets 1D.\n",
    "\n",
    "Los casos más fáciles se dan cuando las secuencias de entrada y salida tienen la misma longitud. La suma de dígitos en decimal cumplen esta condición. Vamos a crear una red en Keras para abordar este problema en concreto.\n",
    "\n",
    "## Suma decimal\n",
    "\n",
    "La entrada a la red será una secuencia de dígitos decimales y en medio el sigo $+$. Cada dígito será como un caracter de una frase. El resultado será una secuencia de caracteres que corresponderá con la suma de los sumandos anteriores.\n",
    "\n",
    "<img src=\"images/addition-rnn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import string\n",
    "\n",
    "# Definir los caracteres permitidos y la longitud máxima de la cadena\n",
    "allowed_chars = string.digits + '+ '\n",
    "len_i_o = 5  # Longitud máxima de la cadena (por ejemplo, \"143+13\")\n",
    "\n",
    "# Función para codificar una cadena de caracteres en un tensor one-hot\n",
    "def string_to_tensor(s):\n",
    "    tensor = torch.zeros(len_i_o, len(allowed_chars))\n",
    "    for i, char in enumerate(s):\n",
    "        tensor[i, allowed_chars.index(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Función para decodificar un tensor one-hot en una cadena de caracteres\n",
    "def tensor_to_string(tensor):\n",
    "    _, max_idx = tensor.max(1)\n",
    "    return ''.join([allowed_chars[i] for i in max_idx])\n",
    "\n",
    "# Definir la arquitectura del modelo seq2seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_len = input.size(0)\n",
    "        encoder_hidden = (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n",
    "        \n",
    "        for i in range(input_len):\n",
    "            _, encoder_hidden = self.encoder(input[i].view(1, 1, -1), encoder_hidden)\n",
    "        \n",
    "        decoder_input = torch.tensor([[allowed_chars.index(' ')]])\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        output_string = []\n",
    "        for _ in range(len_i_o):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input.view(1, 1, -1), decoder_hidden)\n",
    "            output = self.output(decoder_output.view(1, -1))\n",
    "            _, topi = output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            output_string.append(allowed_chars[decoder_input.item()])\n",
    "            if allowed_chars[decoder_input.item()] == ' ':\n",
    "                break\n",
    "\n",
    "        return ''.join(output_string)\n",
    "\n",
    "# Definir hiperparámetros\n",
    "input_size = len(allowed_chars)\n",
    "hidden_size = 64\n",
    "output_size = len(allowed_chars)\n",
    "\n",
    "# Crear el modelo\n",
    "model = Seq2Seq(input_size, hidden_size, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Entrenar el modelo\n",
    "n_epochs = 10000\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    input_str = '143+13'\n",
    "    target_str = '156'\n",
    "    input_tensor = string_to_tensor(input_str)\n",
    "    target_tensor = string_to_tensor(target_str)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output_str = model(input_tensor)\n",
    "    loss = criterion(output_str.view(-1, len(allowed_chars)), target_tensor.view(-1, len(allowed_chars)))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}/{n_epochs}, Loss: {loss.item():.4f}, Output: {output_str}')\n",
    "\n",
    "# Evaluar el modelo\n",
    "input_str = '143+13'\n",
    "input_tensor = string_to_tensor(input_str)\n",
    "output_str = model(input_tensor)\n",
    "print(f'Input: {input_str}, Output: {output_str}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El generador de datos es correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Clase para generar el conjunto de datos\n",
    "class DatasetGenerator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.len_input = 7\n",
    "        self.len_output = 4\n",
    "        self.allowed_chars = string.digits + '+ '\n",
    "        \n",
    "    def string_to_tensor(self, s):\n",
    "        tensor = torch.zeros(len(s), len(self.allowed_chars))\n",
    "        for i, char in enumerate(s):\n",
    "            tensor[i, self.allowed_chars.index(char)] = 1\n",
    "        return tensor\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        samples = []\n",
    "        for _ in range(num_samples):\n",
    "            num1 = random.randint(1, 999)\n",
    "            num2 = random.randint(1, 999)\n",
    "            result = num1 + num2\n",
    "            input_str = str(num1) + '+' + str(num2)\n",
    "            input_str = input_str.rjust(self.len_input)\n",
    "            output_str = str(result).rjust(self.len_output)\n",
    "            samples.append((input_str, output_str))\n",
    "\n",
    "        x = [self.string_to_tensor(sample[0]) for sample in samples]\n",
    "        y = [self.string_to_tensor(sample[1]) for sample in samples]\n",
    "        return torch.stack(x), torch.stack(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definición del modelo seq2seq\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        input_length = input_seq.size(0)\n",
    "        target_length = target_seq.size(0)\n",
    "\n",
    "        encoder_outputs, (encoder_hidden, encoder_cell) = self.encoder(self.embedding(input_seq))\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_cell = encoder_cell\n",
    "        \n",
    "        decoder_outputs = torch.zeros(target_length, 1, self.hidden_size)\n",
    "        \n",
    "        for t in range(target_length):\n",
    "            decoder_output, (decoder_hidden, decoder_cell) = self.decoder(decoder_outputs[t], (decoder_hidden, decoder_cell))\n",
    "            output = self.fc(decoder_output)\n",
    "            decoder_outputs[t] = output\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "# Función para preparar secuencias de entrada y objetivo\n",
    "def prepare_sequence(sequence, to_ix):\n",
    "    idxs = [to_ix[ch] for ch in sequence]\n",
    "    return torch.tensor(idxs, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "# Hiperparámetros\n",
    "input_size = 12  # Tamaño del vocabulario (0-9, \"+\", padding)\n",
    "hidden_size = 128\n",
    "output_size = 10  # 0-9\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10000\n",
    "num_samples = 10000\n",
    "\n",
    "# Generar el conjunto de datos\n",
    "dataset = DatasetGenerator(num_samples)\n",
    "data = dataset.generate_samples()\n",
    "\n",
    "# Crear diccionarios de caracteres a índices\n",
    "char_to_idx = {char: idx for idx, char in enumerate(\"0123456789+\")}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "# Crear el modelo y definir la función de pérdida y el optimizador\n",
    "model = Seq2SeqModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq in data:\n",
    "        input_tensor = prepare_sequence(input_seq, char_to_idx)\n",
    "        target_tensor = prepare_sequence(target_seq, char_to_idx)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(input_tensor, target_tensor)\n",
    "        loss = criterion(output.view(-1, output_size), target_tensor.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss}')\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_input = \"123+456\"\n",
    "test_input_tensor = prepare_sequence(test_input, char_to_idx)\n",
    "output = model(test_input_tensor, torch.zeros(3, 1, hidden_size))  # Longitud máxima de salida\n",
    "predicted = torch.argmax(output, dim=2)\n",
    "predicted_sequence = ''.join([idx_to_char[idx] for idx in predicted.view(-1)])\n",
    "print(f'Entrada: {test_input}, Predicción: {predicted_sequence}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

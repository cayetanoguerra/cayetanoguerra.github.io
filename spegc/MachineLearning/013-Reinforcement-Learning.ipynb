{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/logo-spegc.svg\" width=30%>\n",
    "\n",
    "# Aprendizaje por refuerzo\n",
    "\n",
    "El aprendizaje por refuerzo es un área del aprendizaje automático inspirada en la psicología conductista, cuya ocupación es determinar qué acciones debe escoger un agente en un entorno dado con el fin de maximizar alguna noción de \"recompensa\" o premio acumulado.\n",
    "\n",
    "## Algoritmo Q-Learning\n",
    "\n",
    "<img src=\"imgs/plano.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las puertas que dan directamente al exterior tienen una recompensa de 100. El resto de puertas no tienen recompensa. El plano de la casa puede ser visto como un grafo.\n",
    "\n",
    "<img src=\"imgs/grafo.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de recompensas $R$ sería:\n",
    "\n",
    "$$R = \\begin{pmatrix}\n",
    "-1 & -1 & -1 & -1 & 0 & -1 \\\\\n",
    "-1 & -1 & -1 & 0 & -1 & 100 \\\\\n",
    "-1 & -1 & -1 & 0 & -1 & -1 \\\\\n",
    "-1 & 0 & 0 & -1 & 0 & -1 \\\\\n",
    "0 & -1 & -1 & 0 & -1 & 100 \\\\\n",
    "-1 & 0 & -1 & -1 & 0 & 100\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$Q$ es nuestra tabla de conocimiento del entorno. Inicialmente estará vacía. Ten presente que comenzamos los índices de las tablas $R$ y $Q$ en $0$, no en $1$.\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Representa el valor que tiene un par (estado, acción). Su actualización la haremos con la siguiente fórmula:\n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma max[Q(s',a')]$$\n",
    "\n",
    "Supongamos que comenzamos en la habitación número 1 y, por azar, seleccionamos la acción \"ir a 5\". La actualización de $Q$ será:\n",
    "\n",
    "$$Q(1,5) = R(1,5) + \\gamma max[Q(5,1),Q(5,4),Q(5,5)]$$\n",
    "\n",
    "El parámetro $\\gamma$ es $0.8$. Por tanto, el nuevo valor de la celda (1,5) será:\n",
    "\n",
    "$$Q(1,5) = 100 + 0.8 \\times max[0,0,0] = 100$$\n",
    "\n",
    "Nuestra tabla $Q$ se actualizará de esta forma:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Como el siguiente estado es el 5, hemos llegado al final del episodio. Supongamos ahora que partimos del estado 3. Tenemos tres posibles acciones a realizar: ir al estado 1, al 2 o al 4. Supongamos que, también por azar, seleccionamos ir al estado 1.\n",
    "\n",
    "$$Q(3,1) = R(3,1) + \\gamma max[Q(1,3),Q(1,5)]$$\n",
    "\n",
    "Cuyos valores son:\n",
    "\n",
    "$$Q(3,1) = 0 + 0.8 \\times max[0,100] = 80$$\n",
    "\n",
    "Nuestra tabla $Q$ queda:\n",
    "\n",
    "$$Q = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 80 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Hemos llegado de nuevo al estado 5, así que terminamos el episodio y empezaríamos uno nuevo. Si continuamos durante más episodios veremos que la tabla $Q$ se irá actualizando hasta llegar a converger en la matriz $Q^*$. Alcanzada esta convergencia, diremos que nuestro agente ha aprendido a desenvolverse por este entorno.\n",
    "\n",
    "### Implementación del algoritmo\n",
    "\n",
    "Establecemos los parámetros del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "discount = 0.8 # gamma\n",
    "learning_rate = 0.5 # alfa\n",
    "final_state = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la tabla de recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = [[-1., -1., -1., -1., 0., -1.],\n",
    "           [-1., -1., -1., 0., -1., 100.],\n",
    "           [-1., -1., -1., 0., -1., -1.],\n",
    "           [-1., 0., 0., -1., 0., -1.],\n",
    "           [0., -1., -1., 0., -1., 100.],\n",
    "           [-1., 0., -1., -1., 0., 100.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la tabla $Q$ a cero o cualquier valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "Q = [[0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "import random\n",
    "#for j in xrange(len(rewards)):\n",
    "#    for i in xrange(len(rewards[0])):\n",
    "#        Q[j][i] = round(random.random()*100, 2)\n",
    "        \n",
    "from tabulate import tabulate\n",
    "print(tabulate(Q, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fórmula de actualización de la matriz $Q$ \n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma max[Q(s',a')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qlearning1(s, a):\n",
    "    Q[s][a] = rewards[s][a] + discount * max(Q[a])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(s,a) = \\alpha Q(s,a) + (1-\\alpha)[R(s,a) + \\gamma max[Q(s',a')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qlearning2(s, a):\n",
    "    Q[s][a] = learning_rate * Q[s][a] + (1.0-learning_rate) * (rewards[s][a] + discount * max(Q[a]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   |  0 | 80 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   | 64 |  0 | 100 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   | 64 |  0 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 | 80 | 51.2 |  0 | 80 |   0 |\n",
      "+----+----+------+----+----+-----+\n",
      "| 64 |  0 |  0   | 64 |  0 | 100 |\n",
      "+----+----+------+----+----+-----+\n",
      "|  0 |  0 |  0   |  0 |  0 |   0 |\n",
      "+----+----+------+----+----+-----+\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "\n",
    "    s = random.randint(0, 5)\n",
    "    while s == final_state:\n",
    "        s = random.randint(0, 5)\n",
    "\n",
    "    keep = True\n",
    "    while keep:\n",
    "        a = random.randint(0, 5)\n",
    "        while rewards[s][a] == -1:\n",
    "            a = random.randint(0, 5)\n",
    "        qlearning1(s, a)\n",
    "        s = a\n",
    "        if s == final_state:\n",
    "            keep = False \n",
    "            \n",
    "print(tabulate(Q, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios prácticos\n",
    "\n",
    "* Modifica el código para imprimir los estados por donde se pasa y los valores de las celdas de $Q$ que se modifican durante el aprendizaje.\n",
    "* Actualiza la tabla $Q$ realizando a mano un episodio con el siguiente recorrido $0\\rightarrow4\\rightarrow3\\rightarrow1\\rightarrow5$.\n",
    "* Sustituye la primera fórmula del cálculo de $Q$ por la segunda y describe qué diferencias observas.\n",
    "* ¿Qué ventajas o desventajas ofrece la primera fórmula de cálculo de $Q$ con respecto a la segunda?\n",
    "* Crea una función que, a partir de la matriz $Q^*$, nos lleve a la salida por el camino óptimo desde cualquier habitación.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

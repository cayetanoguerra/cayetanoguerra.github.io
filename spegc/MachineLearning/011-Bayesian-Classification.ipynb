{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/logo-spegc.svg\" width=30%>\n",
    "\n",
    "# Naïve Bayes Classification\n",
    "\n",
    "En el clasificador bayesiano se utiliza el teorema de Bayes y ciertas asunciones de independencia para llevar a cabo una separación de las muestras basadas en sus probabilidades de aparición.\n",
    "\n",
    "El teorema de Bayes indica lo siguiente:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "Traslademos el teorema a una aplicación:\n",
    "\n",
    "$$ P(c_i|o) = \\frac{P(o|c_i)P(c_i)}{P(o)} $$\n",
    "\n",
    "\n",
    "- $P(c_i)$: Probabilidad *a priori* de que una nueva observación $o$ pertenezca a la clase $c_i$.\n",
    "- $P(c_i|o)$: Verosimilitud o probabilidad de que dada una observación $o$ esta pertenezca a la clase $c_i$\n",
    "- $P(o|c_i)$: Probabilidad *a posteri*, es decir, qué probabilidad hay de tener la observación $o$ si ya sabemos que pertenece a la clase $c_i$.\n",
    "- $P(o)$: Evidencia, o probabilidad de nuestra observación.\n",
    "\n",
    "Lo que nos interesa es, dada una observación, determinar cuál es la probabiliad de que pertenezca a cada clase posible. Supongamos que tuviéramos tres clases $(c_1, c_2, c_3)$.\n",
    "\n",
    "$$ P(c_1|o) = \\frac{P(o|c_1)P(c_1)}{P(o)} $$\n",
    "$$ P(c_2|o) = \\frac{P(o|c_2)P(c_2)}{P(o)} $$\n",
    "$$ P(c_3|o) = \\frac{P(o|c_3)P(c_3)}{P(o)} $$\n",
    "\n",
    "Nuestro clasificador se basaría simplemente en escoger la clase de verosimilitud más alta. Pero, si nos fijamos bien, vemos que $P(o)$ es un denominador que se repite para las tres clases con lo que podemos prescindir de él y hacer más fácil el cálculo.\n",
    "\n",
    "$$ c_1 \\leftarrow P(o|c_1)P(c_1) $$\n",
    "$$ c_2 \\leftarrow P(o|c_2)P(c_2) $$\n",
    "$$ c_3 \\leftarrow P(o|c_3)P(c_3) $$\n",
    "\n",
    "Ahora solo tenemos que elegir el $c_i$ más alto.\n",
    "\n",
    "### Independencia probabilística\n",
    "\n",
    "Decimos que la probabilidad de $A$ es independiente de $B$ si: \n",
    "\n",
    "$$ P(A|B) = P(A)$$\n",
    "\n",
    "Es decir, da igual que ocurra $B$ o no para que se dé $A$. Por lo tanto, tenemos que:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\rightarrow {P(A \\cap B)} = P(B)P(A|B) \\rightarrow {P(A) \\cap P(B)} = P(B)P(A) $$\n",
    "\n",
    "\n",
    "## Clasificador de textos\n",
    "\n",
    "Nuestro dataset es un conjunto de textos (correos electrónicos, críticas, comentarios, artículos...) clasificados en algunas categorías. Por ejemplo, comentarios sobre películas clasificados como *positivos* o *negativos*. Cada muestra es un comentario y cada comentario está compuesto por palabras. Al conjunto de todas las palabras que aparecen en todos los comentarios lo llamamos *diccionario*.\n",
    "\n",
    "Por tanto, un comentario $W$ estará formado por un conjunto de n palabras $(w_1, w_2,\\dots, w_n)$. De esta forma, nos interesaría saber cual es la verosimilitud de pertenencia del comentario a una clase.\n",
    "\n",
    "$$ P(c_1|w_1,w_2,\\dots,w_n) = \\frac{P(w_1,w_2,\\dots,w_n|c_1)P(c_1)}{P(w_1,w_2,\\dots,w_n)} $$\n",
    "$$ P(c_2|w_1,w_2,\\dots,w_n) = \\frac{P(w_1,w_2,\\dots,w_n|c_2)P(c_2)}{P(w_1,w_2,\\dots,w_n)} $$\n",
    "\n",
    "Si asumimos la independencia de aparición de las palabras en un comentario tenemos que:\n",
    "\n",
    "$$ P(w_1,w_2,\\dots,w_n|c) = P(w_1|c)P(w_1|c) \\dots P(w_n|c) $$\n",
    "\n",
    "Este hecho es el que otorga el nombre de *naïve* o *ingenuo* al método, ya que asumimos que no hay una relación directa entre las palabras. Esto, obviamiente, no es cierto pero sí nos facilita enormemente la labor, ya que ahora podemos limitarnos a calcular la probabilidad de aparición de cada palabra en las clases *positivo* y *negativo*.\n",
    "\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Transformamos cada oración en un vector de logitud fija. Esta longitud corresponderá al número de palabras de nuestro vocabulario. En nuestro ejemplo clasificaremos frases como ofensivas o no ofensivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['mi', 'perro', 'tiene', 'tos', 'necesito', 'ayuda'],\n",
    "        ['no', 'deberías', 'llevarlo', 'al', 'parque', 'estúpido'],\n",
    "        ['me', 'encanta', 'mi', 'dálmata', 'es', 'adorable'],\n",
    "        ['para', 'de', 'postear', 'tonterías', 'estúpido'],\n",
    "        ['mi', 'gato', 'se', 'comió', 'mi', 'comida'],\n",
    "        ['no', 'compres', 'tonterías', 'a', 'tu', 'estúpido', 'gato']]\n",
    "\n",
    "    classVec = [0,1,0,1,0,1] #1 is abusive, 0 not\n",
    "    return postingList, classVec\n",
    "\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)  # We create a list of 0's of lenght 'vocabList'\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: \n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos un ejemplo de frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mi', 'perro', 'tiene', 'tos', 'necesito', 'ayuda'] 0\n",
      "['no', 'deberías', 'llevarlo', 'al', 'parque', 'estúpido'] 1\n",
      "['me', 'encanta', 'mi', 'dálmata', 'es', 'adorable'] 0\n",
      "['para', 'de', 'postear', 'tonterías', 'estúpido'] 1\n",
      "['mi', 'gato', 'se', 'comió', 'mi', 'comida'] 0\n",
      "['no', 'compres', 'tonterías', 'a', 'tu', 'estúpido', 'gato'] 1\n",
      "\n",
      "Sentence codification:\n",
      "['mi', 'perro', 'tiene', 'tos', 'necesito', 'ayuda']\n",
      "[0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "p, c = loadDataSet()\n",
    "for i, e in zip(p, c):\n",
    "    print(i, e)\n",
    "\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "\n",
    "print(\"\\nSentence codification:\")\n",
    "print(listOPosts[0])\n",
    "print(setOfWords2Vec(myVocabList, listOPosts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador Naíf Bayesiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    \n",
    "    p0Num = np.ones(numWords) # p0Num = 0\n",
    "    p1Num = np.ones(numWords)  # p1Num = 0\n",
    "    p0Denom = len(p0Num) # p0Denom = 0\n",
    "    p1Denom = len(p0Num) # p1Denom = 0\n",
    "\n",
    "    for tm, tc in zip(trainMatrix, trainCategory):\n",
    "        if tc:\n",
    "            p1Num += tm # Number of instances of each word\n",
    "            p1Denom += sum(tm) # Number of words in class 1\n",
    "        else:\n",
    "            p0Num += tm\n",
    "            p0Denom += sum(tm) # Number of words in class 0\n",
    "\n",
    "    p1Vect = np.log(p1Num/p1Denom)  # p1Vect = p1Num/p1Denom\n",
    "    p0Vect = np.log(p0Num/p0Denom)  # p0Vect = p0Num/p0Denom\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    \n",
    "    #p1 = np.prod(vec2Classify * p1Vec) * pClass1\n",
    "    #p0 = np.prod(vec2Classify * p0Vec) * (1.0 - pClass1)\n",
    "    \n",
    "    print(p1, p0)\n",
    "    \n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.136511755608435 -15.919797139641224\n",
      "1\n",
      "-16.007712766516324 -12.454061236841497\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "p0V, p1V, pAb = trainNB0(trainMat,listClasses)\n",
    "\n",
    "s = ['para','de', 'tonterías', 'estúpido']\n",
    "sw = setOfWords2Vec(myVocabList, s)\n",
    "print(classifyNB(sw,p0V,p1V,pAb))\n",
    "\n",
    "s = ['mi', 'perro', 'es', 'adorable']\n",
    "sw = setOfWords2Vec(myVocabList, s)\n",
    "print(classifyNB(sw,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## IMDB Sentiment Analisys\n",
    "\n",
    "En este ejercicio realizaremos una clasificación de análisis de opinión (sentiment analisys) de un dataset de 25.000 comentarios etiquetados de películas extraído de la base de datos cinematográfica IMDB. Para llevarlo a cabo utilizaremos un clasificador bayesiano, y mediremos su rendimiento mediante otro dataset de test con otros 25.000 comentarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocab(dataSet):\n",
    "    vocab = {}\n",
    "    index = 0\n",
    "    for document in dataSet:\n",
    "        for word in document:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def setOfWords2Vec(vocab, inputSet):\n",
    "    words = {}\n",
    "    for word in inputSet:\n",
    "        if word in vocab:\n",
    "            if word not in words:\n",
    "                words[word] = 1\n",
    "            else:\n",
    "                words[word] += 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def load_data(path_to_dir):\n",
    "    \"\"\"\n",
    "    Loads the train and test set into four different lists.\n",
    "    \"\"\"\n",
    "    train_pos = []\n",
    "    train_neg = []\n",
    "    test_pos = []\n",
    "    test_neg = []\n",
    "\n",
    "    print(\"Reading positive train samples...\")\n",
    "    with open(path_to_dir + \"train-pos.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            words = [w.lower() for w in line.strip().split() if len(w) >= 3]\n",
    "            train_pos.append(words)\n",
    "\n",
    "    print(\"Reading negative train samples...\")\n",
    "    with open(path_to_dir + \"train-neg.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            words = [w.lower() for w in line.strip().split() if len(w) >= 3]\n",
    "            train_neg.append(words)\n",
    "\n",
    "    print(\"Reading positive test samples...\")\n",
    "    with open(path_to_dir + \"test-pos.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            words = [w.lower() for w in line.strip().split() if len(w) >= 3]\n",
    "            test_pos.append(words)\n",
    "\n",
    "    print(\"Reading negative test samples...\")\n",
    "    with open(path_to_dir + \"test-neg.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            words = [w.lower() for w in line.strip().split() if len(w) >= 3]\n",
    "            test_neg.append(words)\n",
    "\n",
    "    return train_pos, train_neg, test_pos, test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading positive train samples...\n",
      "Reading negative train samples...\n",
      "Reading positive test samples...\n",
      "Reading negative test samples...\n"
     ]
    }
   ],
   "source": [
    "train_pos, train_neg, test_pos, test_neg = load_data(\"./data/imdb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_pos))\n",
    "print(len(train_neg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/logo-spegc.svg\" width=30%>\n",
    "\n",
    "# Redes Neuronales\n",
    "\n",
    "Una red neuronal es un clasificador. Su funcionamiento se basa en la combinación de hiperplanos que acotan regiones de activación del espacio de muestras.\n",
    "\n",
    "<img src=\"images/red.svg\" width=30%>\n",
    "\n",
    "### Mini-batch\n",
    "\n",
    "En cada ciclo de actualización de los pesos de la red no es necesario calcular el gradiente de descenso para cada muestra. Basta con escoger un lote variado de muestras (mini-batch) y calcular el gradiente resultante de éste. \n",
    "\n",
    "### Loss\n",
    "\n",
    "La pérdida o *loss* es el valor o error resultante de comparar las predicciones de la red con las etiquetas de las muestras.\n",
    "\n",
    "### One-hot\n",
    "\n",
    "Cuando se clasifica en más de dos categorías es necesario que la red tenga tantas neuronas en la capa de salida como clases haya. Por tanto, la activación de cada una de estas neuronas de salida corresponderá con una clase. Por ejemplo, si la red tiene cuatro salidas y se activa la tercera, el vector resultante podría ser algo como: [0.002,0.008,0.95,0.04]. Como normalmente las etiquetas de las muestras están en un formato numérico entero (0,1,2,3...), no podremos realizar la comparación de las salidas con las etiquetas. Así que transformamos la etiqueta en un vector de todo ceros excepto el lugar correspondiente a la etiqueta, el cual tendrá un valor de 1. Por ejemplo: $2\\rightarrow [0,0,1,0]$. (Recordemos que las etiquetas comienzan en 0). Esta función de transformación se denomina **One-hot**.\n",
    "\n",
    "### Época\n",
    "\n",
    "Cada vez que entrenamos una red con todos los lotes o *mini-batches* de nuestro dataset de entrenamiento decimos que hemos completado una **época**. Durante el proceso de entrenamiento de una red se suelen completar múltiples épocas.\n",
    "\n",
    "### Softmax\n",
    "\n",
    "La función softmax es parecida a la sigmoide, ésta también convierte un valor entre menos infinito y más infinito en un valor entre cero y uno. La diferencia está en que la función softmax no trabaja sobre un valor sino sobre un vector. De esta forma, convierte todas las componentes de un vector en valores entre cero y uno, pero, además, garantiza que la suma de todos estos valores sea uno. La función softmax se emplea en la capa de salida de una red, y convierte de esta forma sus salidas en una distribución de probabilidades.\n",
    "\n",
    "$$ Softmax(\\textbf{v})_i = \\frac{e^{v_i}}{\\sum_{j=1}^{n} e^{v_j}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "nan\n",
      "Epoch:  50\n",
      "nan\n",
      "Epoch:  100\n",
      "nan\n",
      "Epoch:  150\n",
      "nan\n",
      "Epoch:  200\n",
      "nan\n",
      "Epoch:  250\n",
      "nan\n",
      "Epoch:  300\n",
      "nan\n",
      "Epoch:  350\n",
      "nan\n",
      "Epoch:  400\n",
      "nan\n",
      "Epoch:  450\n",
      "nan\n",
      "Epoch:  500\n",
      "nan\n",
      "Epoch:  550\n",
      "nan\n",
      "Epoch:  600\n",
      "nan\n",
      "Epoch:  650\n",
      "nan\n",
      "Epoch:  700\n",
      "nan\n",
      "Epoch:  750\n",
      "nan\n",
      "Epoch:  800\n",
      "nan\n",
      "Epoch:  850\n",
      "nan\n",
      "[1. 0. 0.]\n",
      "[nan nan nan]\n",
      "[1. 0. 0.]\n",
      "[nan nan nan]\n",
      "[0. 1. 0.]\n",
      "[nan nan nan]\n",
      "[0. 0. 1.]\n",
      "[nan nan nan]\n",
      "[1. 0. 0.]\n",
      "[nan nan nan]\n",
      "[0. 1. 0.]\n",
      "[nan nan nan]\n",
      "[0. 1. 0.]\n",
      "[nan nan nan]\n",
      "[0. 1. 0.]\n",
      "[nan nan nan]\n",
      "[0. 0. 1.]\n",
      "[nan nan nan]\n",
      "[0. 0. 1.]\n",
      "[nan nan nan]\n",
      "[1. 0. 0.]\n",
      "[nan nan nan]\n",
      "[0. 1. 0.]\n",
      "[nan nan nan]\n",
      "[0. 0. 1.]\n",
      "[nan nan nan]\n",
      "[0. 1. 0.]\n",
      "[nan nan nan]\n",
      "[1. 0. 0.]\n",
      "[nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from NN4Teaching import nn4t\n",
    "\n",
    "data = np.genfromtxt('data/iris.data', delimiter=\",\")\n",
    "np.random.shuffle(data)\n",
    "x_data = data[:, 0:4].astype('f4')\n",
    "y_data = nn4t.one_hot(data[:, 4].astype(int), 3)\n",
    "\n",
    "net = nn4t.Net(layers=[4, 5, 3])\n",
    "\n",
    "for i in range(900):\n",
    "    net.train(x_data, y_data)\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(\"Epoch: \", i)\n",
    "        print(net.loss(x_data, y_data))\n",
    "\n",
    "for x, y in zip(x_data[:15], y_data[:15]):\n",
    "    print(y)\n",
    "    print(net.output(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "- Entrena la red mediante mini-batchs.\n",
    "- Crea un gráfico del loss.\n",
    "- Finaliza el entrenamiento cuando la curva del loss se estabilice.\n",
    "- Divide el dataset en entrenamiento y test.\n",
    "- Calcula la precisión final de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/keras-logo.png\" width=30%>\n",
    "\n",
    "## Keras\n",
    "https://keras.io/\n",
    "\n",
    "Keras es una API de redes neuronales de alto nivel, escrita en Python y capaz de ejecutarse sobre TensorFlow, CNTK o Theano. Fue desarrollado con la idea de permitir la experimentación rápida, poder pasar de la idea al resultado en el menor tiempo posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo montar una red en Keras para clasificar el dataset Iris. En primer lugar importamos la librería y el dataset. La función `LabelBinarizer` nos convertirá las etiquetas numéricas enteras a codificación one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "#load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(iris.target)  # We transform to one-hot\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la red con la capa de entrada correspondiente a los cuatro valores de cada muestra. Una capa oculta de cinco neuronas y una capa de salida de tres neuronas. Aplicamos la función de activación **sigmoide** para la capa oculta y, dado que tenemos más de dos clases, **softmax** para la salida. Utilizamos *Stochastic Gradient Descent* como método optimizador con un **learning rate** de 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential #Sequential Models\n",
    "from keras.layers import Dense #Dense Fully Connected Layer Type\n",
    "from keras.optimizers import SGD #Stochastic Gradient Descent Optimizer\n",
    "\n",
    "def create_network():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_shape=(4,), activation='sigmoid'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "        \n",
    "    #stochastic gradient descent\n",
    "    sgd = SGD(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos realizando 500 épocas sobre el conjunto de entrenamiento. Utilizamos un tamaño del lote (`batch_size`) de 10. Esto quiere decir que actualizamos todos los pesos de nuetra red utilizando solo 10 muestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = create_network()\n",
    "neural_network.fit(X_train, y_train, epochs=500, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neural_network.metrics_names)\n",
    "neural_network.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = neural_network.predict(X_test)\n",
    "for p, l in zip(predictions, y_test):\n",
    "    print(p,\"->\", l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado y entrenado nuestro modelo querremos guardarlo para usarlo en producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "neural_network.save(\"mimodelo.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que apagamos nuestro ordenador (`del neural_network`). Si queremos cargarlo de nuevo haremos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del neural_network  # deletes the existing model\n",
    "\n",
    "# we return a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('mimodelo.h5')\n",
    "print(model.metrics_names)\n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "- Crea una red para clasificar el conjunto MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sampledata\n",
    "\n",
    "batch_size = 10\n",
    "number_of_neurons = 5\n",
    "digits = 8\n",
    "\n",
    "data, label = sampledata.create_data(500)\n",
    "print(data.shape)\n",
    "print(label.shape)\n",
    "label = label.reshape((500, 8, 1))\n",
    "print(label.shape)\n",
    "\n",
    "# Model ------------------------------------------------\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, TimeDistributed\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(number_of_neurons, input_shape=(data.shape[1:]), activation='sigmoid', return_sequences=True, use_bias=True))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "\n",
    "sgd = optimizers.SGD(lr=1.)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# train RNN\n",
    "model.fit(data, label, epochs=100, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué tal ha aprendido a sumar nuestra RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ----------------------------------------------------\n",
    "\n",
    "data, label = sampledata.create_data(100)\n",
    "label = label.reshape((100, 8, 1))\n",
    "\n",
    "error = model.evaluate(data, label, verbose=2)\n",
    "\n",
    "data, label = sampledata.create_data(2)\n",
    "predicted = np.round(model.predict(data))\n",
    "\n",
    "for d, l, r in zip(data, label, predicted.reshape(2, 8)):\n",
    "    print(\"data:\")\n",
    "    print(\" \", np.transpose(d)[0][::-1])\n",
    "    print(\"+\", np.transpose(d)[1][::-1])\n",
    "    print(\"------------------------------\")\n",
    "    print(\" \", r[::-1], \"result\")\n",
    "    print(\" \", l[::-1], \"label\")\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "print(\"Precisión: \"+ str(error[1]*100) + \"%\")\n",
    "print(\"------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ajuste a una curva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from NN4Teaching import nn4t\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(0.0, 10.0, 0.1)\n",
    "secuencia = (np.sin(x-1)*0.5+0.1*np.sin(5*x)+1.5)*0.2 # Nuestra secuencia de valores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, secuencia)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('valores')\n",
    "plt.title('Secuencia de valores')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "longitud_muestra = 10 #Elegido arbitrariamente\n",
    "\n",
    "muestras_vector = []\n",
    "etiquetas_vector = []\n",
    "for i in range(len(secuencia)-longitud_muestra):\n",
    "    muestras_vector.append(secuencia[i:i+longitud_muestra])\n",
    "    etiquetas_vector.append([secuencia[i+longitud_muestra]])\n",
    "    \n",
    "    \n",
    "    \n",
    "print(muestras_vector[0])\n",
    "print(etiquetas_vector[0])\n",
    "print(\"----------------------\")\n",
    "print(muestras_vector[1])\n",
    "print(etiquetas_vector[1])\n",
    "print(\"----------------------\")\n",
    "print(muestras_vector[2])\n",
    "print(etiquetas_vector[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn4t.Net(layers=[longitud_muestra, 5, 1])\n",
    "\n",
    "for i in range(5000):\n",
    "    net.train(muestras_vector, etiquetas_vector)\n",
    "    \n",
    "    if i%500 == 0:\n",
    "        print(\"Epoch: \", i)\n",
    "        print(net.loss(muestras_vector, etiquetas_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []\n",
    "\n",
    "for i in range(longitud_muestra):\n",
    "    resultados.append(muestras_vector[0][i])\n",
    "        \n",
    "for i in range(len(secuencia)-longitud_muestra):\n",
    "    resultados.append(net.output(resultados[i:i+longitud_muestra])[0])\n",
    "   \n",
    "etiquetas_vector = ([[0]]*longitud_muestra) + etiquetas_vector\n",
    "\n",
    "plt.plot(resultados, 'r-', etiquetas_vector, 'g-')\n",
    "plt.xlabel('Longitud de muestra')\n",
    "plt.ylabel('Valores')\n",
    "plt.title('Resultados')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "- Realizar la predicción de las <a href=\"./data/solar_spots.csv\">manchas solares</a>\n",
    "- Medir el error medio y desviación típica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
